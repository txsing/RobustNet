Logging : ./logs/1122/r50os16_gtav_isw/11_22_20/log_2021_11_22_20_54_47_rank_1.log
Logging : ./logs/1122/r50os16_gtav_isw/11_22_20/log_2021_11_22_20_54_47_rank_0.log
Total world size: Total world size:   22

My Rank: 1
My Rank: 0
11-22 20:54:48.846 Added key: store_based_barrier_key:1 to store for rank: 0
Using pytorch sync batch norm
Using pytorch sync batch norm
11-22 20:54:48.903 train fine cities: ['train/folder']
11-22 20:54:49.269 GTAV-train: 12388 images
###### centroids 0
###### centroids 2
###### centroids 3
###### centroids 4
###### centroids 5
###### centroids 8
###### centroids 9
###### centroids 10
###### centroids 11
###### centroids 14
###### centroids 1
###### centroids 6
###### centroids 7
###### centroids 13
###### centroids 17
###### centroids 15
###### centroids 12
###### centroids 18
###### centroids 16
###### centroids 0
###### centroids 2
###### centroids 3
###### centroids 4
###### centroids 5
###### centroids 8
###### centroids 9
###### centroids 10
###### centroids 11
###### centroids 14
###### centroids 1
###### centroids 6
###### centroids 7
###### centroids 13
###### centroids 17
###### centroids 15
###### centroids 12
###### centroids 18
###### centroids 16
11-22 20:54:49.913 Class Uniform Percentage: 0.5
11-22 20:54:49.913 Class Uniform items per Epoch:12388
11-22 20:54:49.916 cls 0 len 12109
11-22 20:54:49.916 cls 1 len 11833
11-22 20:54:49.916 cls 2 len 12301
11-22 20:54:49.917 cls 3 len 10854
11-22 20:54:49.917 cls 4 len 8811
11-22 20:54:49.917 cls 5 len 11928
11-22 20:54:49.917 cls 6 len 7891
11-22 20:54:49.917 cls 7 len 5921
11-22 20:54:49.917 cls 8 len 12132
11-22 20:54:49.917 cls 9 len 11549
11-22 20:54:49.917 cls 10 len 12131
11-22 20:54:49.917 cls 11 len 10691
11-22 20:54:49.917 cls 12 len 986
11-22 20:54:49.917 cls 13 len 10501
11-22 20:54:49.917 cls 14 len 6711
11-22 20:54:49.917 cls 15 len 1861
11-22 20:54:49.917 cls 16 len 493
11-22 20:54:49.917 cls 17 len 1211
11-22 20:54:49.917 cls 18 len 168
11-22 20:54:49.925 val fine cities: ['valid/folder']
11-22 20:54:49.972 GTAV-val: 6382 images
11-22 20:54:49.984 val fine cities: ['val/frankfurt', 'val/munster', 'val/lindau']
11-22 20:54:49.993 Cityscapes-val: 500 images
11-22 20:54:49.994 train fine cities: ['train/folder']
standard cross entropy
standard cross entropy
11-22 20:54:50.061 GTAV-train: 12388 images
standard cross entropy
standard cross entropy
Model : DeepLabv3+, Backbone : ResNet-50
Model : DeepLabv3+, Backbone : ResNet-50
########### pretrained ##############
########### pretrained ##############
Skipped loading parameter layer1.0.bn1.num_batches_tracked
Skipped loading parameter layer1.0.bn2.num_batches_tracked
Skipped loading parameter layer1.0.bn3.num_batches_tracked
Skipped loading parameter layer1.0.downsample.1.num_batches_tracked
Skipped loading parameter layer1.1.bn1.num_batches_tracked
Skipped loading parameter layer1.1.bn2.num_batches_tracked
Skipped loading parameter layer1.1.bn3.num_batches_tracked
Skipped loading parameter layer1.2.bn1.num_batches_tracked
Skipped loading parameter layer1.2.bn2.num_batches_tracked
Skipped loading parameter layer1.2.bn3.num_batches_tracked
Skipped loading parameter layer2.0.bn1.num_batches_tracked
Skipped loading parameter layer2.0.bn2.num_batches_tracked
Skipped loading parameter layer2.0.bn3.num_batches_tracked
Skipped loading parameter layer2.0.downsample.1.num_batches_tracked
Skipped loading parameter layer2.1.bn1.num_batches_tracked
Skipped loading parameter Skipped loading parameterlayer2.1.bn2.num_batches_tracked
 Skipped loading parameterlayer1.0.bn1.num_batches_tracked layer2.1.bn3.num_batches_tracked

Skipped loading parameter layer2.2.bn1.num_batches_tracked
Skipped loading parameterSkipped loading parameter  layer1.0.bn2.num_batches_trackedlayer2.2.bn2.num_batches_tracked

Skipped loading parameterSkipped loading parameter  layer1.0.bn3.num_batches_trackedlayer2.2.bn3.num_batches_tracked

Skipped loading parameterSkipped loading parameter  layer2.3.bn1.num_batches_trackedlayer1.0.downsample.1.num_batches_tracked

Skipped loading parameter layer2.3.bn2.num_batches_trackedSkipped loading parameter
 layer1.1.bn1.num_batches_trackedSkipped loading parameter
 layer2.3.bn3.num_batches_tracked
Skipped loading parameter Skipped loading parameterlayer1.1.bn2.num_batches_tracked 
layer3.0.bn1.num_batches_tracked
Skipped loading parameterSkipped loading parameter  layer1.1.bn3.num_batches_trackedlayer3.0.bn2.num_batches_tracked

Skipped loading parameterSkipped loading parameter  layer1.2.bn1.num_batches_trackedlayer3.0.bn3.num_batches_tracked

Skipped loading parameter Skipped loading parameterlayer3.0.downsample.1.num_batches_tracked 
layer1.2.bn2.num_batches_tracked
Skipped loading parameter layer3.1.bn1.num_batches_trackedSkipped loading parameter
 Skipped loading parameterlayer1.2.bn3.num_batches_tracked 
layer3.1.bn2.num_batches_tracked
Skipped loading parameterSkipped loading parameter  layer2.0.bn1.num_batches_trackedlayer3.1.bn3.num_batches_tracked

Skipped loading parameterSkipped loading parameter  layer2.0.bn2.num_batches_trackedlayer3.2.bn1.num_batches_tracked

Skipped loading parameterSkipped loading parameter  layer3.2.bn2.num_batches_trackedlayer2.0.bn3.num_batches_tracked

Skipped loading parameter layer3.2.bn3.num_batches_trackedSkipped loading parameter
 layer2.0.downsample.1.num_batches_tracked
Skipped loading parameter Skipped loading parameterlayer3.3.bn1.num_batches_tracked 
layer2.1.bn1.num_batches_tracked
Skipped loading parameter layer3.3.bn2.num_batches_trackedSkipped loading parameter
 layer2.1.bn2.num_batches_trackedSkipped loading parameter
 layer3.3.bn3.num_batches_tracked
Skipped loading parameterSkipped loading parameter  layer3.4.bn1.num_batches_trackedlayer2.1.bn3.num_batches_tracked

Skipped loading parameter layer3.4.bn2.num_batches_trackedSkipped loading parameter
 Skipped loading parameterlayer2.2.bn1.num_batches_tracked 
layer3.4.bn3.num_batches_tracked
Skipped loading parameterSkipped loading parameter  layer2.2.bn2.num_batches_trackedlayer3.5.bn1.num_batches_tracked

Skipped loading parameterSkipped loading parameter  layer2.2.bn3.num_batches_trackedlayer3.5.bn2.num_batches_tracked

Skipped loading parameterSkipped loading parameter  layer2.3.bn1.num_batches_trackedlayer3.5.bn3.num_batches_tracked

Skipped loading parameterSkipped loading parameter  layer2.3.bn2.num_batches_trackedlayer4.0.bn1.num_batches_tracked

Skipped loading parameterSkipped loading parameter  layer2.3.bn3.num_batches_trackedlayer4.0.bn2.num_batches_tracked

Skipped loading parameterSkipped loading parameter  layer3.0.bn1.num_batches_trackedlayer4.0.bn3.num_batches_tracked

Skipped loading parameterSkipped loading parameter  layer3.0.bn2.num_batches_trackedlayer4.0.downsample.1.num_batches_tracked

Skipped loading parameterSkipped loading parameter  layer3.0.bn3.num_batches_trackedlayer4.1.bn1.num_batches_tracked

Skipped loading parameterSkipped loading parameter  layer3.0.downsample.1.num_batches_trackedlayer4.1.bn2.num_batches_tracked

Skipped loading parameterSkipped loading parameter  layer3.1.bn1.num_batches_trackedlayer4.1.bn3.num_batches_tracked

Skipped loading parameterSkipped loading parameter  layer3.1.bn2.num_batches_trackedlayer4.2.bn1.num_batches_tracked

Skipped loading parameterSkipped loading parameter  layer3.1.bn3.num_batches_trackedlayer4.2.bn2.num_batches_tracked

Skipped loading parameterSkipped loading parameter  layer3.2.bn1.num_batches_trackedlayer4.2.bn3.num_batches_tracked

Skipped loading parameter layer3.2.bn2.num_batches_tracked
Skipped loading parameter layer3.2.bn3.num_batches_tracked
Skipped loading parameter layer3.3.bn1.num_batches_tracked
Skipped loading parameter layer3.3.bn2.num_batches_tracked
Skipped loading parameter layer3.3.bn3.num_batches_tracked
Skipped loading parameter layer3.4.bn1.num_batches_tracked
Skipped loading parameter layer3.4.bn2.num_batches_tracked
Skipped loading parameter layer3.4.bn3.num_batches_tracked
Skipped loading parameter layer3.5.bn1.num_batches_tracked
Skipped loading parameter layer3.5.bn2.num_batches_tracked
Skipped loading parameter layer3.5.bn3.num_batches_tracked
Skipped loading parameter layer4.0.bn1.num_batches_tracked
Skipped loading parameter layer4.0.bn2.num_batches_tracked
Skipped loading parameter layer4.0.bn3.num_batches_tracked
Skipped loading parameter layer4.0.downsample.1.num_batches_tracked
Skipped loading parameter layer4.1.bn1.num_batches_tracked
Skipped loading parameter layer4.1.bn2.num_batches_tracked
Skipped loading parameter layer4.1.bn3.num_batches_tracked
Skipped loading parameter layer4.2.bn1.num_batches_tracked
Skipped loading parameter layer4.2.bn2.num_batches_tracked
Skipped loading parameter layer4.2.bn3.num_batches_tracked
output_stride =  16
output_stride =  16
num_off_diagonal tensor(2016., device='cuda:1')
relax_denom == 0!!!!!
cluster ==  3
num_off_diagonal tensor(32640., device='cuda:1')
relax_denom == 0!!!!!
cluster ==  3
num_off_diagonal tensor(130816., device='cuda:1')
relax_denom == 0!!!!!
cluster ==  3
num_off_diagonal tensor(2016., device='cuda:0')
relax_denom == 0!!!!!
cluster ==  3
num_off_diagonal tensor(32640., device='cuda:0')
relax_denom == 0!!!!!
cluster ==  3
num_off_diagonal tensor(130816., device='cuda:0')
relax_denom == 0!!!!!
cluster ==  3
11-22 20:54:58.907 Model params = 45.081M
#### epoch: 0 #### iteration: 0
#### epoch: 0 #### iteration: 0
[W reducer.cpp:1050] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters. This flag results in an extra traversal of the autograd graph every iteration, which can adversely affect performance. If your model indeed never has any unused parameters, consider turning this flag off. Note that this warning may be a false positive your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1050] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters. This flag results in an extra traversal of the autograd graph every iteration, which can adversely affect performance. If your model indeed never has any unused parameters, consider turning this flag off. Note that this warning may be a false positive your model has flow control causing later iterations to have unused parameters. (function operator())
11-22 20:56:16.971 [epoch 0], [iter 50 / 1548 : 49], [loss 1.968689], [lr 0.009989], [time 0.2654]
Whitening Loss 0
11-22 20:57:21.715 [epoch 0], [iter 100 / 1548 : 99], [loss 1.247278], [lr 0.009978], [time 0.2560]
Whitening Loss 0
11-22 20:58:27.139 [epoch 0], [iter 150 / 1548 : 149], [loss 1.111867], [lr 0.009966], [time 0.2565]
Whitening Loss 0
11-22 20:59:31.687 [epoch 0], [iter 200 / 1548 : 199], [loss 0.943856], [lr 0.009955], [time 0.2584]
Whitening Loss 0
11-22 21:00:37.789 [epoch 0], [iter 250 / 1548 : 249], [loss 0.948765], [lr 0.009944], [time 0.2585]
Whitening Loss 0
11-22 21:01:40.264 [epoch 0], [iter 300 / 1548 : 299], [loss 0.975046], [lr 0.009933], [time 0.2558]
Whitening Loss 0
11-22 21:02:44.039 [epoch 0], [iter 350 / 1548 : 349], [loss 0.837051], [lr 0.009921], [time 0.2718]
Whitening Loss 0
11-22 21:03:48.939 [epoch 0], [iter 400 / 1548 : 399], [loss 0.774517], [lr 0.009910], [time 0.2694]
Whitening Loss 0
11-22 21:04:53.033 [epoch 0], [iter 450 / 1548 : 449], [loss 0.834384], [lr 0.009899], [time 0.2857]
Whitening Loss 0
11-22 21:05:57.843 [epoch 0], [iter 500 / 1548 : 499], [loss 0.806112], [lr 0.009888], [time 0.2602]
Whitening Loss 0
11-22 21:07:00.930 [epoch 0], [iter 550 / 1548 : 549], [loss 0.783197], [lr 0.009876], [time 0.2552]
Whitening Loss 0
Error!! (957, 526) (1052, 1914) 15188
Dropping  tensor(5673)
11-22 21:08:05.830 [epoch 0], [iter 600 / 1548 : 599], [loss 0.686663], [lr 0.009865], [time 0.2575]
Whitening Loss 0
11-22 21:09:13.178 [epoch 0], [iter 650 / 1548 : 649], [loss 0.679993], [lr 0.009854], [time 0.2561]
Whitening Loss 0
11-22 21:10:16.559 [epoch 0], [iter 700 / 1548 : 699], [loss 0.732446], [lr 0.009843], [time 0.2555]
Whitening Loss 0
11-22 21:11:19.631 [epoch 0], [iter 750 / 1548 : 749], [loss 0.723367], [lr 0.009831], [time 0.2562]
Whitening Loss 0
11-22 21:12:23.478 [epoch 0], [iter 800 / 1548 : 799], [loss 0.719374], [lr 0.009820], [time 0.2557]
Whitening Loss 0
11-22 21:13:29.278 [epoch 0], [iter 850 / 1548 : 849], [loss 0.708728], [lr 0.009809], [time 0.2555]
Whitening Loss 0
11-22 21:14:33.143 [epoch 0], [iter 900 / 1548 : 899], [loss 0.686996], [lr 0.009797], [time 0.2568]
Whitening Loss 0
11-22 21:15:35.805 [epoch 0], [iter 950 / 1548 : 949], [loss 0.639719], [lr 0.009786], [time 0.2572]
Whitening Loss 0
11-22 21:16:39.959 [epoch 0], [iter 1000 / 1548 : 999], [loss 0.639571], [lr 0.009775], [time 0.2566]
Whitening Loss 0
11-22 21:17:41.338 [epoch 0], [iter 1050 / 1548 : 1049], [loss 0.622102], [lr 0.009764], [time 0.2564]
Whitening Loss 0
11-22 21:18:45.214 [epoch 0], [iter 1100 / 1548 : 1099], [loss 0.635444], [lr 0.009752], [time 0.2641]
Whitening Loss 0
11-22 21:19:49.528 [epoch 0], [iter 1150 / 1548 : 1149], [loss 0.620567], [lr 0.009741], [time 0.3134]
Whitening Loss 0
11-22 21:20:53.646 [epoch 0], [iter 1200 / 1548 : 1199], [loss 0.637504], [lr 0.009730], [time 0.2846]
Whitening Loss 0
11-22 21:21:56.411 [epoch 0], [iter 1250 / 1548 : 1249], [loss 0.632355], [lr 0.009719], [time 0.2824]
Whitening Loss 0
11-22 21:22:58.825 [epoch 0], [iter 1300 / 1548 : 1299], [loss 0.594885], [lr 0.009707], [time 0.2806]
Whitening Loss 0
11-22 21:24:02.857 [epoch 0], [iter 1350 / 1548 : 1349], [loss 0.577505], [lr 0.009696], [time 0.2638]
Whitening Loss 0
11-22 21:25:06.905 [epoch 0], [iter 1400 / 1548 : 1399], [loss 0.572500], [lr 0.009685], [time 0.2964]
Whitening Loss 0
11-22 21:26:11.445 [epoch 0], [iter 1450 / 1548 : 1449], [loss 0.572073], [lr 0.009673], [time 0.2759]
Whitening Loss 0
11-22 21:27:15.303 [epoch 0], [iter 1500 / 1548 : 1499], [loss 0.559030], [lr 0.009662], [time 0.3061]
Whitening Loss 0
Saving pth file...
11-22 21:28:15.128 Saved file to ./logs/1122/r50os16_gtav_isw/11_22_20/last_None_epoch_0_mean-iu_0.00000.pth
11-22 21:28:15.129 Class Uniform Percentage: 0.5
11-22 21:28:15.129 Class Uniform items per Epoch:12388
11-22 21:28:15.132 cls 0 len 12109
11-22 21:28:15.132 cls 1 len 11833
11-22 21:28:15.132 cls 2 len 12301
11-22 21:28:15.132 cls 3 len 10854
11-22 21:28:15.132 cls 4 len 8811
11-22 21:28:15.132 cls 5 len 11928
11-22 21:28:15.132 cls 6 len 7891
11-22 21:28:15.132 cls 7 len 5921
11-22 21:28:15.132 cls 8 len 12132
11-22 21:28:15.132 cls 9 len 11549
11-22 21:28:15.132 cls 10 len 12131
11-22 21:28:15.132 cls 11 len 10691
11-22 21:28:15.132 cls 12 len 986
11-22 21:28:15.132 cls 13 len 10501
11-22 21:28:15.132 cls 14 len 6711
11-22 21:28:15.132 cls 15 len 1861
11-22 21:28:15.132 cls 16 len 493
11-22 21:28:15.132 cls 17 len 1211
11-22 21:28:15.132 cls 18 len 168
11-22 21:29:13.609 [epoch 1], [iter 50 / 1548 : 1597], [loss 0.542782], [lr 0.009640], [time 0.2498]
Whitening Loss 0
11-22 21:30:03.897 [epoch 1], [iter 100 / 1548 : 1647], [loss 0.576901], [lr 0.009629], [time 0.2468]
Whitening Loss 0
11-22 21:30:53.937 [epoch 1], [iter 150 / 1548 : 1697], [loss 0.572798], [lr 0.009617], [time 0.2454]
Whitening Loss 0
Error!! (957, 526) (1052, 1914) 15188
Dropping  tensor(1427)
11-22 21:31:44.060 [epoch 1], [iter 200 / 1548 : 1747], [loss 0.615329], [lr 0.009606], [time 0.2457]
Whitening Loss 0
11-22 21:32:34.411 [epoch 1], [iter 250 / 1548 : 1797], [loss 0.546719], [lr 0.009595], [time 0.2465]
Whitening Loss 0
11-22 21:33:24.482 [epoch 1], [iter 300 / 1548 : 1847], [loss 0.551288], [lr 0.009583], [time 0.2452]
Whitening Loss 0
11-22 21:34:14.867 [epoch 1], [iter 350 / 1548 : 1897], [loss 0.566876], [lr 0.009572], [time 0.2469]
Whitening Loss 0
11-22 21:35:05.439 [epoch 1], [iter 400 / 1548 : 1947], [loss 0.543172], [lr 0.009561], [time 0.2475]
Whitening Loss 0
11-22 21:35:55.499 [epoch 1], [iter 450 / 1548 : 1997], [loss 0.586218], [lr 0.009550], [time 0.2456]
Whitening Loss 0
11-22 21:36:45.553 [epoch 1], [iter 500 / 1548 : 2047], [loss 0.550990], [lr 0.009538], [time 0.2451]
Whitening Loss 0
11-22 21:37:35.824 [epoch 1], [iter 550 / 1548 : 2097], [loss 0.495848], [lr 0.009527], [time 0.2460]
Whitening Loss 0
11-22 21:38:26.183 [epoch 1], [iter 600 / 1548 : 2147], [loss 0.512797], [lr 0.009516], [time 0.2465]
Whitening Loss 0
11-22 21:39:16.437 [epoch 1], [iter 650 / 1548 : 2197], [loss 0.490781], [lr 0.009504], [time 0.2462]
Whitening Loss 0
11-22 21:40:06.583 [epoch 1], [iter 700 / 1548 : 2247], [loss 0.544497], [lr 0.009493], [time 0.2459]
Whitening Loss 0
11-22 21:40:56.741 [epoch 1], [iter 750 / 1548 : 2297], [loss 0.611156], [lr 0.009482], [time 0.2458]
Whitening Loss 0
11-22 21:41:46.868 [epoch 1], [iter 800 / 1548 : 2347], [loss 0.518448], [lr 0.009470], [time 0.2456]
Whitening Loss 0
11-22 21:42:36.925 [epoch 1], [iter 850 / 1548 : 2397], [loss 0.529631], [lr 0.009459], [time 0.2453]
Whitening Loss 0
11-22 21:43:26.915 [epoch 1], [iter 900 / 1548 : 2447], [loss 0.566656], [lr 0.009448], [time 0.2451]
Whitening Loss 0
11-22 21:44:17.053 [epoch 1], [iter 950 / 1548 : 2497], [loss 0.532648], [lr 0.009436], [time 0.2459]
Whitening Loss 0
11-22 21:45:07.343 [epoch 1], [iter 1000 / 1548 : 2547], [loss 0.509609], [lr 0.009425], [time 0.2466]
Whitening Loss 0
11-22 21:45:57.530 [epoch 1], [iter 1050 / 1548 : 2597], [loss 0.625081], [lr 0.009414], [time 0.2461]
Whitening Loss 0
11-22 21:46:47.548 [epoch 1], [iter 1100 / 1548 : 2647], [loss 0.549984], [lr 0.009402], [time 0.2453]
Whitening Loss 0
11-22 21:47:37.691 [epoch 1], [iter 1150 / 1548 : 2697], [loss 0.575927], [lr 0.009391], [time 0.2460]
Whitening Loss 0
11-22 21:48:27.792 [epoch 1], [iter 1200 / 1548 : 2747], [loss 0.531858], [lr 0.009380], [time 0.2455]
Whitening Loss 0
11-22 21:49:17.785 [epoch 1], [iter 1250 / 1548 : 2797], [loss 0.524721], [lr 0.009368], [time 0.2452]
Whitening Loss 0
11-22 21:50:07.757 [epoch 1], [iter 1300 / 1548 : 2847], [loss 0.503103], [lr 0.009357], [time 0.2449]
Whitening Loss 0
11-22 21:50:57.662 [epoch 1], [iter 1350 / 1548 : 2897], [loss 0.536574], [lr 0.009346], [time 0.2448]
Whitening Loss 0
11-22 21:51:48.060 [epoch 1], [iter 1400 / 1548 : 2947], [loss 0.539890], [lr 0.009334], [time 0.2468]
Whitening Loss 0
11-22 21:52:38.358 [epoch 1], [iter 1450 / 1548 : 2997], [loss 0.557154], [lr 0.009323], [time 0.2465]
Whitening Loss 0
11-22 21:53:28.284 [epoch 1], [iter 1500 / 1548 : 3047], [loss 0.534875], [lr 0.009312], [time 0.2452]
Whitening Loss 0
Saving pth file...
11-22 21:54:16.703 Saved file to ./logs/1122/r50os16_gtav_isw/11_22_20/last_None_epoch_1_mean-iu_0.00000.pth
11-22 21:54:16.704 Class Uniform Percentage: 0.5
11-22 21:54:16.705 Class Uniform items per Epoch:12388
11-22 21:54:16.707 cls 0 len 12109
11-22 21:54:16.708 cls 1 len 11833
11-22 21:54:16.708 cls 2 len 12301
11-22 21:54:16.708 cls 3 len 10854
11-22 21:54:16.708 cls 4 len 8811
11-22 21:54:16.708 cls 5 len 11928
11-22 21:54:16.708 cls 6 len 7891
11-22 21:54:16.708 cls 7 len 5921
11-22 21:54:16.708 cls 8 len 12132
11-22 21:54:16.708 cls 9 len 11549
11-22 21:54:16.708 cls 10 len 12131
11-22 21:54:16.708 cls 11 len 10691
11-22 21:54:16.708 cls 12 len 986
11-22 21:54:16.708 cls 13 len 10501
11-22 21:54:16.708 cls 14 len 6711
11-22 21:54:16.708 cls 15 len 1861
11-22 21:54:16.708 cls 16 len 493
11-22 21:54:16.708 cls 17 len 1211
11-22 21:54:16.708 cls 18 len 168
11-22 21:55:15.159 [epoch 2], [iter 50 / 1548 : 3145], [loss 0.522035], [lr 0.009290], [time 0.2487]
Whitening Loss 0
11-22 21:56:05.657 [epoch 2], [iter 100 / 1548 : 3195], [loss 0.516804], [lr 0.009278], [time 0.2478]
Whitening Loss 0
11-22 21:56:56.175 [epoch 2], [iter 150 / 1548 : 3245], [loss 0.499303], [lr 0.009267], [time 0.2479]
Whitening Loss 0
11-22 21:57:46.964 [epoch 2], [iter 200 / 1548 : 3295], [loss 0.468810], [lr 0.009255], [time 0.2489]
Whitening Loss 0
11-22 21:58:37.473 [epoch 2], [iter 250 / 1548 : 3345], [loss 0.494660], [lr 0.009244], [time 0.2476]
Whitening Loss 0
11-22 21:59:28.288 [epoch 2], [iter 300 / 1548 : 3395], [loss 0.455417], [lr 0.009233], [time 0.2496]
Whitening Loss 0
11-22 22:00:18.745 [epoch 2], [iter 350 / 1548 : 3445], [loss 0.476962], [lr 0.009221], [time 0.2476]
Whitening Loss 0
11-22 22:01:09.484 [epoch 2], [iter 400 / 1548 : 3495], [loss 0.469806], [lr 0.009210], [time 0.2483]
Whitening Loss 0
11-22 22:02:00.017 [epoch 2], [iter 450 / 1548 : 3545], [loss 0.508656], [lr 0.009199], [time 0.2479]
Whitening Loss 0
11-22 22:02:50.718 [epoch 2], [iter 500 / 1548 : 3595], [loss 0.478459], [lr 0.009187], [time 0.2486]
Whitening Loss 0
11-22 22:03:41.375 [epoch 2], [iter 550 / 1548 : 3645], [loss 0.453855], [lr 0.009176], [time 0.2486]
Whitening Loss 0
11-22 22:04:32.024 [epoch 2], [iter 600 / 1548 : 3695], [loss 0.469287], [lr 0.009165], [time 0.2486]
Whitening Loss 0
11-22 22:05:22.575 [epoch 2], [iter 650 / 1548 : 3745], [loss 0.499393], [lr 0.009153], [time 0.2480]
Whitening Loss 0
11-22 22:06:12.995 [epoch 2], [iter 700 / 1548 : 3795], [loss 0.464176], [lr 0.009142], [time 0.2475]
Whitening Loss 0
11-22 22:07:03.468 [epoch 2], [iter 750 / 1548 : 3845], [loss 0.464204], [lr 0.009131], [time 0.2476]
Whitening Loss 0
11-22 22:07:54.068 [epoch 2], [iter 800 / 1548 : 3895], [loss 0.434641], [lr 0.009119], [time 0.2483]
Whitening Loss 0
11-22 22:08:44.548 [epoch 2], [iter 850 / 1548 : 3945], [loss 0.484674], [lr 0.009108], [time 0.2477]
Whitening Loss 0
11-22 22:09:34.986 [epoch 2], [iter 900 / 1548 : 3995], [loss 0.530704], [lr 0.009096], [time 0.2476]
Whitening Loss 0
11-22 22:10:25.439 [epoch 2], [iter 950 / 1548 : 4045], [loss 0.448958], [lr 0.009085], [time 0.2477]
Whitening Loss 0
11-22 22:11:15.794 [epoch 2], [iter 1000 / 1548 : 4095], [loss 0.472365], [lr 0.009074], [time 0.2467]
Whitening Loss 0
11-22 22:12:06.228 [epoch 2], [iter 1050 / 1548 : 4145], [loss 0.494816], [lr 0.009062], [time 0.2476]
Whitening Loss 0
11-22 22:12:56.837 [epoch 2], [iter 1100 / 1548 : 4195], [loss 0.473339], [lr 0.009051], [time 0.2484]
Whitening Loss 0
11-22 22:13:47.147 [epoch 2], [iter 1150 / 1548 : 4245], [loss 0.413202], [lr 0.009040], [time 0.2470]
Whitening Loss 0
11-22 22:14:37.734 [epoch 2], [iter 1200 / 1548 : 4295], [loss 0.470093], [lr 0.009028], [time 0.2480]
Whitening Loss 0
11-22 22:15:28.296 [epoch 2], [iter 1250 / 1548 : 4345], [loss 0.419821], [lr 0.009017], [time 0.2481]
Whitening Loss 0
11-22 22:16:18.872 [epoch 2], [iter 1300 / 1548 : 4395], [loss 0.468272], [lr 0.009005], [time 0.2483]
Whitening Loss 0
Error!! (957, 526) (1052, 1914) 15188
Dropping  tensor(971)
11-22 22:17:09.378 [epoch 2], [iter 1350 / 1548 : 4445], [loss 0.457999], [lr 0.008994], [time 0.2480]
Whitening Loss 0
11-22 22:17:59.976 [epoch 2], [iter 1400 / 1548 : 4495], [loss 0.465047], [lr 0.008983], [time 0.2482]
Whitening Loss 0
11-22 22:18:50.603 [epoch 2], [iter 1450 / 1548 : 4545], [loss 0.429962], [lr 0.008971], [time 0.2485]
Whitening Loss 0
11-22 22:19:41.016 [epoch 2], [iter 1500 / 1548 : 4595], [loss 0.442235], [lr 0.008960], [time 0.2473]
Whitening Loss 0
Saving pth file...
11-22 22:20:29.669 Saved file to ./logs/1122/r50os16_gtav_isw/11_22_20/last_None_epoch_2_mean-iu_0.00000.pth
11-22 22:20:29.670 Class Uniform Percentage: 0.5
11-22 22:20:29.671 Class Uniform items per Epoch:12388
11-22 22:20:29.674 cls 0 len 12109
11-22 22:20:29.674 cls 1 len 11833
11-22 22:20:29.674 cls 2 len 12301
11-22 22:20:29.674 cls 3 len 10854
11-22 22:20:29.674 cls 4 len 8811
11-22 22:20:29.674 cls 5 len 11928
11-22 22:20:29.674 cls 6 len 7891
11-22 22:20:29.674 cls 7 len 5921
11-22 22:20:29.674 cls 8 len 12132
11-22 22:20:29.675 cls 9 len 11549
11-22 22:20:29.675 cls 10 len 12131
11-22 22:20:29.675 cls 11 len 10691
11-22 22:20:29.675 cls 12 len 986
11-22 22:20:29.675 cls 13 len 10501
11-22 22:20:29.675 cls 14 len 6711
11-22 22:20:29.675 cls 15 len 1861
11-22 22:20:29.675 cls 16 len 493
11-22 22:20:29.675 cls 17 len 1211
11-22 22:20:29.675 cls 18 len 168
11-22 22:21:28.838 [epoch 3], [iter 50 / 1548 : 4693], [loss 0.488768], [lr 0.008938], [time 0.2489]
Whitening Loss 0
11-22 22:22:19.635 [epoch 3], [iter 100 / 1548 : 4743], [loss 0.474957], [lr 0.008926], [time 0.2490]
Whitening Loss 0
11-22 22:23:10.513 [epoch 3], [iter 150 / 1548 : 4793], [loss 0.464643], [lr 0.008915], [time 0.2497]
Whitening Loss 0
11-22 22:24:01.173 [epoch 3], [iter 200 / 1548 : 4843], [loss 0.439963], [lr 0.008903], [time 0.2489]
Whitening Loss 0
11-22 22:24:51.588 [epoch 3], [iter 250 / 1548 : 4893], [loss 0.457265], [lr 0.008892], [time 0.2472]
Whitening Loss 0
11-22 22:25:42.224 [epoch 3], [iter 300 / 1548 : 4943], [loss 0.487048], [lr 0.008881], [time 0.2483]
Whitening Loss 0
11-22 22:26:32.605 [epoch 3], [iter 350 / 1548 : 4993], [loss 0.476735], [lr 0.008869], [time 0.2474]
Whitening Loss 0
11-22 22:27:22.997 [epoch 3], [iter 400 / 1548 : 5043], [loss 0.471805], [lr 0.008858], [time 0.2472]
Whitening Loss 0
11-22 22:28:13.503 [epoch 3], [iter 450 / 1548 : 5093], [loss 0.417822], [lr 0.008846], [time 0.2483]
Whitening Loss 0
11-22 22:29:04.136 [epoch 3], [iter 500 / 1548 : 5143], [loss 0.422910], [lr 0.008835], [time 0.2484]
Whitening Loss 0
11-22 22:29:54.499 [epoch 3], [iter 550 / 1548 : 5193], [loss 0.454179], [lr 0.008824], [time 0.2475]
Whitening Loss 0
11-22 22:30:45.053 [epoch 3], [iter 600 / 1548 : 5243], [loss 0.459055], [lr 0.008812], [time 0.2480]
Whitening Loss 0
11-22 22:31:35.515 [epoch 3], [iter 650 / 1548 : 5293], [loss 0.446879], [lr 0.008801], [time 0.2475]
Whitening Loss 0
11-22 22:32:26.223 [epoch 3], [iter 700 / 1548 : 5343], [loss 0.407761], [lr 0.008789], [time 0.2491]
Whitening Loss 0
11-22 22:33:16.485 [epoch 3], [iter 750 / 1548 : 5393], [loss 0.390268], [lr 0.008778], [time 0.2470]
Whitening Loss 0
11-22 22:34:06.810 [epoch 3], [iter 800 / 1548 : 5443], [loss 0.472379], [lr 0.008767], [time 0.2472]
Whitening Loss 0
11-22 22:34:57.241 [epoch 3], [iter 850 / 1548 : 5493], [loss 0.425747], [lr 0.008755], [time 0.2475]
Whitening Loss 0
11-22 22:35:47.640 [epoch 3], [iter 900 / 1548 : 5543], [loss 0.411994], [lr 0.008744], [time 0.2472]
Whitening Loss 0
11-22 22:36:38.245 [epoch 3], [iter 950 / 1548 : 5593], [loss 0.424990], [lr 0.008732], [time 0.2483]
Whitening Loss 0
11-22 22:37:28.643 [epoch 3], [iter 1000 / 1548 : 5643], [loss 0.402385], [lr 0.008721], [time 0.2474]
Whitening Loss 0
11-22 22:38:19.106 [epoch 3], [iter 1050 / 1548 : 5693], [loss 0.417790], [lr 0.008709], [time 0.2476]
Whitening Loss 0
11-22 22:39:09.639 [epoch 3], [iter 1100 / 1548 : 5743], [loss 0.394730], [lr 0.008698], [time 0.2482]
Whitening Loss 0
11-22 22:40:00.193 [epoch 3], [iter 1150 / 1548 : 5793], [loss 0.432425], [lr 0.008687], [time 0.2480]
Whitening Loss 0
11-22 22:40:50.790 [epoch 3], [iter 1200 / 1548 : 5843], [loss 0.434451], [lr 0.008675], [time 0.2483]
Whitening Loss 0
11-22 22:41:41.337 [epoch 3], [iter 1250 / 1548 : 5893], [loss 0.475724], [lr 0.008664], [time 0.2478]
Whitening Loss 0
11-22 22:42:31.775 [epoch 3], [iter 1300 / 1548 : 5943], [loss 0.409493], [lr 0.008652], [time 0.2475]
Whitening Loss 0
11-22 22:43:22.269 [epoch 3], [iter 1350 / 1548 : 5993], [loss 0.447264], [lr 0.008641], [time 0.2477]
Whitening Loss 0
11-22 22:44:12.839 [epoch 3], [iter 1400 / 1548 : 6043], [loss 0.470734], [lr 0.008629], [time 0.2484]
Whitening Loss 0
11-22 22:45:03.379 [epoch 3], [iter 1450 / 1548 : 6093], [loss 0.484327], [lr 0.008618], [time 0.2481]
Whitening Loss 0
11-22 22:45:53.972 [epoch 3], [iter 1500 / 1548 : 6143], [loss 0.458144], [lr 0.008607], [time 0.2480]
Whitening Loss 0
Saving pth file...
11-22 22:46:42.855 Saved file to ./logs/1122/r50os16_gtav_isw/11_22_20/last_None_epoch_3_mean-iu_0.00000.pth
11-22 22:46:42.856 Class Uniform Percentage: 0.5
11-22 22:46:42.856 Class Uniform items per Epoch:12388
11-22 22:46:42.860 cls 0 len 12109
11-22 22:46:42.860 cls 1 len 11833
11-22 22:46:42.860 cls 2 len 12301
11-22 22:46:42.860 cls 3 len 10854
11-22 22:46:42.860 cls 4 len 8811
11-22 22:46:42.860 cls 5 len 11928
11-22 22:46:42.860 cls 6 len 7891
11-22 22:46:42.860 cls 7 len 5921
11-22 22:46:42.860 cls 8 len 12132
11-22 22:46:42.860 cls 9 len 11549
11-22 22:46:42.860 cls 10 len 12131
11-22 22:46:42.860 cls 11 len 10691
11-22 22:46:42.860 cls 12 len 986
11-22 22:46:42.861 cls 13 len 10501
11-22 22:46:42.861 cls 14 len 6711
11-22 22:46:42.861 cls 15 len 1861
11-22 22:46:42.861 cls 16 len 493
11-22 22:46:42.861 cls 17 len 1211
11-22 22:46:42.861 cls 18 len 168
Error!! (957, 526) (1052, 1914) 15188
Dropping  tensor(10746)
11-22 22:47:41.377 [epoch 4], [iter 50 / 1548 : 6241], [loss 0.432074], [lr 0.008584], [time 0.2486]
Whitening Loss 0
11-22 22:48:31.769 [epoch 4], [iter 100 / 1548 : 6291], [loss 0.435193], [lr 0.008573], [time 0.2471]
Whitening Loss 0
11-22 22:49:22.390 [epoch 4], [iter 150 / 1548 : 6341], [loss 0.397847], [lr 0.008561], [time 0.2481]
Whitening Loss 0
11-22 22:50:13.134 [epoch 4], [iter 200 / 1548 : 6391], [loss 0.364612], [lr 0.008550], [time 0.2483]
Whitening Loss 0
11-22 22:51:03.782 [epoch 4], [iter 250 / 1548 : 6441], [loss 0.419323], [lr 0.008538], [time 0.2484]
Whitening Loss 0
11-22 22:51:54.212 [epoch 4], [iter 300 / 1548 : 6491], [loss 0.426412], [lr 0.008527], [time 0.2473]
Whitening Loss 0
11-22 22:52:44.660 [epoch 4], [iter 350 / 1548 : 6541], [loss 0.488665], [lr 0.008515], [time 0.2476]
Whitening Loss 0
11-22 22:53:35.009 [epoch 4], [iter 400 / 1548 : 6591], [loss 0.387603], [lr 0.008504], [time 0.2471]
Whitening Loss 0
11-22 22:54:25.192 [epoch 4], [iter 450 / 1548 : 6641], [loss 0.412751], [lr 0.008493], [time 0.2459]
Whitening Loss 0
11-22 22:55:15.571 [epoch 4], [iter 500 / 1548 : 6691], [loss 0.394342], [lr 0.008481], [time 0.2471]
Whitening Loss 0
11-22 22:56:06.085 [epoch 4], [iter 550 / 1548 : 6741], [loss 0.442593], [lr 0.008470], [time 0.2477]
Whitening Loss 0
11-22 22:56:56.427 [epoch 4], [iter 600 / 1548 : 6791], [loss 0.398601], [lr 0.008458], [time 0.2471]
Whitening Loss 0
11-22 22:57:46.813 [epoch 4], [iter 650 / 1548 : 6841], [loss 0.398798], [lr 0.008447], [time 0.2473]
Whitening Loss 0
11-22 22:58:37.485 [epoch 4], [iter 700 / 1548 : 6891], [loss 0.445532], [lr 0.008435], [time 0.2485]
Whitening Loss 0
11-22 22:59:27.854 [epoch 4], [iter 750 / 1548 : 6941], [loss 0.366807], [lr 0.008424], [time 0.2473]
Whitening Loss 0
11-22 23:00:18.271 [epoch 4], [iter 800 / 1548 : 6991], [loss 0.442551], [lr 0.008412], [time 0.2474]
Whitening Loss 0
11-22 23:01:08.767 [epoch 4], [iter 850 / 1548 : 7041], [loss 0.440892], [lr 0.008401], [time 0.2478]
Whitening Loss 0
11-22 23:01:59.110 [epoch 4], [iter 900 / 1548 : 7091], [loss 0.416635], [lr 0.008389], [time 0.2468]
Whitening Loss 0
Error!! (957, 526) (1052, 1914) 15188
Dropping  tensor(2546)
11-22 23:02:49.494 [epoch 4], [iter 950 / 1548 : 7141], [loss 0.446226], [lr 0.008378], [time 0.2470]
Whitening Loss 0
11-22 23:03:39.979 [epoch 4], [iter 1000 / 1548 : 7191], [loss 0.442301], [lr 0.008366], [time 0.2475]
Whitening Loss 0
11-22 23:04:30.477 [epoch 4], [iter 1050 / 1548 : 7241], [loss 0.458430], [lr 0.008355], [time 0.2478]
Whitening Loss 0
11-22 23:05:20.981 [epoch 4], [iter 1100 / 1548 : 7291], [loss 0.409346], [lr 0.008343], [time 0.2476]
Whitening Loss 0
11-22 23:06:11.453 [epoch 4], [iter 1150 / 1548 : 7341], [loss 0.403815], [lr 0.008332], [time 0.2475]
Whitening Loss 0
11-22 23:07:01.893 [epoch 4], [iter 1200 / 1548 : 7391], [loss 0.383946], [lr 0.008321], [time 0.2472]
Whitening Loss 0
11-22 23:07:52.413 [epoch 4], [iter 1250 / 1548 : 7441], [loss 0.390169], [lr 0.008309], [time 0.2481]
Whitening Loss 0
11-22 23:08:42.982 [epoch 4], [iter 1300 / 1548 : 7491], [loss 0.415966], [lr 0.008298], [time 0.2481]
Whitening Loss 0
11-22 23:09:33.492 [epoch 4], [iter 1350 / 1548 : 7541], [loss 0.439819], [lr 0.008286], [time 0.2478]
Whitening Loss 0
11-22 23:10:23.865 [epoch 4], [iter 1400 / 1548 : 7591], [loss 0.423539], [lr 0.008275], [time 0.2471]
Whitening Loss 0
11-22 23:11:14.140 [epoch 4], [iter 1450 / 1548 : 7641], [loss 0.411341], [lr 0.008263], [time 0.2466]
Whitening Loss 0
11-22 23:12:04.716 [epoch 4], [iter 1500 / 1548 : 7691], [loss 0.450204], [lr 0.008252], [time 0.2481]
Whitening Loss 0
Saving pth file...
11-22 23:12:53.629 Saved file to ./logs/1122/r50os16_gtav_isw/11_22_20/last_None_epoch_4_mean-iu_0.00000.pth
11-22 23:12:53.631 Class Uniform Percentage: 0.5
11-22 23:12:53.631 Class Uniform items per Epoch:12388
11-22 23:12:53.635 cls 0 len 12109
11-22 23:12:53.635 cls 1 len 11833
11-22 23:12:53.635 cls 2 len 12301
11-22 23:12:53.635 cls 3 len 10854
11-22 23:12:53.635 cls 4 len 8811
11-22 23:12:53.635 cls 5 len 11928
11-22 23:12:53.635 cls 6 len 7891
11-22 23:12:53.635 cls 7 len 5921
11-22 23:12:53.635 cls 8 len 12132
11-22 23:12:53.636 cls 9 len 11549
11-22 23:12:53.636 cls 10 len 12131
11-22 23:12:53.636 cls 11 len 10691
11-22 23:12:53.636 cls 12 len 986
11-22 23:12:53.636 cls 13 len 10501
11-22 23:12:53.636 cls 14 len 6711
11-22 23:12:53.636 cls 15 len 1861
11-22 23:12:53.636 cls 16 len 493
11-22 23:12:53.636 cls 17 len 1211
11-22 23:12:53.636 cls 18 len 168
11-22 23:13:53.291 [epoch 5], [iter 50 / 1548 : 7789], [loss 0.461402], [lr 0.008229], [time 0.2492]
Whitening Loss 0
11-22 23:14:44.237 [epoch 5], [iter 100 / 1548 : 7839], [loss 0.434666], [lr 0.008218], [time 0.2495]
Whitening Loss 0
11-22 23:15:34.925 [epoch 5], [iter 150 / 1548 : 7889], [loss 0.388159], [lr 0.008206], [time 0.2488]
Whitening Loss 0
11-22 23:16:25.436 [epoch 5], [iter 200 / 1548 : 7939], [loss 0.395928], [lr 0.008195], [time 0.2477]
Whitening Loss 0
11-22 23:17:15.939 [epoch 5], [iter 250 / 1548 : 7989], [loss 0.383803], [lr 0.008183], [time 0.2474]
Whitening Loss 0
11-22 23:18:06.659 [epoch 5], [iter 300 / 1548 : 8039], [loss 0.391115], [lr 0.008172], [time 0.2486]
Whitening Loss 0
11-22 23:18:57.419 [epoch 5], [iter 350 / 1548 : 8089], [loss 0.434229], [lr 0.008160], [time 0.2489]
Whitening Loss 0
11-22 23:19:47.824 [epoch 5], [iter 400 / 1548 : 8139], [loss 0.437851], [lr 0.008149], [time 0.2471]
Whitening Loss 0
11-22 23:20:38.360 [epoch 5], [iter 450 / 1548 : 8189], [loss 0.402519], [lr 0.008137], [time 0.2477]
Whitening Loss 0
11-22 23:21:28.559 [epoch 5], [iter 500 / 1548 : 8239], [loss 0.402638], [lr 0.008126], [time 0.2464]
Whitening Loss 0
11-22 23:22:19.207 [epoch 5], [iter 550 / 1548 : 8289], [loss 0.449652], [lr 0.008114], [time 0.2481]
Whitening Loss 0
11-22 23:23:09.767 [epoch 5], [iter 600 / 1548 : 8339], [loss 0.443921], [lr 0.008102], [time 0.2479]
Whitening Loss 0
11-22 23:24:00.323 [epoch 5], [iter 650 / 1548 : 8389], [loss 0.390624], [lr 0.008091], [time 0.2479]
Whitening Loss 0
11-22 23:24:50.926 [epoch 5], [iter 700 / 1548 : 8439], [loss 0.388914], [lr 0.008079], [time 0.2481]
Whitening Loss 0
11-22 23:25:41.544 [epoch 5], [iter 750 / 1548 : 8489], [loss 0.403754], [lr 0.008068], [time 0.2483]
Whitening Loss 0
11-22 23:26:32.017 [epoch 5], [iter 800 / 1548 : 8539], [loss 0.367794], [lr 0.008056], [time 0.2473]
Whitening Loss 0
11-22 23:27:22.751 [epoch 5], [iter 850 / 1548 : 8589], [loss 0.395302], [lr 0.008045], [time 0.2487]
Whitening Loss 0
11-22 23:28:13.179 [epoch 5], [iter 900 / 1548 : 8639], [loss 0.437273], [lr 0.008033], [time 0.2473]
Whitening Loss 0
Error!! (957, 526) (1052, 1914) 15188
Dropping  tensor(4193)
11-22 23:29:03.707 [epoch 5], [iter 950 / 1548 : 8689], [loss 0.427981], [lr 0.008022], [time 0.2477]
Whitening Loss 0
11-22 23:29:54.039 [epoch 5], [iter 1000 / 1548 : 8739], [loss 0.404951], [lr 0.008010], [time 0.2470]
Whitening Loss 0
11-22 23:30:44.691 [epoch 5], [iter 1050 / 1548 : 8789], [loss 0.418484], [lr 0.007999], [time 0.2481]
Whitening Loss 0
11-22 23:31:35.261 [epoch 5], [iter 1100 / 1548 : 8839], [loss 0.462981], [lr 0.007987], [time 0.2481]
Whitening Loss 0
11-22 23:32:25.770 [epoch 5], [iter 1150 / 1548 : 8889], [loss 0.391704], [lr 0.007976], [time 0.2475]
Whitening Loss 0
11-22 23:33:16.418 [epoch 5], [iter 1200 / 1548 : 8939], [loss 0.381530], [lr 0.007964], [time 0.2484]
Whitening Loss 0
11-22 23:34:07.213 [epoch 5], [iter 1250 / 1548 : 8989], [loss 0.421658], [lr 0.007953], [time 0.2490]
Whitening Loss 0
11-22 23:34:57.902 [epoch 5], [iter 1300 / 1548 : 9039], [loss 0.388544], [lr 0.007941], [time 0.2488]
Whitening Loss 0
11-22 23:35:48.464 [epoch 5], [iter 1350 / 1548 : 9089], [loss 0.455307], [lr 0.007930], [time 0.2479]
Whitening Loss 0
11-22 23:36:39.104 [epoch 5], [iter 1400 / 1548 : 9139], [loss 0.404827], [lr 0.007918], [time 0.2484]
Whitening Loss 0
11-22 23:37:29.471 [epoch 5], [iter 1450 / 1548 : 9189], [loss 0.376626], [lr 0.007906], [time 0.2467]
Whitening Loss 0
11-22 23:38:20.146 [epoch 5], [iter 1500 / 1548 : 9239], [loss 0.366049], [lr 0.007895], [time 0.2487]
Whitening Loss 0
11-22 23:39:11.130 validating: 1 / 100
11-22 23:39:20.944 validating: 21 / 100
11-22 23:39:31.018 validating: 41 / 100
11-22 23:39:40.526 validating: 61 / 100
11-22 23:39:50.511 validating: 81 / 100
11-22 23:40:00.522 validating: 101 / 100
11-22 23:40:10.066 validating: 121 / 100
11-22 23:40:20.150 validating: 141 / 100
11-22 23:40:29.655 validating: 161 / 100
11-22 23:40:39.429 validating: 181 / 100
11-22 23:40:49.127 validating: 201 / 100
11-22 23:40:59.044 validating: 221 / 100
11-22 23:41:09.468 validating: 241 / 100
11-22 23:41:19.365 validating: 261 / 100
11-22 23:41:29.792 validating: 281 / 100
11-22 23:41:39.374 validating: 301 / 100
11-22 23:41:49.604 validating: 321 / 100
11-22 23:41:59.522 validating: 341 / 100
11-22 23:42:09.312 validating: 361 / 100
11-22 23:42:19.327 validating: 381 / 100
11-22 23:42:29.117 validating: 401 / 100
11-22 23:42:39.206 validating: 421 / 100
11-22 23:42:48.916 validating: 441 / 100
11-22 23:42:59.131 validating: 461 / 100
11-22 23:43:08.721 validating: 481 / 100
num_sensitive, centroids = 310 [0.0033912447211299646, 0.05627849766690477, 0.39055689175923664]
Check whether two ints are same 310 tensor(310., device='cuda:1')
num_sensitive, centroids = 3100 [0.001559801018329033, 0.025485989707676255, 0.14625180335715413]
Check whether two ints are same 3100 tensor(3100., device='cuda:1')
num_sensitive, centroids = 26609 [0.00026204315133637556, 0.0027504930164988445, 0.011387402841743706]
Check whether two ints are same 26609 tensor(26609., device='cuda:1')
num_sensitive, centroids = 310 [0.0033912447211299646, 0.05627849766690477, 0.39055689175923664]
Check whether two ints are same 310 tensor(310., device='cuda:0')
Covariance Info: (CXC Shape, Num_Off_Diagonal) torch.Size([64, 64]) tensor(2016., device='cuda:0')
Selective (Sensitive Covariance) tensor(310., device='cuda:0')
num_sensitive, centroids = 3100 [0.001559801018329033, 0.025485989707676255, 0.14625180335715413]
Check whether two ints are same 3100 tensor(3100., device='cuda:0')
Covariance Info: (CXC Shape, Num_Off_Diagonal) torch.Size([256, 256]) tensor(32640., device='cuda:0')
Selective (Sensitive Covariance) tensor(3100., device='cuda:0')
num_sensitive, centroids = 26609 [0.00026204315133637556, 0.0027504930164988445, 0.011387402841743706]
Check whether two ints are same 26609 tensor(26609., device='cuda:0')
Covariance Info: (CXC Shape, Num_Off_Diagonal) torch.Size([512, 512]) tensor(130816., device='cuda:0')
Selective (Sensitive Covariance) tensor(26609., device='cuda:0')
11-22 23:43:28.363 validating: 1 / 100
11-22 23:43:38.856 validating: 21 / 100
11-22 23:43:49.098 validating: 41 / 100
11-22 23:43:59.227 validating: 61 / 100
11-22 23:44:09.543 validating: 81 / 100
11-22 23:44:19.531 validating: 101 / 100
11-22 23:44:30.016 validating: 121 / 100
11-22 23:44:40.338 validating: 141 / 100
11-22 23:44:50.774 validating: 161 / 100
11-22 23:45:01.347 validating: 181 / 100
11-22 23:45:11.795 validating: 201 / 100
11-22 23:45:22.484 validating: 221 / 100
11-22 23:45:33.070 validating: 241 / 100
11-22 23:45:43.443 validating: 261 / 100
11-22 23:45:53.327 validating: 281 / 100
11-22 23:46:04.028 validating: 301 / 100
11-22 23:46:14.212 validating: 321 / 100
11-22 23:46:24.355 validating: 341 / 100
11-22 23:46:34.485 validating: 361 / 100
11-22 23:46:44.858 validating: 381 / 100
11-22 23:46:55.119 validating: 401 / 100
11-22 23:47:05.366 validating: 421 / 100
11-22 23:47:15.854 validating: 441 / 100
11-22 23:47:26.563 validating: 461 / 100
11-22 23:47:37.040 validating: 481 / 100
num_sensitive, centroids = 310 [0.003196125975403496, 0.05191907932729808, 0.36361900397709435]
Check whether two ints are same 310 tensor(299., device='cuda:1')
num_sensitive, centroids = 3118 [0.0014931646199774706, 0.024390775679443033, 0.15073009177639676]
Check whether two ints are same 3118 tensor(2964., device='cuda:1')
num_sensitive, centroids = 25687 [0.00025358021002085503, 0.0026856907545456457, 0.011309852936664163]
Check whether two ints are same 25687 tensor(24731., device='cuda:1')
num_sensitive, centroids = 310 [0.003196125975403496, 0.05191907932729808, 0.36361900397709435]
Check whether two ints are same 310 tensor(299., device='cuda:0')
Covariance Info: (CXC Shape, Num_Off_Diagonal) torch.Size([64, 64]) tensor(2016., device='cuda:0')
Selective (Sensitive Covariance) tensor(299., device='cuda:0')
num_sensitive, centroids = 3118 [0.0014931646199774706, 0.024390775679443033, 0.15073009177639676]
Check whether two ints are same 3118 tensor(2964., device='cuda:0')
Covariance Info: (CXC Shape, Num_Off_Diagonal) torch.Size([256, 256]) tensor(32640., device='cuda:0')
Selective (Sensitive Covariance) tensor(2964., device='cuda:0')
num_sensitive, centroids = 25687 [0.00025358021002085503, 0.0026856907545456457, 0.011309852936664163]
Check whether two ints are same 25687 tensor(24731., device='cuda:0')
Covariance Info: (CXC Shape, Num_Off_Diagonal) torch.Size([512, 512]) tensor(130816., device='cuda:0')
Selective (Sensitive Covariance) tensor(24731., device='cuda:0')
11-22 23:47:54.399 validating: 1 / 100
11-22 23:48:04.471 validating: 21 / 100
11-22 23:48:14.602 validating: 41 / 100
11-22 23:48:25.060 validating: 61 / 100
11-22 23:48:35.119 validating: 81 / 100
11-22 23:48:45.396 validating: 101 / 100
11-22 23:48:55.239 validating: 121 / 100
11-22 23:49:06.021 validating: 141 / 100
11-22 23:49:16.141 validating: 161 / 100
11-22 23:49:26.966 validating: 181 / 100
11-22 23:49:36.828 validating: 201 / 100
11-22 23:49:46.969 validating: 221 / 100
11-22 23:49:56.919 validating: 241 / 100
11-22 23:50:07.037 validating: 261 / 100
11-22 23:50:17.668 validating: 281 / 100
11-22 23:50:28.227 validating: 301 / 100
11-22 23:50:38.096 validating: 321 / 100
11-22 23:50:48.220 validating: 341 / 100
11-22 23:50:58.678 validating: 361 / 100
11-22 23:51:08.416 validating: 381 / 100
11-22 23:51:18.351 validating: 401 / 100
11-22 23:51:28.319 validating: 421 / 100
11-22 23:51:38.464 validating: 441 / 100
11-22 23:51:48.734 validating: 461 / 100
num_sensitive, centroids = 333 [0.002871193113195989, 0.04860475802211309, 0.3395483408655439]
Check whether two ints are same 333 tensor(296., device='cuda:1')
11-22 23:51:58.972 validating: 481 / 100
num_sensitive, centroids = 3007 [0.0014593350339845985, 0.02414397359078704, 0.14175609199257638]
Check whether two ints are same 3007 tensor(2852., device='cuda:1')
num_sensitive, centroids = 24896 [0.0002512768477166552, 0.0026868934171881786, 0.011409973815453814]
Check whether two ints are same 24896 tensor(23584., device='cuda:1')
num_sensitive, centroids = 333 [0.002871193113195989, 0.04860475802211309, 0.3395483408655439]
Check whether two ints are same 333 tensor(296., device='cuda:0')
Covariance Info: (CXC Shape, Num_Off_Diagonal) torch.Size([64, 64]) tensor(2016., device='cuda:0')
Selective (Sensitive Covariance) tensor(296., device='cuda:0')
num_sensitive, centroids = 3007 [0.0014593350339845985, 0.02414397359078704, 0.14175609199257638]
Check whether two ints are same 3007 tensor(2852., device='cuda:0')
Covariance Info: (CXC Shape, Num_Off_Diagonal) torch.Size([256, 256]) tensor(32640., device='cuda:0')
Selective (Sensitive Covariance) tensor(2852., device='cuda:0')
num_sensitive, centroids = 24896 [0.0002512768477166552, 0.0026868934171881786, 0.011409973815453814]
Check whether two ints are same 24896 tensor(23584., device='cuda:0')
Covariance Info: (CXC Shape, Num_Off_Diagonal) torch.Size([512, 512]) tensor(130816., device='cuda:0')
Selective (Sensitive Covariance) tensor(23584., device='cuda:0')
11-22 23:52:18.458 validating: 1 / 100
11-22 23:52:28.700 validating: 21 / 100
11-22 23:52:38.395 validating: 41 / 100
11-22 23:52:48.149 validating: 61 / 100
11-22 23:52:57.643 validating: 81 / 100
11-22 23:53:07.839 validating: 101 / 100
11-22 23:53:17.996 validating: 121 / 100
11-22 23:53:27.705 validating: 141 / 100
11-22 23:53:38.063 validating: 161 / 100
11-22 23:53:47.791 validating: 181 / 100
11-22 23:53:57.591 validating: 201 / 100
11-22 23:54:07.760 validating: 221 / 100
11-22 23:54:17.316 validating: 241 / 100
11-22 23:54:27.630 validating: 261 / 100
11-22 23:54:37.145 validating: 281 / 100
11-22 23:54:46.974 validating: 301 / 100
11-22 23:54:56.765 validating: 321 / 100
11-22 23:55:06.655 validating: 341 / 100
11-22 23:55:16.386 validating: 361 / 100
11-22 23:55:25.961 validating: 381 / 100
11-22 23:55:36.017 validating: 401 / 100
11-22 23:55:45.721 validating: 421 / 100
11-22 23:55:55.617 validating: 441 / 100
11-22 23:56:05.292 validating: 461 / 100
num_sensitive, centroids = 294 [0.0034314681384082077, 0.0587752079364003, 0.3792755646365029]
Check whether two ints are same 294 tensor(283., device='cuda:1')
num_sensitive, centroids = 2994 [0.0015697149279519462, 0.02596550045287297, 0.15255357079992168]
Check whether two ints are same 2994 tensor(2752., device='cuda:1')
num_sensitive, centroids = 26949 [0.0002613861464700273, 0.002694102532285981, 0.011354670224482435]
Check whether two ints are same 26949 tensor(23343., device='cuda:1')
11-22 23:56:14.863 validating: 481 / 100
num_sensitive, centroids = 294 [0.0034314681384082077, 0.0587752079364003, 0.3792755646365029]
Check whether two ints are same 294 tensor(283., device='cuda:0')
Covariance Info: (CXC Shape, Num_Off_Diagonal) torch.Size([64, 64]) tensor(2016., device='cuda:0')
Selective (Sensitive Covariance) tensor(283., device='cuda:0')
num_sensitive, centroids = 2994 [0.0015697149279519462, 0.02596550045287297, 0.15255357079992168]
Check whether two ints are same 2994 tensor(2752., device='cuda:0')
Covariance Info: (CXC Shape, Num_Off_Diagonal) torch.Size([256, 256]) tensor(32640., device='cuda:0')
Selective (Sensitive Covariance) tensor(2752., device='cuda:0')
num_sensitive, centroids = 26949 [0.0002613861464700273, 0.002694102532285981, 0.011354670224482435]
Check whether two ints are same 26949 tensor(23343., device='cuda:0')
Covariance Info: (CXC Shape, Num_Off_Diagonal) torch.Size([512, 512]) tensor(130816., device='cuda:0')
Selective (Sensitive Covariance) tensor(23343., device='cuda:0')
11-22 23:56:32.983 validating: 1 / 100
11-22 23:56:43.364 validating: 21 / 100
11-22 23:56:53.152 validating: 41 / 100
11-22 23:57:02.702 validating: 61 / 100
11-22 23:57:11.407 validating: 81 / 100
11-22 23:57:20.216 validating: 101 / 100
11-22 23:57:29.161 validating: 121 / 100
11-22 23:57:37.952 validating: 141 / 100
11-22 23:57:46.813 validating: 161 / 100
11-22 23:57:55.802 validating: 181 / 100
11-22 23:58:04.883 validating: 201 / 100
11-22 23:58:13.822 validating: 221 / 100
11-22 23:58:22.907 validating: 241 / 100
11-22 23:58:31.971 validating: 261 / 100
11-22 23:58:40.974 validating: 281 / 100
11-22 23:58:50.030 validating: 301 / 100
11-22 23:58:58.950 validating: 321 / 100
11-22 23:59:07.875 validating: 341 / 100
11-22 23:59:16.621 validating: 361 / 100
11-22 23:59:25.522 validating: 381 / 100
11-22 23:59:34.491 validating: 401 / 100
11-22 23:59:43.460 validating: 421 / 100
11-22 23:59:52.531 validating: 441 / 100
num_sensitive, centroids = 337 [0.0032862392244892495, 0.05200453482455378, 0.34708629122802187]
Check whether two ints are same 337 tensor(282., device='cuda:1')
num_sensitive, centroids = 3264 [0.0015151136858828108, 0.02396259078504809, 0.133514412244161]
Check whether two ints are same 3264 tensor(2730., device='cuda:1')
11-23 00:00:01.576 validating: 461 / 100
num_sensitive, centroids = 24851 [0.00025310531622891143, 0.0026992897269778114, 0.011435400124754174]
Check whether two ints are same 24851 tensor(22629., device='cuda:1')
11-23 00:00:10.776 validating: 481 / 100
num_sensitive, centroids = 337 [0.0032862392244892495, 0.05200453482455378, 0.34708629122802187]
Check whether two ints are same 337 tensor(282., device='cuda:0')
Covariance Info: (CXC Shape, Num_Off_Diagonal) torch.Size([64, 64]) tensor(2016., device='cuda:0')
Selective (Sensitive Covariance) tensor(282., device='cuda:0')
num_sensitive, centroids = 3264 [0.0015151136858828108, 0.02396259078504809, 0.133514412244161]
Check whether two ints are same 3264 tensor(2730., device='cuda:0')
Covariance Info: (CXC Shape, Num_Off_Diagonal) torch.Size([256, 256]) tensor(32640., device='cuda:0')
Selective (Sensitive Covariance) tensor(2730., device='cuda:0')
num_sensitive, centroids = 24851 [0.00025310531622891143, 0.0026992897269778114, 0.011435400124754174]
Check whether two ints are same 24851 tensor(22629., device='cuda:0')
Covariance Info: (CXC Shape, Num_Off_Diagonal) torch.Size([512, 512]) tensor(130816., device='cuda:0')
Selective (Sensitive Covariance) tensor(22629., device='cuda:0')
11-23 00:00:28.007 validating: 1 / 100
11-23 00:00:37.157 validating: 21 / 100
11-23 00:00:46.261 validating: 41 / 100
11-23 00:00:55.184 validating: 61 / 100
11-23 00:01:04.081 validating: 81 / 100
11-23 00:01:13.123 validating: 101 / 100
11-23 00:01:22.219 validating: 121 / 100
11-23 00:01:31.343 validating: 141 / 100
11-23 00:01:40.299 validating: 161 / 100
11-23 00:01:49.204 validating: 181 / 100
11-23 00:01:58.107 validating: 201 / 100
11-23 00:02:07.006 validating: 221 / 100
11-23 00:02:15.740 validating: 241 / 100
11-23 00:02:24.462 validating: 261 / 100
11-23 00:02:33.227 validating: 281 / 100
11-23 00:02:42.102 validating: 301 / 100
11-23 00:02:50.989 validating: 321 / 100
11-23 00:02:59.916 validating: 341 / 100
11-23 00:03:08.807 validating: 361 / 100
11-23 00:03:17.728 validating: 381 / 100
11-23 00:03:26.577 validating: 401 / 100
11-23 00:03:35.463 validating: 421 / 100
11-23 00:03:44.340 validating: 441 / 100
num_sensitive, centroids = 325 [0.0030014200623357744, 0.05464729871718013, 0.3863575692687716]
Check whether two ints are same 325 tensor(280., device='cuda:1')
num_sensitive, centroids = 2714 [0.0015767549870596756, 0.027533180043679665, 0.1580887089853417]
Check whether two ints are same 2714 tensor(2532., device='cuda:1')
num_sensitive, centroids = 24895 [0.00026603923003987856, 0.002862258423880429, 0.012109163315497235]
Check whether two ints are same 24895 tensor(22226., device='cuda:1')
11-23 00:03:53.178 validating: 461 / 100
11-23 00:04:02.083 validating: 481 / 100
num_sensitive, centroids = 325 [0.0030014200623357744, 0.05464729871718013, 0.3863575692687716]
Check whether two ints are same 325 tensor(280., device='cuda:0')
Covariance Info: (CXC Shape, Num_Off_Diagonal) torch.Size([64, 64]) tensor(2016., device='cuda:0')
Selective (Sensitive Covariance) tensor(280., device='cuda:0')
num_sensitive, centroids = 2714 [0.0015767549870596756, 0.027533180043679665, 0.1580887089853417]
Check whether two ints are same 2714 tensor(2532., device='cuda:0')
Covariance Info: (CXC Shape, Num_Off_Diagonal) torch.Size([256, 256]) tensor(32640., device='cuda:0')
Selective (Sensitive Covariance) tensor(2532., device='cuda:0')
num_sensitive, centroids = 24895 [0.00026603923003987856, 0.002862258423880429, 0.012109163315497235]
Check whether two ints are same 24895 tensor(22226., device='cuda:0')
Covariance Info: (CXC Shape, Num_Off_Diagonal) torch.Size([512, 512]) tensor(130816., device='cuda:0')
Selective (Sensitive Covariance) tensor(22226., device='cuda:0')
11-23 00:04:18.768 validating: 1 / 100
11-23 00:04:28.042 validating: 21 / 100
11-23 00:04:37.240 validating: 41 / 100
11-23 00:04:46.384 validating: 61 / 100
11-23 00:04:55.269 validating: 81 / 100
11-23 00:05:04.282 validating: 101 / 100
11-23 00:05:13.308 validating: 121 / 100
11-23 00:05:22.355 validating: 141 / 100
11-23 00:05:31.159 validating: 161 / 100
11-23 00:05:39.958 validating: 181 / 100
11-23 00:05:48.816 validating: 201 / 100
11-23 00:05:57.826 validating: 221 / 100
11-23 00:06:06.762 validating: 241 / 100
11-23 00:06:15.681 validating: 261 / 100
11-23 00:06:24.509 validating: 281 / 100
11-23 00:06:33.434 validating: 301 / 100
11-23 00:06:42.320 validating: 321 / 100
11-23 00:06:51.220 validating: 341 / 100
11-23 00:06:59.892 validating: 361 / 100
11-23 00:07:08.823 validating: 381 / 100
11-23 00:07:17.696 validating: 401 / 100
11-23 00:07:26.546 validating: 421 / 100
num_sensitive, centroids = 301 [0.003069850232323511, 0.050735513521295, 0.3658696796212878]
Check whether two ints are same 301 tensor(279., device='cuda:1')
11-23 00:07:35.414 validating: 441 / 100
num_sensitive, centroids = 3021 [0.0014796824821739758, 0.02472657720900602, 0.15271898882614598]
Check whether two ints are same 3021 tensor(2515., device='cuda:1')
num_sensitive, centroids = 24555 [0.0002595497788810387, 0.002793430603629761, 0.01191623276252237]
Check whether two ints are same 24555 tensor(21866., device='cuda:1')
11-23 00:07:44.390 validating: 461 / 100
11-23 00:07:53.377 validating: 481 / 100
num_sensitive, centroids = 301 [0.003069850232323511, 0.050735513521295, 0.3658696796212878]
Check whether two ints are same 301 tensor(279., device='cuda:0')
Covariance Info: (CXC Shape, Num_Off_Diagonal) torch.Size([64, 64]) tensor(2016., device='cuda:0')
Selective (Sensitive Covariance) tensor(279., device='cuda:0')
num_sensitive, centroids = 3021 [0.0014796824821739758, 0.02472657720900602, 0.15271898882614598]
Check whether two ints are same 3021 tensor(2515., device='cuda:0')
Covariance Info: (CXC Shape, Num_Off_Diagonal) torch.Size([256, 256]) tensor(32640., device='cuda:0')
Selective (Sensitive Covariance) tensor(2515., device='cuda:0')
num_sensitive, centroids = 24555 [0.0002595497788810387, 0.002793430603629761, 0.01191623276252237]
Check whether two ints are same 24555 tensor(21866., device='cuda:0')
Covariance Info: (CXC Shape, Num_Off_Diagonal) torch.Size([512, 512]) tensor(130816., device='cuda:0')
Selective (Sensitive Covariance) tensor(21866., device='cuda:0')
11-23 00:08:10.272 validating: 1 / 100
11-23 00:08:19.428 validating: 21 / 100
11-23 00:08:28.420 validating: 41 / 100
11-23 00:08:37.392 validating: 61 / 100
11-23 00:08:46.411 validating: 81 / 100
11-23 00:08:55.421 validating: 101 / 100
11-23 00:09:04.237 validating: 121 / 100
11-23 00:09:13.094 validating: 141 / 100
11-23 00:09:21.960 validating: 161 / 100
11-23 00:09:30.875 validating: 181 / 100
11-23 00:09:39.580 validating: 201 / 100
11-23 00:09:48.355 validating: 221 / 100
11-23 00:09:57.383 validating: 241 / 100
11-23 00:10:06.251 validating: 261 / 100
11-23 00:10:15.113 validating: 281 / 100
11-23 00:10:24.050 validating: 301 / 100
11-23 00:10:33.048 validating: 321 / 100
11-23 00:10:41.874 validating: 341 / 100
11-23 00:10:50.658 validating: 361 / 100
11-23 00:10:59.457 validating: 381 / 100
11-23 00:11:08.439 validating: 401 / 100
11-23 00:11:17.275 validating: 421 / 100
num_sensitive, centroids = 323 [0.002929150625091876, 0.05129626438705417, 0.3649409775223051]
Check whether two ints are same 323 tensor(272., device='cuda:1')
num_sensitive, centroids = 3029 [0.0014871898377716455, 0.02432508644020711, 0.1480289388980185]
Check whether two ints are same 3029 tensor(2506., device='cuda:1')
11-23 00:11:26.212 validating: 441 / 100
num_sensitive, centroids = 26054 [0.00025227023204061396, 0.0026462207860113622, 0.011144180424562436]
Check whether two ints are same 26054 tensor(21786., device='cuda:1')
11-23 00:11:35.286 validating: 461 / 100
11-23 00:11:44.335 validating: 481 / 100
num_sensitive, centroids = 323 [0.002929150625091876, 0.05129626438705417, 0.3649409775223051]
Check whether two ints are same 323 tensor(272., device='cuda:0')
Covariance Info: (CXC Shape, Num_Off_Diagonal) torch.Size([64, 64]) tensor(2016., device='cuda:0')
Selective (Sensitive Covariance) tensor(272., device='cuda:0')
num_sensitive, centroids = 3029 [0.0014871898377716455, 0.02432508644020711, 0.1480289388980185]
Check whether two ints are same 3029 tensor(2506., device='cuda:0')
Covariance Info: (CXC Shape, Num_Off_Diagonal) torch.Size([256, 256]) tensor(32640., device='cuda:0')
Selective (Sensitive Covariance) tensor(2506., device='cuda:0')
num_sensitive, centroids = 26054 [0.00025227023204061396, 0.0026462207860113622, 0.011144180424562436]
Check whether two ints are same 26054 tensor(21786., device='cuda:0')
Covariance Info: (CXC Shape, Num_Off_Diagonal) torch.Size([512, 512]) tensor(130816., device='cuda:0')
Selective (Sensitive Covariance) tensor(21786., device='cuda:0')
11-23 00:12:00.672 validating: 1 / 100
11-23 00:12:09.765 validating: 21 / 100
11-23 00:12:18.607 validating: 41 / 100
11-23 00:12:27.489 validating: 61 / 100
11-23 00:12:36.293 validating: 81 / 100
11-23 00:12:45.554 validating: 101 / 100
11-23 00:12:54.676 validating: 121 / 100
11-23 00:13:03.862 validating: 141 / 100
11-23 00:13:13.137 validating: 161 / 100
11-23 00:13:22.263 validating: 181 / 100
11-23 00:13:31.301 validating: 201 / 100
11-23 00:13:40.192 validating: 221 / 100
11-23 00:13:49.505 validating: 241 / 100
11-23 00:13:58.500 validating: 261 / 100
11-23 00:14:07.497 validating: 281 / 100
11-23 00:14:16.478 validating: 301 / 100
11-23 00:14:25.481 validating: 321 / 100
11-23 00:14:34.391 validating: 341 / 100
11-23 00:14:43.375 validating: 361 / 100
11-23 00:14:52.324 validating: 381 / 100
11-23 00:15:01.285 validating: 401 / 100
11-23 00:15:10.179 validating: 421 / 100
num_sensitive, centroids = 330 [0.003031159449311942, 0.05189563976975599, 0.3830197432211467]
Check whether two ints are same 330 tensor(272., device='cuda:1')
num_sensitive, centroids = 3022 [0.0015409484735080393, 0.025697201932985962, 0.15814802619187457]
Check whether two ints are same 3022 tensor(2498., device='cuda:1')
11-23 00:15:18.808 validating: 441 / 100
num_sensitive, centroids = 25338 [0.0002590886651184888, 0.002766207821009194, 0.011703918927013786]
Check whether two ints are same 25338 tensor(21686., device='cuda:1')
11-23 00:15:27.634 validating: 461 / 100
11-23 00:15:36.333 validating: 481 / 100
num_sensitive, centroids = 330 [0.003031159449311942, 0.05189563976975599, 0.3830197432211467]
Check whether two ints are same 330 tensor(272., device='cuda:0')
Covariance Info: (CXC Shape, Num_Off_Diagonal) torch.Size([64, 64]) tensor(2016., device='cuda:0')
Selective (Sensitive Covariance) tensor(272., device='cuda:0')
num_sensitive, centroids = 3022 [0.0015409484735080393, 0.025697201932985962, 0.15814802619187457]
Check whether two ints are same 3022 tensor(2498., device='cuda:0')
Covariance Info: (CXC Shape, Num_Off_Diagonal) torch.Size([256, 256]) tensor(32640., device='cuda:0')
Selective (Sensitive Covariance) tensor(2498., device='cuda:0')
num_sensitive, centroids = 25338 [0.0002590886651184888, 0.002766207821009194, 0.011703918927013786]
Check whether two ints are same 25338 tensor(21686., device='cuda:0')
Covariance Info: (CXC Shape, Num_Off_Diagonal) torch.Size([512, 512]) tensor(130816., device='cuda:0')
Selective (Sensitive Covariance) tensor(21686., device='cuda:0')
11-23 00:15:52.987 validating: 1 / 100
11-23 00:16:02.142 validating: 21 / 100
11-23 00:16:11.359 validating: 41 / 100
11-23 00:16:20.332 validating: 61 / 100
11-23 00:16:29.170 validating: 81 / 100
11-23 00:16:38.066 validating: 101 / 100
11-23 00:16:47.123 validating: 121 / 100
11-23 00:16:56.106 validating: 141 / 100
11-23 00:17:04.915 validating: 161 / 100
11-23 00:17:13.751 validating: 181 / 100
11-23 00:17:22.402 validating: 201 / 100
11-23 00:17:31.166 validating: 221 / 100
11-23 00:17:40.027 validating: 241 / 100
11-23 00:17:48.931 validating: 261 / 100
11-23 00:17:57.884 validating: 281 / 100
11-23 00:18:06.865 validating: 301 / 100
11-23 00:18:15.747 validating: 321 / 100
11-23 00:18:24.638 validating: 341 / 100
11-23 00:18:33.362 validating: 361 / 100
11-23 00:18:42.097 validating: 381 / 100
11-23 00:18:51.037 validating: 401 / 100
11-23 00:18:59.776 validating: 421 / 100
num_sensitive, centroids = 317 [0.003072743596274072, 0.05436570606524905, 0.35966715855257847]
Check whether two ints are same 317 tensor(268., device='cuda:1')
num_sensitive, centroids = 2793 [0.0015319959016856022, 0.026137349793237088, 0.1502460553065727]
Check whether two ints are same 2793 tensor(2465., device='cuda:1')
11-23 00:19:08.614 validating: 441 / 100
num_sensitive, centroids = 26252 [0.00025387883768966316, 0.002673828439884321, 0.011214852062873766]
Check whether two ints are same 26252 tensor(21628., device='cuda:1')
11-23 00:19:18.026 validating: 461 / 100
11-23 00:19:27.035 validating: 481 / 100
num_sensitive, centroids = 317 [0.003072743596274072, 0.05436570606524905, 0.35966715855257847]
Check whether two ints are same 317 tensor(268., device='cuda:0')
Covariance Info: (CXC Shape, Num_Off_Diagonal) torch.Size([64, 64]) tensor(2016., device='cuda:0')
Selective (Sensitive Covariance) tensor(268., device='cuda:0')
num_sensitive, centroids = 2793 [0.0015319959016856022, 0.026137349793237088, 0.1502460553065727]
Check whether two ints are same 2793 tensor(2465., device='cuda:0')
Covariance Info: (CXC Shape, Num_Off_Diagonal) torch.Size([256, 256]) tensor(32640., device='cuda:0')
Selective (Sensitive Covariance) tensor(2465., device='cuda:0')
num_sensitive, centroids = 26252 [0.00025387883768966316, 0.002673828439884321, 0.011214852062873766]
Check whether two ints are same 26252 tensor(21628., device='cuda:0')
Covariance Info: (CXC Shape, Num_Off_Diagonal) torch.Size([512, 512]) tensor(130816., device='cuda:0')
Selective (Sensitive Covariance) tensor(21628., device='cuda:0')
Saving pth file...
11-23 00:19:40.595 Saved file to ./logs/1122/r50os16_gtav_isw/11_22_20/last_None_epoch_5_mean-iu_0.00000.pth
11-23 00:19:40.595 Class Uniform Percentage: 0.5
11-23 00:19:40.595 Class Uniform items per Epoch:12388
11-23 00:19:40.598 cls 0 len 12109
11-23 00:19:40.598 cls 1 len 11833
11-23 00:19:40.598 cls 2 len 12301
11-23 00:19:40.598 cls 3 len 10854
11-23 00:19:40.598 cls 4 len 8811
11-23 00:19:40.598 cls 5 len 11928
11-23 00:19:40.598 cls 6 len 7891
11-23 00:19:40.598 cls 7 len 5921
11-23 00:19:40.598 cls 8 len 12132
11-23 00:19:40.598 cls 9 len 11549
11-23 00:19:40.598 cls 10 len 12131
11-23 00:19:40.598 cls 11 len 10691
11-23 00:19:40.598 cls 12 len 986
11-23 00:19:40.598 cls 13 len 10501
11-23 00:19:40.598 cls 14 len 6711
11-23 00:19:40.598 cls 15 len 1861
11-23 00:19:40.598 cls 16 len 493
11-23 00:19:40.598 cls 17 len 1211
11-23 00:19:40.598 cls 18 len 168
11-23 00:20:36.580 [epoch 6], [iter 50 / 1548 : 9337], [loss 0.517724], [lr 0.007872], [time 0.2428]
Whitening Loss tensor([0.2277], device='cuda:0', grad_fn=<DivBackward0>)
11-23 00:21:25.777 [epoch 6], [iter 100 / 1548 : 9387], [loss 0.504381], [lr 0.007861], [time 0.2419]
Whitening Loss tensor([0.1833], device='cuda:0', grad_fn=<DivBackward0>)
11-23 00:22:14.932 [epoch 6], [iter 150 / 1548 : 9437], [loss 0.515359], [lr 0.007849], [time 0.2418]
Whitening Loss tensor([0.2236], device='cuda:0', grad_fn=<DivBackward0>)
11-23 00:23:04.161 [epoch 6], [iter 200 / 1548 : 9487], [loss 0.522164], [lr 0.007838], [time 0.2422]
Whitening Loss tensor([0.2338], device='cuda:0', grad_fn=<DivBackward0>)
11-23 00:23:53.338 [epoch 6], [iter 250 / 1548 : 9537], [loss 0.557268], [lr 0.007826], [time 0.2419]
Whitening Loss tensor([0.2074], device='cuda:0', grad_fn=<DivBackward0>)
11-23 00:24:42.528 [epoch 6], [iter 300 / 1548 : 9587], [loss 0.543161], [lr 0.007814], [time 0.2419]
Whitening Loss tensor([0.2353], device='cuda:0', grad_fn=<DivBackward0>)
11-23 00:25:31.767 [epoch 6], [iter 350 / 1548 : 9637], [loss 0.545155], [lr 0.007803], [time 0.2421]
Whitening Loss tensor([0.2148], device='cuda:0', grad_fn=<DivBackward0>)
11-23 00:26:20.965 [epoch 6], [iter 400 / 1548 : 9687], [loss 0.576523], [lr 0.007791], [time 0.2418]
Whitening Loss tensor([0.1979], device='cuda:0', grad_fn=<DivBackward0>)
11-23 00:27:10.276 [epoch 6], [iter 450 / 1548 : 9737], [loss 0.562009], [lr 0.007780], [time 0.2424]
Whitening Loss tensor([0.2193], device='cuda:0', grad_fn=<DivBackward0>)
11-23 00:27:59.450 [epoch 6], [iter 500 / 1548 : 9787], [loss 0.515678], [lr 0.007768], [time 0.2417]
Whitening Loss tensor([0.1904], device='cuda:0', grad_fn=<DivBackward0>)
11-23 00:28:48.684 [epoch 6], [iter 550 / 1548 : 9837], [loss 0.480451], [lr 0.007757], [time 0.2420]
Whitening Loss tensor([0.1765], device='cuda:0', grad_fn=<DivBackward0>)
11-23 00:29:37.995 [epoch 6], [iter 600 / 1548 : 9887], [loss 0.484378], [lr 0.007745], [time 0.2423]
Whitening Loss tensor([0.1903], device='cuda:0', grad_fn=<DivBackward0>)
11-23 00:30:27.153 [epoch 6], [iter 650 / 1548 : 9937], [loss 0.570180], [lr 0.007733], [time 0.2416]
Whitening Loss tensor([0.1802], device='cuda:0', grad_fn=<DivBackward0>)
11-23 00:31:16.453 [epoch 6], [iter 700 / 1548 : 9987], [loss 0.518898], [lr 0.007722], [time 0.2423]
Whitening Loss tensor([0.1998], device='cuda:0', grad_fn=<DivBackward0>)
11-23 00:32:05.695 [epoch 6], [iter 750 / 1548 : 10037], [loss 0.552415], [lr 0.007710], [time 0.2419]
Whitening Loss tensor([0.1967], device='cuda:0', grad_fn=<DivBackward0>)
11-23 00:32:54.963 [epoch 6], [iter 800 / 1548 : 10087], [loss 0.517457], [lr 0.007699], [time 0.2421]
Whitening Loss tensor([0.1634], device='cuda:0', grad_fn=<DivBackward0>)
11-23 00:33:44.240 [epoch 6], [iter 850 / 1548 : 10137], [loss 0.506882], [lr 0.007687], [time 0.2421]
Whitening Loss tensor([0.1872], device='cuda:0', grad_fn=<DivBackward0>)
11-23 00:34:33.507 [epoch 6], [iter 900 / 1548 : 10187], [loss 0.509213], [lr 0.007676], [time 0.2420]
Whitening Loss tensor([0.1803], device='cuda:0', grad_fn=<DivBackward0>)
11-23 00:35:22.732 [epoch 6], [iter 950 / 1548 : 10237], [loss 0.528799], [lr 0.007664], [time 0.2419]
Whitening Loss tensor([0.1974], device='cuda:0', grad_fn=<DivBackward0>)
11-23 00:36:12.031 [epoch 6], [iter 1000 / 1548 : 10287], [loss 0.454527], [lr 0.007652], [time 0.2423]
Whitening Loss tensor([0.1417], device='cuda:0', grad_fn=<DivBackward0>)
11-23 00:37:01.227 [epoch 6], [iter 1050 / 1548 : 10337], [loss 0.522294], [lr 0.007641], [time 0.2418]
Whitening Loss tensor([0.1785], device='cuda:0', grad_fn=<DivBackward0>)
11-23 00:37:50.460 [epoch 6], [iter 1100 / 1548 : 10387], [loss 0.469921], [lr 0.007629], [time 0.2420]
Whitening Loss tensor([0.1343], device='cuda:0', grad_fn=<DivBackward0>)
11-23 00:38:39.729 [epoch 6], [iter 1150 / 1548 : 10437], [loss 0.451043], [lr 0.007618], [time 0.2421]
Whitening Loss tensor([0.1711], device='cuda:0', grad_fn=<DivBackward0>)
11-23 00:39:29.011 [epoch 6], [iter 1200 / 1548 : 10487], [loss 0.486696], [lr 0.007606], [time 0.2422]
Whitening Loss tensor([0.1694], device='cuda:0', grad_fn=<DivBackward0>)
11-23 00:40:18.250 [epoch 6], [iter 1250 / 1548 : 10537], [loss 0.495918], [lr 0.007594], [time 0.2419]
Whitening Loss tensor([0.1695], device='cuda:0', grad_fn=<DivBackward0>)
11-23 00:41:07.483 [epoch 6], [iter 1300 / 1548 : 10587], [loss 0.436030], [lr 0.007583], [time 0.2420]
Whitening Loss tensor([0.1439], device='cuda:0', grad_fn=<DivBackward0>)
11-23 00:41:56.734 [epoch 6], [iter 1350 / 1548 : 10637], [loss 0.508679], [lr 0.007571], [time 0.2421]
Whitening Loss tensor([0.1659], device='cuda:0', grad_fn=<DivBackward0>)
11-23 00:42:45.995 [epoch 6], [iter 1400 / 1548 : 10687], [loss 0.528014], [lr 0.007560], [time 0.2422]
Whitening Loss tensor([0.1532], device='cuda:0', grad_fn=<DivBackward0>)
11-23 00:43:35.265 [epoch 6], [iter 1450 / 1548 : 10737], [loss 0.501114], [lr 0.007548], [time 0.2420]
Whitening Loss tensor([0.1516], device='cuda:0', grad_fn=<DivBackward0>)
11-23 00:44:24.499 [epoch 6], [iter 1500 / 1548 : 10787], [loss 0.477100], [lr 0.007536], [time 0.2419]
Whitening Loss tensor([0.1462], device='cuda:0', grad_fn=<DivBackward0>)
Saving pth file...
11-23 00:45:12.213 Saved file to ./logs/1122/r50os16_gtav_isw/11_22_20/last_None_epoch_6_mean-iu_0.00000.pth
11-23 00:45:12.214 Class Uniform Percentage: 0.5
11-23 00:45:12.214 Class Uniform items per Epoch:12388
11-23 00:45:12.216 cls 0 len 12109
11-23 00:45:12.216 cls 1 len 11833
11-23 00:45:12.216 cls 2 len 12301
11-23 00:45:12.216 cls 3 len 10854
11-23 00:45:12.216 cls 4 len 8811
11-23 00:45:12.216 cls 5 len 11928
11-23 00:45:12.217 cls 6 len 7891
11-23 00:45:12.217 cls 7 len 5921
11-23 00:45:12.217 cls 8 len 12132
11-23 00:45:12.217 cls 9 len 11549
11-23 00:45:12.217 cls 10 len 12131
11-23 00:45:12.217 cls 11 len 10691
11-23 00:45:12.217 cls 12 len 986
11-23 00:45:12.217 cls 13 len 10501
11-23 00:45:12.217 cls 14 len 6711
11-23 00:45:12.217 cls 15 len 1861
11-23 00:45:12.217 cls 16 len 493
11-23 00:45:12.217 cls 17 len 1211
11-23 00:45:12.217 cls 18 len 168
11-23 00:46:08.442 [epoch 7], [iter 50 / 1548 : 10885], [loss 0.526119], [lr 0.007514], [time 0.2419]
Whitening Loss tensor([0.1411], device='cuda:0', grad_fn=<DivBackward0>)
11-23 00:46:57.287 [epoch 7], [iter 100 / 1548 : 10935], [loss 0.567156], [lr 0.007502], [time 0.2409]
Whitening Loss tensor([0.1474], device='cuda:0', grad_fn=<DivBackward0>)
11-23 00:47:46.129 [epoch 7], [iter 150 / 1548 : 10985], [loss 0.485105], [lr 0.007490], [time 0.2410]
Whitening Loss tensor([0.1456], device='cuda:0', grad_fn=<DivBackward0>)
11-23 00:48:34.959 [epoch 7], [iter 200 / 1548 : 11035], [loss 0.436159], [lr 0.007479], [time 0.2408]
Whitening Loss tensor([0.1469], device='cuda:0', grad_fn=<DivBackward0>)
11-23 00:49:23.778 [epoch 7], [iter 250 / 1548 : 11085], [loss 0.473155], [lr 0.007467], [time 0.2409]
Whitening Loss tensor([0.1326], device='cuda:0', grad_fn=<DivBackward0>)
11-23 00:50:12.637 [epoch 7], [iter 300 / 1548 : 11135], [loss 0.554098], [lr 0.007456], [time 0.2410]
Whitening Loss tensor([0.1634], device='cuda:0', grad_fn=<DivBackward0>)
11-23 00:51:01.335 [epoch 7], [iter 350 / 1548 : 11185], [loss 0.471170], [lr 0.007444], [time 0.2403]
Whitening Loss tensor([0.1462], device='cuda:0', grad_fn=<DivBackward0>)
11-23 00:51:50.066 [epoch 7], [iter 400 / 1548 : 11235], [loss 0.495095], [lr 0.007432], [time 0.2405]
Whitening Loss tensor([0.1505], device='cuda:0', grad_fn=<DivBackward0>)
11-23 00:52:38.738 [epoch 7], [iter 450 / 1548 : 11285], [loss 0.493902], [lr 0.007421], [time 0.2402]
Whitening Loss tensor([0.1382], device='cuda:0', grad_fn=<DivBackward0>)
11-23 00:53:27.446 [epoch 7], [iter 500 / 1548 : 11335], [loss 0.467164], [lr 0.007409], [time 0.2404]
Whitening Loss tensor([0.1423], device='cuda:0', grad_fn=<DivBackward0>)
11-23 00:54:16.206 [epoch 7], [iter 550 / 1548 : 11385], [loss 0.436176], [lr 0.007397], [time 0.2406]
Whitening Loss tensor([0.1429], device='cuda:0', grad_fn=<DivBackward0>)
11-23 00:55:05.015 [epoch 7], [iter 600 / 1548 : 11435], [loss 0.443060], [lr 0.007386], [time 0.2409]
Whitening Loss tensor([0.1125], device='cuda:0', grad_fn=<DivBackward0>)
11-23 00:55:53.867 [epoch 7], [iter 650 / 1548 : 11485], [loss 0.484914], [lr 0.007374], [time 0.2411]
Whitening Loss tensor([0.1398], device='cuda:0', grad_fn=<DivBackward0>)
11-23 00:56:42.690 [epoch 7], [iter 700 / 1548 : 11535], [loss 0.486442], [lr 0.007363], [time 0.2410]
Whitening Loss tensor([0.1508], device='cuda:0', grad_fn=<DivBackward0>)
11-23 00:57:31.374 [epoch 7], [iter 750 / 1548 : 11585], [loss 0.436303], [lr 0.007351], [time 0.2403]
Whitening Loss tensor([0.1578], device='cuda:0', grad_fn=<DivBackward0>)
11-23 00:58:20.157 [epoch 7], [iter 800 / 1548 : 11635], [loss 0.436033], [lr 0.007339], [time 0.2407]
Whitening Loss tensor([0.1322], device='cuda:0', grad_fn=<DivBackward0>)
11-23 00:59:08.966 [epoch 7], [iter 850 / 1548 : 11685], [loss 0.509160], [lr 0.007328], [time 0.2408]
Whitening Loss tensor([0.1835], device='cuda:0', grad_fn=<DivBackward0>)
11-23 00:59:57.785 [epoch 7], [iter 900 / 1548 : 11735], [loss 0.487426], [lr 0.007316], [time 0.2408]
Whitening Loss tensor([0.1411], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:00:46.565 [epoch 7], [iter 950 / 1548 : 11785], [loss 0.467479], [lr 0.007304], [time 0.2407]
Whitening Loss tensor([0.1299], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:01:35.285 [epoch 7], [iter 1000 / 1548 : 11835], [loss 0.491540], [lr 0.007293], [time 0.2405]
Whitening Loss tensor([0.1258], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:02:23.919 [epoch 7], [iter 1050 / 1548 : 11885], [loss 0.474262], [lr 0.007281], [time 0.2401]
Whitening Loss tensor([0.1285], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:03:12.679 [epoch 7], [iter 1100 / 1548 : 11935], [loss 0.475512], [lr 0.007269], [time 0.2406]
Whitening Loss tensor([0.1320], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:04:01.505 [epoch 7], [iter 1150 / 1548 : 11985], [loss 0.550835], [lr 0.007258], [time 0.2409]
Whitening Loss tensor([0.1344], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:04:50.299 [epoch 7], [iter 1200 / 1548 : 12035], [loss 0.541323], [lr 0.007246], [time 0.2408]
Whitening Loss tensor([0.1104], device='cuda:0', grad_fn=<DivBackward0>)
Error!! (957, 526) (1052, 1914) 15188
Dropping  tensor(6860)
11-23 01:05:39.088 [epoch 7], [iter 1250 / 1548 : 12085], [loss 0.465188], [lr 0.007234], [time 0.2406]
Whitening Loss tensor([0.1112], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:06:27.860 [epoch 7], [iter 1300 / 1548 : 12135], [loss 0.465747], [lr 0.007223], [time 0.2407]
Whitening Loss tensor([0.1472], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:07:16.577 [epoch 7], [iter 1350 / 1548 : 12185], [loss 0.469851], [lr 0.007211], [time 0.2405]
Whitening Loss tensor([0.1340], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:08:05.338 [epoch 7], [iter 1400 / 1548 : 12235], [loss 0.495773], [lr 0.007199], [time 0.2406]
Whitening Loss tensor([0.1571], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:08:54.104 [epoch 7], [iter 1450 / 1548 : 12285], [loss 0.493359], [lr 0.007188], [time 0.2407]
Whitening Loss tensor([0.1429], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:09:42.873 [epoch 7], [iter 1500 / 1548 : 12335], [loss 0.471779], [lr 0.007176], [time 0.2406]
Whitening Loss tensor([0.1335], device='cuda:0', grad_fn=<DivBackward0>)
Saving pth file...
11-23 01:10:30.166 Saved file to ./logs/1122/r50os16_gtav_isw/11_22_20/last_None_epoch_7_mean-iu_0.00000.pth
11-23 01:10:30.167 Class Uniform Percentage: 0.5
11-23 01:10:30.167 Class Uniform items per Epoch:12388
11-23 01:10:30.169 cls 0 len 12109
11-23 01:10:30.169 cls 1 len 11833
11-23 01:10:30.169 cls 2 len 12301
11-23 01:10:30.169 cls 3 len 10854
11-23 01:10:30.169 cls 4 len 8811
11-23 01:10:30.169 cls 5 len 11928
11-23 01:10:30.169 cls 6 len 7891
11-23 01:10:30.169 cls 7 len 5921
11-23 01:10:30.169 cls 8 len 12132
11-23 01:10:30.169 cls 9 len 11549
11-23 01:10:30.169 cls 10 len 12131
11-23 01:10:30.169 cls 11 len 10691
11-23 01:10:30.169 cls 12 len 986
11-23 01:10:30.170 cls 13 len 10501
11-23 01:10:30.170 cls 14 len 6711
11-23 01:10:30.170 cls 15 len 1861
11-23 01:10:30.170 cls 16 len 493
11-23 01:10:30.170 cls 17 len 1211
11-23 01:10:30.170 cls 18 len 168
11-23 01:11:25.931 [epoch 8], [iter 50 / 1548 : 12433], [loss 0.501392], [lr 0.007153], [time 0.2417]
Whitening Loss tensor([0.1283], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:12:14.736 [epoch 8], [iter 100 / 1548 : 12483], [loss 0.451431], [lr 0.007141], [time 0.2407]
Whitening Loss tensor([0.1095], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:13:03.596 [epoch 8], [iter 150 / 1548 : 12533], [loss 0.442832], [lr 0.007130], [time 0.2410]
Whitening Loss tensor([0.1466], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:13:52.375 [epoch 8], [iter 200 / 1548 : 12583], [loss 0.430844], [lr 0.007118], [time 0.2407]
Whitening Loss tensor([0.1084], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:14:41.099 [epoch 8], [iter 250 / 1548 : 12633], [loss 0.410542], [lr 0.007106], [time 0.2404]
Whitening Loss tensor([0.1357], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:15:29.956 [epoch 8], [iter 300 / 1548 : 12683], [loss 0.463964], [lr 0.007095], [time 0.2410]
Whitening Loss tensor([0.1236], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:16:18.734 [epoch 8], [iter 350 / 1548 : 12733], [loss 0.504881], [lr 0.007083], [time 0.2407]
Whitening Loss tensor([0.1273], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:17:07.555 [epoch 8], [iter 400 / 1548 : 12783], [loss 0.459329], [lr 0.007071], [time 0.2409]
Whitening Loss tensor([0.1240], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:17:56.305 [epoch 8], [iter 450 / 1548 : 12833], [loss 0.452222], [lr 0.007060], [time 0.2406]
Whitening Loss tensor([0.1137], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:18:45.168 [epoch 8], [iter 500 / 1548 : 12883], [loss 0.463281], [lr 0.007048], [time 0.2412]
Whitening Loss tensor([0.1221], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:19:33.896 [epoch 8], [iter 550 / 1548 : 12933], [loss 0.410450], [lr 0.007036], [time 0.2406]
Whitening Loss tensor([0.0985], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:20:22.708 [epoch 8], [iter 600 / 1548 : 12983], [loss 0.480914], [lr 0.007025], [time 0.2409]
Whitening Loss tensor([0.1114], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:21:11.488 [epoch 8], [iter 650 / 1548 : 13033], [loss 0.471401], [lr 0.007013], [time 0.2407]
Whitening Loss tensor([0.1316], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:22:00.398 [epoch 8], [iter 700 / 1548 : 13083], [loss 0.434013], [lr 0.007001], [time 0.2412]
Whitening Loss tensor([0.0993], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:22:49.338 [epoch 8], [iter 750 / 1548 : 13133], [loss 0.498282], [lr 0.006989], [time 0.2415]
Whitening Loss tensor([0.1126], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:23:38.152 [epoch 8], [iter 800 / 1548 : 13183], [loss 0.478646], [lr 0.006978], [time 0.2409]
Whitening Loss tensor([0.1129], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:24:27.001 [epoch 8], [iter 850 / 1548 : 13233], [loss 0.428174], [lr 0.006966], [time 0.2411]
Whitening Loss tensor([0.1160], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:25:15.821 [epoch 8], [iter 900 / 1548 : 13283], [loss 0.490295], [lr 0.006954], [time 0.2409]
Whitening Loss tensor([0.1038], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:26:04.694 [epoch 8], [iter 950 / 1548 : 13333], [loss 0.450894], [lr 0.006943], [time 0.2412]
Whitening Loss tensor([0.1045], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:26:53.553 [epoch 8], [iter 1000 / 1548 : 13383], [loss 0.445949], [lr 0.006931], [time 0.2411]
Whitening Loss tensor([0.1526], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:27:42.385 [epoch 8], [iter 1050 / 1548 : 13433], [loss 0.468707], [lr 0.006919], [time 0.2410]
Whitening Loss tensor([0.1187], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:28:31.182 [epoch 8], [iter 1100 / 1548 : 13483], [loss 0.477981], [lr 0.006907], [time 0.2408]
Whitening Loss tensor([0.1258], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:29:20.013 [epoch 8], [iter 1150 / 1548 : 13533], [loss 0.453151], [lr 0.006896], [time 0.2410]
Whitening Loss tensor([0.1022], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:30:08.847 [epoch 8], [iter 1200 / 1548 : 13583], [loss 0.449129], [lr 0.006884], [time 0.2410]
Whitening Loss tensor([0.0991], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:30:57.689 [epoch 8], [iter 1250 / 1548 : 13633], [loss 0.436900], [lr 0.006872], [time 0.2410]
Whitening Loss tensor([0.1141], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:31:46.628 [epoch 8], [iter 1300 / 1548 : 13683], [loss 0.560544], [lr 0.006861], [time 0.2415]
Whitening Loss tensor([0.1376], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:32:35.480 [epoch 8], [iter 1350 / 1548 : 13733], [loss 0.480902], [lr 0.006849], [time 0.2410]
Whitening Loss tensor([0.1242], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:33:24.421 [epoch 8], [iter 1400 / 1548 : 13783], [loss 0.419347], [lr 0.006837], [time 0.2415]
Whitening Loss tensor([0.1268], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:34:13.285 [epoch 8], [iter 1450 / 1548 : 13833], [loss 0.437622], [lr 0.006825], [time 0.2411]
Whitening Loss tensor([0.1060], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:35:02.114 [epoch 8], [iter 1500 / 1548 : 13883], [loss 0.398168], [lr 0.006814], [time 0.2409]
Whitening Loss tensor([0.1042], device='cuda:0', grad_fn=<DivBackward0>)
Saving pth file...
11-23 01:35:49.537 Saved file to ./logs/1122/r50os16_gtav_isw/11_22_20/last_None_epoch_8_mean-iu_0.00000.pth
11-23 01:35:49.539 Class Uniform Percentage: 0.5
11-23 01:35:49.539 Class Uniform items per Epoch:12388
11-23 01:35:49.543 cls 0 len 12109
11-23 01:35:49.543 cls 1 len 11833
11-23 01:35:49.543 cls 2 len 12301
11-23 01:35:49.543 cls 3 len 10854
11-23 01:35:49.543 cls 4 len 8811
11-23 01:35:49.543 cls 5 len 11928
11-23 01:35:49.543 cls 6 len 7891
11-23 01:35:49.543 cls 7 len 5921
11-23 01:35:49.543 cls 8 len 12132
11-23 01:35:49.544 cls 9 len 11549
11-23 01:35:49.544 cls 10 len 12131
11-23 01:35:49.544 cls 11 len 10691
11-23 01:35:49.544 cls 12 len 986
11-23 01:35:49.544 cls 13 len 10501
11-23 01:35:49.544 cls 14 len 6711
11-23 01:35:49.544 cls 15 len 1861
11-23 01:35:49.544 cls 16 len 493
11-23 01:35:49.544 cls 17 len 1211
11-23 01:35:49.544 cls 18 len 168
11-23 01:36:45.936 [epoch 9], [iter 50 / 1548 : 13981], [loss 0.415910], [lr 0.006791], [time 0.2421]
Whitening Loss tensor([0.1086], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:37:35.016 [epoch 9], [iter 100 / 1548 : 14031], [loss 0.423271], [lr 0.006779], [time 0.2417]
Whitening Loss tensor([0.1030], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:38:24.025 [epoch 9], [iter 150 / 1548 : 14081], [loss 0.424831], [lr 0.006767], [time 0.2415]
Whitening Loss tensor([0.1227], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:39:12.998 [epoch 9], [iter 200 / 1548 : 14131], [loss 0.434404], [lr 0.006755], [time 0.2414]
Whitening Loss tensor([0.0993], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:40:01.972 [epoch 9], [iter 250 / 1548 : 14181], [loss 0.433758], [lr 0.006744], [time 0.2413]
Whitening Loss tensor([0.1234], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:40:50.985 [epoch 9], [iter 300 / 1548 : 14231], [loss 0.478021], [lr 0.006732], [time 0.2415]
Whitening Loss tensor([0.1115], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:41:40.046 [epoch 9], [iter 350 / 1548 : 14281], [loss 0.419386], [lr 0.006720], [time 0.2417]
Whitening Loss tensor([0.1117], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:42:29.119 [epoch 9], [iter 400 / 1548 : 14331], [loss 0.424361], [lr 0.006708], [time 0.2418]
Whitening Loss tensor([0.1258], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:43:18.132 [epoch 9], [iter 450 / 1548 : 14381], [loss 0.414404], [lr 0.006697], [time 0.2417]
Whitening Loss tensor([0.1137], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:44:07.177 [epoch 9], [iter 500 / 1548 : 14431], [loss 0.399059], [lr 0.006685], [time 0.2418]
Whitening Loss tensor([0.1144], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:44:56.163 [epoch 9], [iter 550 / 1548 : 14481], [loss 0.451777], [lr 0.006673], [time 0.2416]
Whitening Loss tensor([0.1206], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:45:45.185 [epoch 9], [iter 600 / 1548 : 14531], [loss 0.403694], [lr 0.006661], [time 0.2416]
Whitening Loss tensor([0.1377], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:46:34.253 [epoch 9], [iter 650 / 1548 : 14581], [loss 0.446376], [lr 0.006649], [time 0.2418]
Whitening Loss tensor([0.1265], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:47:23.204 [epoch 9], [iter 700 / 1548 : 14631], [loss 0.435800], [lr 0.006638], [time 0.2412]
Whitening Loss tensor([0.1081], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:48:12.273 [epoch 9], [iter 750 / 1548 : 14681], [loss 0.397539], [lr 0.006626], [time 0.2417]
Whitening Loss tensor([0.1288], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:49:01.304 [epoch 9], [iter 800 / 1548 : 14731], [loss 0.412040], [lr 0.006614], [time 0.2416]
Whitening Loss tensor([0.1007], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:49:50.296 [epoch 9], [iter 850 / 1548 : 14781], [loss 0.431584], [lr 0.006602], [time 0.2415]
Whitening Loss tensor([0.1039], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:50:39.291 [epoch 9], [iter 900 / 1548 : 14831], [loss 0.460083], [lr 0.006591], [time 0.2415]
Whitening Loss tensor([0.1193], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:51:28.292 [epoch 9], [iter 950 / 1548 : 14881], [loss 0.395951], [lr 0.006579], [time 0.2416]
Whitening Loss tensor([0.0940], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:52:17.292 [epoch 9], [iter 1000 / 1548 : 14931], [loss 0.416667], [lr 0.006567], [time 0.2414]
Whitening Loss tensor([0.1067], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:53:06.377 [epoch 9], [iter 1050 / 1548 : 14981], [loss 0.396988], [lr 0.006555], [time 0.2418]
Whitening Loss tensor([0.1128], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:53:55.485 [epoch 9], [iter 1100 / 1548 : 15031], [loss 0.487968], [lr 0.006543], [time 0.2419]
Whitening Loss tensor([0.0899], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:54:44.551 [epoch 9], [iter 1150 / 1548 : 15081], [loss 0.490981], [lr 0.006532], [time 0.2418]
Whitening Loss tensor([0.1234], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:55:33.592 [epoch 9], [iter 1200 / 1548 : 15131], [loss 0.487967], [lr 0.006520], [time 0.2416]
Whitening Loss tensor([0.1221], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:56:22.580 [epoch 9], [iter 1250 / 1548 : 15181], [loss 0.450470], [lr 0.006508], [time 0.2415]
Whitening Loss tensor([0.0951], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:57:11.550 [epoch 9], [iter 1300 / 1548 : 15231], [loss 0.462985], [lr 0.006496], [time 0.2413]
Whitening Loss tensor([0.1084], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:58:00.601 [epoch 9], [iter 1350 / 1548 : 15281], [loss 0.489806], [lr 0.006484], [time 0.2417]
Whitening Loss tensor([0.1165], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:58:49.601 [epoch 9], [iter 1400 / 1548 : 15331], [loss 0.459527], [lr 0.006473], [time 0.2414]
Whitening Loss tensor([0.1145], device='cuda:0', grad_fn=<DivBackward0>)
11-23 01:59:38.613 [epoch 9], [iter 1450 / 1548 : 15381], [loss 0.410983], [lr 0.006461], [time 0.2415]
Whitening Loss tensor([0.1253], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:00:27.735 [epoch 9], [iter 1500 / 1548 : 15431], [loss 0.395372], [lr 0.006449], [time 0.2421]
Whitening Loss tensor([0.0923], device='cuda:0', grad_fn=<DivBackward0>)
Saving pth file...
11-23 02:01:15.321 Saved file to ./logs/1122/r50os16_gtav_isw/11_22_20/last_None_epoch_9_mean-iu_0.00000.pth
11-23 02:01:15.322 Class Uniform Percentage: 0.5
11-23 02:01:15.322 Class Uniform items per Epoch:12388
11-23 02:01:15.325 cls 0 len 12109
11-23 02:01:15.325 cls 1 len 11833
11-23 02:01:15.325 cls 2 len 12301
11-23 02:01:15.325 cls 3 len 10854
11-23 02:01:15.325 cls 4 len 8811
11-23 02:01:15.325 cls 5 len 11928
11-23 02:01:15.325 cls 6 len 7891
11-23 02:01:15.325 cls 7 len 5921
11-23 02:01:15.325 cls 8 len 12132
11-23 02:01:15.325 cls 9 len 11549
11-23 02:01:15.325 cls 10 len 12131
11-23 02:01:15.325 cls 11 len 10691
11-23 02:01:15.325 cls 12 len 986
11-23 02:01:15.325 cls 13 len 10501
11-23 02:01:15.325 cls 14 len 6711
11-23 02:01:15.325 cls 15 len 1861
11-23 02:01:15.325 cls 16 len 493
11-23 02:01:15.326 cls 17 len 1211
11-23 02:01:15.326 cls 18 len 168
11-23 02:02:11.571 [epoch 10], [iter 50 / 1548 : 15529], [loss 0.450227], [lr 0.006426], [time 0.2423]
Whitening Loss tensor([0.1024], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:03:00.630 [epoch 10], [iter 100 / 1548 : 15579], [loss 0.422638], [lr 0.006414], [time 0.2420]
Whitening Loss tensor([0.1022], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:03:49.542 [epoch 10], [iter 150 / 1548 : 15629], [loss 0.409018], [lr 0.006402], [time 0.2413]
Whitening Loss tensor([0.0876], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:04:38.400 [epoch 10], [iter 200 / 1548 : 15679], [loss 0.429515], [lr 0.006390], [time 0.2411]
Whitening Loss tensor([0.0981], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:05:27.297 [epoch 10], [iter 250 / 1548 : 15729], [loss 0.451347], [lr 0.006379], [time 0.2412]
Whitening Loss tensor([0.1015], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:06:16.293 [epoch 10], [iter 300 / 1548 : 15779], [loss 0.468757], [lr 0.006367], [time 0.2417]
Whitening Loss tensor([0.1131], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:07:05.187 [epoch 10], [iter 350 / 1548 : 15829], [loss 0.403062], [lr 0.006355], [time 0.2413]
Whitening Loss tensor([0.0970], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:07:54.179 [epoch 10], [iter 400 / 1548 : 15879], [loss 0.435224], [lr 0.006343], [time 0.2417]
Whitening Loss tensor([0.1214], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:08:43.113 [epoch 10], [iter 450 / 1548 : 15929], [loss 0.465005], [lr 0.006331], [time 0.2415]
Whitening Loss tensor([0.0940], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:09:31.966 [epoch 10], [iter 500 / 1548 : 15979], [loss 0.422144], [lr 0.006319], [time 0.2411]
Whitening Loss tensor([0.1090], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:10:20.806 [epoch 10], [iter 550 / 1548 : 16029], [loss 0.417772], [lr 0.006308], [time 0.2410]
Whitening Loss tensor([0.0987], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:11:09.753 [epoch 10], [iter 600 / 1548 : 16079], [loss 0.425634], [lr 0.006296], [time 0.2416]
Whitening Loss tensor([0.1056], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:11:58.642 [epoch 10], [iter 650 / 1548 : 16129], [loss 0.404426], [lr 0.006284], [time 0.2412]
Whitening Loss tensor([0.1065], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:12:47.551 [epoch 10], [iter 700 / 1548 : 16179], [loss 0.390801], [lr 0.006272], [time 0.2414]
Whitening Loss tensor([0.1103], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:13:36.409 [epoch 10], [iter 750 / 1548 : 16229], [loss 0.491368], [lr 0.006260], [time 0.2410]
Whitening Loss tensor([0.1049], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:14:25.253 [epoch 10], [iter 800 / 1548 : 16279], [loss 0.404486], [lr 0.006248], [time 0.2410]
Whitening Loss tensor([0.0901], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:15:14.262 [epoch 10], [iter 850 / 1548 : 16329], [loss 0.410320], [lr 0.006237], [time 0.2417]
Whitening Loss tensor([0.1039], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:16:03.195 [epoch 10], [iter 900 / 1548 : 16379], [loss 0.405019], [lr 0.006225], [time 0.2414]
Whitening Loss tensor([0.1328], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:16:52.202 [epoch 10], [iter 950 / 1548 : 16429], [loss 0.403550], [lr 0.006213], [time 0.2416]
Whitening Loss tensor([0.0951], device='cuda:0', grad_fn=<DivBackward0>)
Error!! (957, 526) (1052, 1914) 15188
Dropping  tensor(4133)
11-23 02:17:41.122 [epoch 10], [iter 1000 / 1548 : 16479], [loss 0.414012], [lr 0.006201], [time 0.2414]
Whitening Loss tensor([0.1190], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:18:30.032 [epoch 10], [iter 1050 / 1548 : 16529], [loss 0.428255], [lr 0.006189], [time 0.2413]
Whitening Loss tensor([0.1173], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:19:18.968 [epoch 10], [iter 1100 / 1548 : 16579], [loss 0.445416], [lr 0.006177], [time 0.2413]
Whitening Loss tensor([0.0900], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:20:07.949 [epoch 10], [iter 1150 / 1548 : 16629], [loss 0.396120], [lr 0.006165], [time 0.2416]
Whitening Loss tensor([0.1238], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:20:56.847 [epoch 10], [iter 1200 / 1548 : 16679], [loss 0.393735], [lr 0.006153], [time 0.2412]
Whitening Loss tensor([0.1147], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:21:45.818 [epoch 10], [iter 1250 / 1548 : 16729], [loss 0.451832], [lr 0.006142], [time 0.2416]
Whitening Loss tensor([0.1125], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:22:34.724 [epoch 10], [iter 1300 / 1548 : 16779], [loss 0.501342], [lr 0.006130], [time 0.2413]
Whitening Loss tensor([0.0952], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:23:23.663 [epoch 10], [iter 1350 / 1548 : 16829], [loss 0.439656], [lr 0.006118], [time 0.2415]
Whitening Loss tensor([0.0924], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:24:12.590 [epoch 10], [iter 1400 / 1548 : 16879], [loss 0.411564], [lr 0.006106], [time 0.2414]
Whitening Loss tensor([0.0938], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:25:01.476 [epoch 10], [iter 1450 / 1548 : 16929], [loss 0.438082], [lr 0.006094], [time 0.2413]
Whitening Loss tensor([0.0976], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:25:50.425 [epoch 10], [iter 1500 / 1548 : 16979], [loss 0.393577], [lr 0.006082], [time 0.2415]
Whitening Loss tensor([0.1062], device='cuda:0', grad_fn=<DivBackward0>)
Saving pth file...
11-23 02:26:37.872 Saved file to ./logs/1122/r50os16_gtav_isw/11_22_20/last_None_epoch_10_mean-iu_0.00000.pth
11-23 02:26:37.873 Class Uniform Percentage: 0.5
11-23 02:26:37.873 Class Uniform items per Epoch:12388
11-23 02:26:37.876 cls 0 len 12109
11-23 02:26:37.876 cls 1 len 11833
11-23 02:26:37.877 cls 2 len 12301
11-23 02:26:37.877 cls 3 len 10854
11-23 02:26:37.877 cls 4 len 8811
11-23 02:26:37.877 cls 5 len 11928
11-23 02:26:37.877 cls 6 len 7891
11-23 02:26:37.877 cls 7 len 5921
11-23 02:26:37.877 cls 8 len 12132
11-23 02:26:37.877 cls 9 len 11549
11-23 02:26:37.877 cls 10 len 12131
11-23 02:26:37.877 cls 11 len 10691
11-23 02:26:37.877 cls 12 len 986
11-23 02:26:37.877 cls 13 len 10501
11-23 02:26:37.877 cls 14 len 6711
11-23 02:26:37.877 cls 15 len 1861
11-23 02:26:37.877 cls 16 len 493
11-23 02:26:37.877 cls 17 len 1211
11-23 02:26:37.877 cls 18 len 168
11-23 02:27:33.845 [epoch 11], [iter 50 / 1548 : 17077], [loss 0.440890], [lr 0.006059], [time 0.2413]
Whitening Loss tensor([0.0868], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:28:22.737 [epoch 11], [iter 100 / 1548 : 17127], [loss 0.415644], [lr 0.006047], [time 0.2412]
Whitening Loss tensor([0.0918], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:29:11.572 [epoch 11], [iter 150 / 1548 : 17177], [loss 0.408843], [lr 0.006035], [time 0.2410]
Whitening Loss tensor([0.0974], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:30:00.370 [epoch 11], [iter 200 / 1548 : 17227], [loss 0.412486], [lr 0.006023], [time 0.2408]
Whitening Loss tensor([0.0927], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:30:49.248 [epoch 11], [iter 250 / 1548 : 17277], [loss 0.402365], [lr 0.006011], [time 0.2409]
Whitening Loss tensor([0.0903], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:31:38.100 [epoch 11], [iter 300 / 1548 : 17327], [loss 0.457438], [lr 0.005999], [time 0.2410]
Whitening Loss tensor([0.1226], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:32:26.908 [epoch 11], [iter 350 / 1548 : 17377], [loss 0.396824], [lr 0.005987], [time 0.2408]
Whitening Loss tensor([0.0892], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:33:15.729 [epoch 11], [iter 400 / 1548 : 17427], [loss 0.416652], [lr 0.005976], [time 0.2408]
Whitening Loss tensor([0.0993], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:34:04.618 [epoch 11], [iter 450 / 1548 : 17477], [loss 0.374316], [lr 0.005964], [time 0.2413]
Whitening Loss tensor([0.0822], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:34:53.463 [epoch 11], [iter 500 / 1548 : 17527], [loss 0.386710], [lr 0.005952], [time 0.2411]
Whitening Loss tensor([0.0977], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:35:42.256 [epoch 11], [iter 550 / 1548 : 17577], [loss 0.383585], [lr 0.005940], [time 0.2408]
Whitening Loss tensor([0.1063], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:36:31.027 [epoch 11], [iter 600 / 1548 : 17627], [loss 0.398391], [lr 0.005928], [time 0.2408]
Whitening Loss tensor([0.1052], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:37:19.742 [epoch 11], [iter 650 / 1548 : 17677], [loss 0.377072], [lr 0.005916], [time 0.2405]
Whitening Loss tensor([0.0910], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:38:08.557 [epoch 11], [iter 700 / 1548 : 17727], [loss 0.421598], [lr 0.005904], [time 0.2410]
Whitening Loss tensor([0.0898], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:38:57.394 [epoch 11], [iter 750 / 1548 : 17777], [loss 0.456490], [lr 0.005892], [time 0.2409]
Whitening Loss tensor([0.1393], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:39:46.241 [epoch 11], [iter 800 / 1548 : 17827], [loss 0.431377], [lr 0.005880], [time 0.2409]
Whitening Loss tensor([0.0922], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:40:35.061 [epoch 11], [iter 850 / 1548 : 17877], [loss 0.349675], [lr 0.005868], [time 0.2409]
Whitening Loss tensor([0.0790], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:41:23.845 [epoch 11], [iter 900 / 1548 : 17927], [loss 0.370102], [lr 0.005856], [time 0.2408]
Whitening Loss tensor([0.0871], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:42:12.614 [epoch 11], [iter 950 / 1548 : 17977], [loss 0.441971], [lr 0.005844], [time 0.2406]
Whitening Loss tensor([0.1143], device='cuda:0', grad_fn=<DivBackward0>)
Error!! (957, 526) (1052, 1914) 15188
Dropping  tensor(3048)
11-23 02:43:01.417 [epoch 11], [iter 1000 / 1548 : 18027], [loss 0.404514], [lr 0.005832], [time 0.2408]
Whitening Loss tensor([0.1029], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:43:50.229 [epoch 11], [iter 1050 / 1548 : 18077], [loss 0.379684], [lr 0.005820], [time 0.2409]
Whitening Loss tensor([0.0874], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:44:39.073 [epoch 11], [iter 1100 / 1548 : 18127], [loss 0.397890], [lr 0.005808], [time 0.2410]
Whitening Loss tensor([0.0931], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:45:28.053 [epoch 11], [iter 1150 / 1548 : 18177], [loss 0.383168], [lr 0.005797], [time 0.2416]
Whitening Loss tensor([0.0839], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:46:16.944 [epoch 11], [iter 1200 / 1548 : 18227], [loss 0.422659], [lr 0.005785], [time 0.2412]
Whitening Loss tensor([0.0828], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:47:05.733 [epoch 11], [iter 1250 / 1548 : 18277], [loss 0.386079], [lr 0.005773], [time 0.2407]
Whitening Loss tensor([0.1016], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:47:54.564 [epoch 11], [iter 1300 / 1548 : 18327], [loss 0.367388], [lr 0.005761], [time 0.2409]
Whitening Loss tensor([0.0874], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:48:43.326 [epoch 11], [iter 1350 / 1548 : 18377], [loss 0.455815], [lr 0.005749], [time 0.2406]
Whitening Loss tensor([0.1065], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:49:32.224 [epoch 11], [iter 1400 / 1548 : 18427], [loss 0.440912], [lr 0.005737], [time 0.2412]
Whitening Loss tensor([0.1057], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:50:21.087 [epoch 11], [iter 1450 / 1548 : 18477], [loss 0.372094], [lr 0.005725], [time 0.2411]
Whitening Loss tensor([0.0736], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:51:09.863 [epoch 11], [iter 1500 / 1548 : 18527], [loss 0.393433], [lr 0.005713], [time 0.2407]
Whitening Loss tensor([0.0886], device='cuda:0', grad_fn=<DivBackward0>)
Saving pth file...
11-23 02:51:57.261 Saved file to ./logs/1122/r50os16_gtav_isw/11_22_20/last_None_epoch_11_mean-iu_0.00000.pth
11-23 02:51:57.262 Class Uniform Percentage: 0.5
11-23 02:51:57.262 Class Uniform items per Epoch:12388
11-23 02:51:57.265 cls 0 len 12109
11-23 02:51:57.265 cls 1 len 11833
11-23 02:51:57.265 cls 2 len 12301
11-23 02:51:57.265 cls 3 len 10854
11-23 02:51:57.265 cls 4 len 8811
11-23 02:51:57.265 cls 5 len 11928
11-23 02:51:57.265 cls 6 len 7891
11-23 02:51:57.265 cls 7 len 5921
11-23 02:51:57.265 cls 8 len 12132
11-23 02:51:57.265 cls 9 len 11549
11-23 02:51:57.265 cls 10 len 12131
11-23 02:51:57.265 cls 11 len 10691
11-23 02:51:57.265 cls 12 len 986
11-23 02:51:57.265 cls 13 len 10501
11-23 02:51:57.265 cls 14 len 6711
11-23 02:51:57.266 cls 15 len 1861
11-23 02:51:57.266 cls 16 len 493
11-23 02:51:57.266 cls 17 len 1211
11-23 02:51:57.266 cls 18 len 168
11-23 02:52:53.534 [epoch 12], [iter 50 / 1548 : 18625], [loss 0.402505], [lr 0.005689], [time 0.2419]
Whitening Loss tensor([0.0871], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:53:42.471 [epoch 12], [iter 100 / 1548 : 18675], [loss 0.391554], [lr 0.005677], [time 0.2412]
Whitening Loss tensor([0.0951], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:54:31.278 [epoch 12], [iter 150 / 1548 : 18725], [loss 0.409120], [lr 0.005665], [time 0.2407]
Whitening Loss tensor([0.0924], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:55:20.087 [epoch 12], [iter 200 / 1548 : 18775], [loss 0.380303], [lr 0.005653], [time 0.2408]
Whitening Loss tensor([0.0809], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:56:08.939 [epoch 12], [iter 250 / 1548 : 18825], [loss 0.391665], [lr 0.005641], [time 0.2409]
Whitening Loss tensor([0.0968], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:56:57.795 [epoch 12], [iter 300 / 1548 : 18875], [loss 0.410247], [lr 0.005629], [time 0.2409]
Whitening Loss tensor([0.0976], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:57:46.753 [epoch 12], [iter 350 / 1548 : 18925], [loss 0.344577], [lr 0.005617], [time 0.2414]
Whitening Loss tensor([0.1061], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:58:35.590 [epoch 12], [iter 400 / 1548 : 18975], [loss 0.392275], [lr 0.005605], [time 0.2409]
Whitening Loss tensor([0.0901], device='cuda:0', grad_fn=<DivBackward0>)
11-23 02:59:24.511 [epoch 12], [iter 450 / 1548 : 19025], [loss 0.474040], [lr 0.005593], [time 0.2413]
Whitening Loss tensor([0.1031], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:00:13.415 [epoch 12], [iter 500 / 1548 : 19075], [loss 0.425112], [lr 0.005581], [time 0.2413]
Whitening Loss tensor([0.0865], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:01:02.250 [epoch 12], [iter 550 / 1548 : 19125], [loss 0.393581], [lr 0.005569], [time 0.2408]
Whitening Loss tensor([0.0789], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:01:51.035 [epoch 12], [iter 600 / 1548 : 19175], [loss 0.423228], [lr 0.005557], [time 0.2407]
Whitening Loss tensor([0.0846], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:02:39.835 [epoch 12], [iter 650 / 1548 : 19225], [loss 0.394164], [lr 0.005545], [time 0.2408]
Whitening Loss tensor([0.0927], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:03:28.612 [epoch 12], [iter 700 / 1548 : 19275], [loss 0.409282], [lr 0.005533], [time 0.2407]
Whitening Loss tensor([0.0851], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:04:17.360 [epoch 12], [iter 750 / 1548 : 19325], [loss 0.381969], [lr 0.005521], [time 0.2406]
Whitening Loss tensor([0.0882], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:05:06.199 [epoch 12], [iter 800 / 1548 : 19375], [loss 0.379033], [lr 0.005509], [time 0.2411]
Whitening Loss tensor([0.0801], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:05:54.970 [epoch 12], [iter 850 / 1548 : 19425], [loss 0.414580], [lr 0.005497], [time 0.2406]
Whitening Loss tensor([0.0793], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:06:43.774 [epoch 12], [iter 900 / 1548 : 19475], [loss 0.401388], [lr 0.005485], [time 0.2408]
Whitening Loss tensor([0.0830], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:07:32.567 [epoch 12], [iter 950 / 1548 : 19525], [loss 0.372788], [lr 0.005473], [time 0.2408]
Whitening Loss tensor([0.0817], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:08:21.346 [epoch 12], [iter 1000 / 1548 : 19575], [loss 0.391712], [lr 0.005461], [time 0.2407]
Whitening Loss tensor([0.0930], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:09:10.262 [epoch 12], [iter 1050 / 1548 : 19625], [loss 0.415744], [lr 0.005449], [time 0.2413]
Whitening Loss tensor([0.0967], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:09:59.059 [epoch 12], [iter 1100 / 1548 : 19675], [loss 0.413822], [lr 0.005437], [time 0.2408]
Whitening Loss tensor([0.0837], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:10:47.877 [epoch 12], [iter 1150 / 1548 : 19725], [loss 0.407367], [lr 0.005425], [time 0.2409]
Whitening Loss tensor([0.0952], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:11:36.665 [epoch 12], [iter 1200 / 1548 : 19775], [loss 0.373305], [lr 0.005413], [time 0.2408]
Whitening Loss tensor([0.0835], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:12:25.531 [epoch 12], [iter 1250 / 1548 : 19825], [loss 0.392960], [lr 0.005401], [time 0.2411]
Whitening Loss tensor([0.0999], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:13:14.401 [epoch 12], [iter 1300 / 1548 : 19875], [loss 0.395902], [lr 0.005389], [time 0.2411]
Whitening Loss tensor([0.0895], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:14:03.235 [epoch 12], [iter 1350 / 1548 : 19925], [loss 0.378869], [lr 0.005377], [time 0.2409]
Whitening Loss tensor([0.1015], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:14:52.114 [epoch 12], [iter 1400 / 1548 : 19975], [loss 0.390835], [lr 0.005365], [time 0.2410]
Whitening Loss tensor([0.0954], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:15:40.973 [epoch 12], [iter 1450 / 1548 : 20025], [loss 0.409830], [lr 0.005353], [time 0.2411]
Whitening Loss tensor([0.0927], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:16:29.845 [epoch 12], [iter 1500 / 1548 : 20075], [loss 0.387530], [lr 0.005341], [time 0.2411]
Whitening Loss tensor([0.0925], device='cuda:0', grad_fn=<DivBackward0>)
Saving pth file...
11-23 03:17:17.286 Saved file to ./logs/1122/r50os16_gtav_isw/11_22_20/last_None_epoch_12_mean-iu_0.00000.pth
11-23 03:17:17.287 Class Uniform Percentage: 0.5
11-23 03:17:17.287 Class Uniform items per Epoch:12388
11-23 03:17:17.290 cls 0 len 12109
11-23 03:17:17.290 cls 1 len 11833
11-23 03:17:17.290 cls 2 len 12301
11-23 03:17:17.290 cls 3 len 10854
11-23 03:17:17.290 cls 4 len 8811
11-23 03:17:17.290 cls 5 len 11928
11-23 03:17:17.290 cls 6 len 7891
11-23 03:17:17.290 cls 7 len 5921
11-23 03:17:17.290 cls 8 len 12132
11-23 03:17:17.290 cls 9 len 11549
11-23 03:17:17.290 cls 10 len 12131
11-23 03:17:17.290 cls 11 len 10691
11-23 03:17:17.290 cls 12 len 986
11-23 03:17:17.290 cls 13 len 10501
11-23 03:17:17.290 cls 14 len 6711
11-23 03:17:17.290 cls 15 len 1861
11-23 03:17:17.290 cls 16 len 493
11-23 03:17:17.290 cls 17 len 1211
11-23 03:17:17.290 cls 18 len 168
11-23 03:18:13.473 [epoch 13], [iter 50 / 1548 : 20173], [loss 0.450108], [lr 0.005317], [time 0.2415]
Whitening Loss tensor([0.0992], device='cuda:0', grad_fn=<DivBackward0>)
Error!! (957, 526) (1052, 1914) 15188
Dropping  tensor(8861)
11-23 03:19:02.302 [epoch 13], [iter 100 / 1548 : 20223], [loss 0.429452], [lr 0.005305], [time 0.2409]
Whitening Loss tensor([0.0811], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:19:51.189 [epoch 13], [iter 150 / 1548 : 20273], [loss 0.356640], [lr 0.005293], [time 0.2412]
Whitening Loss tensor([0.0891], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:20:40.111 [epoch 13], [iter 200 / 1548 : 20323], [loss 0.430347], [lr 0.005281], [time 0.2414]
Whitening Loss tensor([0.0803], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:21:28.961 [epoch 13], [iter 250 / 1548 : 20373], [loss 0.370358], [lr 0.005269], [time 0.2411]
Whitening Loss tensor([0.0800], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:22:17.776 [epoch 13], [iter 300 / 1548 : 20423], [loss 0.391851], [lr 0.005257], [time 0.2410]
Whitening Loss tensor([0.0900], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:23:06.507 [epoch 13], [iter 350 / 1548 : 20473], [loss 0.432882], [lr 0.005245], [time 0.2404]
Whitening Loss tensor([0.0871], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:23:55.248 [epoch 13], [iter 400 / 1548 : 20523], [loss 0.383237], [lr 0.005233], [time 0.2406]
Whitening Loss tensor([0.0737], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:24:44.055 [epoch 13], [iter 450 / 1548 : 20573], [loss 0.376665], [lr 0.005220], [time 0.2409]
Whitening Loss tensor([0.1046], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:25:32.941 [epoch 13], [iter 500 / 1548 : 20623], [loss 0.399839], [lr 0.005208], [time 0.2412]
Whitening Loss tensor([0.0844], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:26:21.719 [epoch 13], [iter 550 / 1548 : 20673], [loss 0.385989], [lr 0.005196], [time 0.2407]
Whitening Loss tensor([0.0806], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:27:10.531 [epoch 13], [iter 600 / 1548 : 20723], [loss 0.394869], [lr 0.005184], [time 0.2409]
Whitening Loss tensor([0.0861], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:27:59.377 [epoch 13], [iter 650 / 1548 : 20773], [loss 0.399556], [lr 0.005172], [time 0.2410]
Whitening Loss tensor([0.0923], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:28:48.276 [epoch 13], [iter 700 / 1548 : 20823], [loss 0.391408], [lr 0.005160], [time 0.2412]
Whitening Loss tensor([0.0826], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:29:37.086 [epoch 13], [iter 750 / 1548 : 20873], [loss 0.384798], [lr 0.005148], [time 0.2408]
Whitening Loss tensor([0.0834], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:30:25.919 [epoch 13], [iter 800 / 1548 : 20923], [loss 0.413878], [lr 0.005136], [time 0.2410]
Whitening Loss tensor([0.0847], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:31:14.747 [epoch 13], [iter 850 / 1548 : 20973], [loss 0.405658], [lr 0.005124], [time 0.2410]
Whitening Loss tensor([0.0752], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:32:03.619 [epoch 13], [iter 900 / 1548 : 21023], [loss 0.375412], [lr 0.005112], [time 0.2412]
Whitening Loss tensor([0.0818], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:32:52.524 [epoch 13], [iter 950 / 1548 : 21073], [loss 0.395211], [lr 0.005099], [time 0.2413]
Whitening Loss tensor([0.0875], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:33:41.316 [epoch 13], [iter 1000 / 1548 : 21123], [loss 0.381890], [lr 0.005087], [time 0.2408]
Whitening Loss tensor([0.0796], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:34:30.202 [epoch 13], [iter 1050 / 1548 : 21173], [loss 0.364823], [lr 0.005075], [time 0.2412]
Whitening Loss tensor([0.0822], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:35:19.052 [epoch 13], [iter 1100 / 1548 : 21223], [loss 0.389015], [lr 0.005063], [time 0.2410]
Whitening Loss tensor([0.1071], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:36:07.832 [epoch 13], [iter 1150 / 1548 : 21273], [loss 0.392431], [lr 0.005051], [time 0.2407]
Whitening Loss tensor([0.0935], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:36:56.557 [epoch 13], [iter 1200 / 1548 : 21323], [loss 0.376359], [lr 0.005039], [time 0.2404]
Whitening Loss tensor([0.0788], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:37:45.451 [epoch 13], [iter 1250 / 1548 : 21373], [loss 0.396505], [lr 0.005027], [time 0.2413]
Whitening Loss tensor([0.0832], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:38:34.241 [epoch 13], [iter 1300 / 1548 : 21423], [loss 0.375068], [lr 0.005014], [time 0.2407]
Whitening Loss tensor([0.0767], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:39:23.030 [epoch 13], [iter 1350 / 1548 : 21473], [loss 0.417819], [lr 0.005002], [time 0.2407]
Whitening Loss tensor([0.0986], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:40:11.890 [epoch 13], [iter 1400 / 1548 : 21523], [loss 0.410366], [lr 0.004990], [time 0.2410]
Whitening Loss tensor([0.0943], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:41:00.725 [epoch 13], [iter 1450 / 1548 : 21573], [loss 0.367816], [lr 0.004978], [time 0.2410]
Whitening Loss tensor([0.0847], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:41:49.562 [epoch 13], [iter 1500 / 1548 : 21623], [loss 0.361149], [lr 0.004966], [time 0.2409]
Whitening Loss tensor([0.0879], device='cuda:0', grad_fn=<DivBackward0>)
Saving pth file...
11-23 03:42:36.884 Saved file to ./logs/1122/r50os16_gtav_isw/11_22_20/last_None_epoch_13_mean-iu_0.00000.pth
11-23 03:42:36.885 Class Uniform Percentage: 0.5
11-23 03:42:36.886 Class Uniform items per Epoch:12388
11-23 03:42:36.889 cls 0 len 12109
11-23 03:42:36.889 cls 1 len 11833
11-23 03:42:36.889 cls 2 len 12301
11-23 03:42:36.889 cls 3 len 10854
11-23 03:42:36.890 cls 4 len 8811
11-23 03:42:36.890 cls 5 len 11928
11-23 03:42:36.890 cls 6 len 7891
11-23 03:42:36.890 cls 7 len 5921
11-23 03:42:36.890 cls 8 len 12132
11-23 03:42:36.890 cls 9 len 11549
11-23 03:42:36.890 cls 10 len 12131
11-23 03:42:36.890 cls 11 len 10691
11-23 03:42:36.890 cls 12 len 986
11-23 03:42:36.890 cls 13 len 10501
11-23 03:42:36.890 cls 14 len 6711
11-23 03:42:36.890 cls 15 len 1861
11-23 03:42:36.890 cls 16 len 493
11-23 03:42:36.890 cls 17 len 1211
11-23 03:42:36.890 cls 18 len 168
11-23 03:43:32.843 [epoch 14], [iter 50 / 1548 : 21721], [loss 0.359541], [lr 0.004942], [time 0.2418]
Whitening Loss tensor([0.0746], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:44:21.679 [epoch 14], [iter 100 / 1548 : 21771], [loss 0.366998], [lr 0.004930], [time 0.2409]
Whitening Loss tensor([0.0796], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:45:10.527 [epoch 14], [iter 150 / 1548 : 21821], [loss 0.390324], [lr 0.004918], [time 0.2410]
Whitening Loss tensor([0.0763], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:45:59.378 [epoch 14], [iter 200 / 1548 : 21871], [loss 0.406950], [lr 0.004905], [time 0.2410]
Whitening Loss tensor([0.0899], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:46:48.200 [epoch 14], [iter 250 / 1548 : 21921], [loss 0.392281], [lr 0.004893], [time 0.2409]
Whitening Loss tensor([0.0922], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:47:37.019 [epoch 14], [iter 300 / 1548 : 21971], [loss 0.390017], [lr 0.004881], [time 0.2409]
Whitening Loss tensor([0.0976], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:48:25.810 [epoch 14], [iter 350 / 1548 : 22021], [loss 0.426704], [lr 0.004869], [time 0.2408]
Whitening Loss tensor([0.0986], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:49:14.618 [epoch 14], [iter 400 / 1548 : 22071], [loss 0.408522], [lr 0.004857], [time 0.2408]
Whitening Loss tensor([0.0899], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:50:03.381 [epoch 14], [iter 450 / 1548 : 22121], [loss 0.363890], [lr 0.004845], [time 0.2406]
Whitening Loss tensor([0.0770], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:50:52.191 [epoch 14], [iter 500 / 1548 : 22171], [loss 0.345314], [lr 0.004832], [time 0.2409]
Whitening Loss tensor([0.0893], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:51:41.036 [epoch 14], [iter 550 / 1548 : 22221], [loss 0.349844], [lr 0.004820], [time 0.2409]
Whitening Loss tensor([0.0743], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:52:29.893 [epoch 14], [iter 600 / 1548 : 22271], [loss 0.363068], [lr 0.004808], [time 0.2411]
Whitening Loss tensor([0.0894], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:53:18.763 [epoch 14], [iter 650 / 1548 : 22321], [loss 0.373027], [lr 0.004796], [time 0.2411]
Whitening Loss tensor([0.0850], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:54:07.577 [epoch 14], [iter 700 / 1548 : 22371], [loss 0.370313], [lr 0.004784], [time 0.2409]
Whitening Loss tensor([0.0948], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:54:56.443 [epoch 14], [iter 750 / 1548 : 22421], [loss 0.409714], [lr 0.004771], [time 0.2412]
Whitening Loss tensor([0.0835], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:55:45.227 [epoch 14], [iter 800 / 1548 : 22471], [loss 0.387459], [lr 0.004759], [time 0.2407]
Whitening Loss tensor([0.0777], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:56:34.042 [epoch 14], [iter 850 / 1548 : 22521], [loss 0.370824], [lr 0.004747], [time 0.2409]
Whitening Loss tensor([0.0901], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:57:22.821 [epoch 14], [iter 900 / 1548 : 22571], [loss 0.399909], [lr 0.004735], [time 0.2407]
Whitening Loss tensor([0.0745], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:58:11.711 [epoch 14], [iter 950 / 1548 : 22621], [loss 0.380098], [lr 0.004722], [time 0.2412]
Whitening Loss tensor([0.0957], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:59:00.515 [epoch 14], [iter 1000 / 1548 : 22671], [loss 0.342409], [lr 0.004710], [time 0.2408]
Whitening Loss tensor([0.0890], device='cuda:0', grad_fn=<DivBackward0>)
11-23 03:59:49.340 [epoch 14], [iter 1050 / 1548 : 22721], [loss 0.366612], [lr 0.004698], [time 0.2409]
Whitening Loss tensor([0.0844], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:00:38.238 [epoch 14], [iter 1100 / 1548 : 22771], [loss 0.380528], [lr 0.004686], [time 0.2412]
Whitening Loss tensor([0.0848], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:01:27.170 [epoch 14], [iter 1150 / 1548 : 22821], [loss 0.359635], [lr 0.004674], [time 0.2414]
Whitening Loss tensor([0.0815], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:02:16.053 [epoch 14], [iter 1200 / 1548 : 22871], [loss 0.382796], [lr 0.004661], [time 0.2412]
Whitening Loss tensor([0.1041], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:03:04.831 [epoch 14], [iter 1250 / 1548 : 22921], [loss 0.373772], [lr 0.004649], [time 0.2407]
Whitening Loss tensor([0.0825], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:03:53.704 [epoch 14], [iter 1300 / 1548 : 22971], [loss 0.389712], [lr 0.004637], [time 0.2411]
Whitening Loss tensor([0.0825], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:04:42.599 [epoch 14], [iter 1350 / 1548 : 23021], [loss 0.389157], [lr 0.004625], [time 0.2412]
Whitening Loss tensor([0.0803], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:05:31.469 [epoch 14], [iter 1400 / 1548 : 23071], [loss 0.392292], [lr 0.004612], [time 0.2411]
Whitening Loss tensor([0.1013], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:06:20.237 [epoch 14], [iter 1450 / 1548 : 23121], [loss 0.358689], [lr 0.004600], [time 0.2406]
Whitening Loss tensor([0.0829], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:07:09.104 [epoch 14], [iter 1500 / 1548 : 23171], [loss 0.333187], [lr 0.004588], [time 0.2411]
Whitening Loss tensor([0.0691], device='cuda:0', grad_fn=<DivBackward0>)
Saving pth file...
11-23 04:07:56.464 Saved file to ./logs/1122/r50os16_gtav_isw/11_22_20/last_None_epoch_14_mean-iu_0.00000.pth
11-23 04:07:56.466 Class Uniform Percentage: 0.5
11-23 04:07:56.466 Class Uniform items per Epoch:12388
11-23 04:07:56.468 cls 0 len 12109
11-23 04:07:56.468 cls 1 len 11833
11-23 04:07:56.469 cls 2 len 12301
11-23 04:07:56.469 cls 3 len 10854
11-23 04:07:56.469 cls 4 len 8811
11-23 04:07:56.469 cls 5 len 11928
11-23 04:07:56.469 cls 6 len 7891
11-23 04:07:56.469 cls 7 len 5921
11-23 04:07:56.469 cls 8 len 12132
11-23 04:07:56.469 cls 9 len 11549
11-23 04:07:56.469 cls 10 len 12131
11-23 04:07:56.469 cls 11 len 10691
11-23 04:07:56.469 cls 12 len 986
11-23 04:07:56.469 cls 13 len 10501
11-23 04:07:56.469 cls 14 len 6711
11-23 04:07:56.469 cls 15 len 1861
11-23 04:07:56.469 cls 16 len 493
11-23 04:07:56.469 cls 17 len 1211
11-23 04:07:56.469 cls 18 len 168
11-23 04:08:52.916 [epoch 15], [iter 50 / 1548 : 23269], [loss 0.345584], [lr 0.004564], [time 0.2424]
Whitening Loss tensor([0.0828], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:09:42.194 [epoch 15], [iter 100 / 1548 : 23319], [loss 0.384419], [lr 0.004551], [time 0.2423]
Whitening Loss tensor([0.0798], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:10:31.571 [epoch 15], [iter 150 / 1548 : 23369], [loss 0.388526], [lr 0.004539], [time 0.2428]
Whitening Loss tensor([0.0748], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:11:20.834 [epoch 15], [iter 200 / 1548 : 23419], [loss 0.363893], [lr 0.004527], [time 0.2423]
Whitening Loss tensor([0.0885], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:12:10.226 [epoch 15], [iter 250 / 1548 : 23469], [loss 0.332128], [lr 0.004515], [time 0.2429]
Whitening Loss tensor([0.0761], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:12:59.526 [epoch 15], [iter 300 / 1548 : 23519], [loss 0.365997], [lr 0.004502], [time 0.2423]
Whitening Loss tensor([0.0765], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:13:48.868 [epoch 15], [iter 350 / 1548 : 23569], [loss 0.367978], [lr 0.004490], [time 0.2426]
Whitening Loss tensor([0.0812], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:14:38.246 [epoch 15], [iter 400 / 1548 : 23619], [loss 0.380630], [lr 0.004478], [time 0.2426]
Whitening Loss tensor([0.0826], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:15:27.615 [epoch 15], [iter 450 / 1548 : 23669], [loss 0.375619], [lr 0.004465], [time 0.2428]
Whitening Loss tensor([0.0738], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:16:16.987 [epoch 15], [iter 500 / 1548 : 23719], [loss 0.449955], [lr 0.004453], [time 0.2427]
Whitening Loss tensor([0.0822], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:17:06.239 [epoch 15], [iter 550 / 1548 : 23769], [loss 0.443071], [lr 0.004441], [time 0.2422]
Whitening Loss tensor([0.0820], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:17:55.604 [epoch 15], [iter 600 / 1548 : 23819], [loss 0.403843], [lr 0.004428], [time 0.2426]
Whitening Loss tensor([0.0759], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:18:44.943 [epoch 15], [iter 650 / 1548 : 23869], [loss 0.338142], [lr 0.004416], [time 0.2424]
Whitening Loss tensor([0.0974], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:19:34.309 [epoch 15], [iter 700 / 1548 : 23919], [loss 0.428410], [lr 0.004404], [time 0.2426]
Whitening Loss tensor([0.0969], device='cuda:0', grad_fn=<DivBackward0>)
Error!! (957, 526) (1052, 1914) 15188
Dropping  tensor(398)
11-23 04:20:23.652 [epoch 15], [iter 750 / 1548 : 23969], [loss 0.404032], [lr 0.004391], [time 0.2426]
Whitening Loss tensor([0.0922], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:21:12.862 [epoch 15], [iter 800 / 1548 : 24019], [loss 0.429162], [lr 0.004379], [time 0.2421]
Whitening Loss tensor([0.1228], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:22:02.225 [epoch 15], [iter 850 / 1548 : 24069], [loss 0.414795], [lr 0.004367], [time 0.2427]
Whitening Loss tensor([0.1165], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:22:51.566 [epoch 15], [iter 900 / 1548 : 24119], [loss 0.363937], [lr 0.004354], [time 0.2426]
Whitening Loss tensor([0.1096], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:23:40.929 [epoch 15], [iter 950 / 1548 : 24169], [loss 0.405136], [lr 0.004342], [time 0.2426]
Whitening Loss tensor([0.1150], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:24:30.177 [epoch 15], [iter 1000 / 1548 : 24219], [loss 0.404137], [lr 0.004330], [time 0.2422]
Whitening Loss tensor([0.1102], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:25:19.505 [epoch 15], [iter 1050 / 1548 : 24269], [loss 0.413982], [lr 0.004317], [time 0.2426]
Whitening Loss tensor([0.1119], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:26:08.935 [epoch 15], [iter 1100 / 1548 : 24319], [loss 0.405829], [lr 0.004305], [time 0.2430]
Whitening Loss tensor([0.1052], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:26:58.312 [epoch 15], [iter 1150 / 1548 : 24369], [loss 0.415813], [lr 0.004293], [time 0.2427]
Whitening Loss tensor([0.0998], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:27:47.527 [epoch 15], [iter 1200 / 1548 : 24419], [loss 0.375509], [lr 0.004280], [time 0.2421]
Whitening Loss tensor([0.0816], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:28:36.818 [epoch 15], [iter 1250 / 1548 : 24469], [loss 0.428688], [lr 0.004268], [time 0.2423]
Whitening Loss tensor([0.1028], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:29:26.162 [epoch 15], [iter 1300 / 1548 : 24519], [loss 0.376033], [lr 0.004256], [time 0.2429]
Whitening Loss tensor([0.0847], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:30:15.494 [epoch 15], [iter 1350 / 1548 : 24569], [loss 0.357500], [lr 0.004243], [time 0.2426]
Whitening Loss tensor([0.0888], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:31:04.795 [epoch 15], [iter 1400 / 1548 : 24619], [loss 0.411874], [lr 0.004231], [time 0.2424]
Whitening Loss tensor([0.0827], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:31:54.272 [epoch 15], [iter 1450 / 1548 : 24669], [loss 0.380530], [lr 0.004219], [time 0.2433]
Whitening Loss tensor([0.0984], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:32:43.627 [epoch 15], [iter 1500 / 1548 : 24719], [loss 0.381862], [lr 0.004206], [time 0.2426]
Whitening Loss tensor([0.1028], device='cuda:0', grad_fn=<DivBackward0>)
Error!! (957, 526) (1052, 1914) 15188
Dropping  tensor(6329)
Saving pth file...
11-23 04:33:31.391 Saved file to ./logs/1122/r50os16_gtav_isw/11_22_20/last_None_epoch_15_mean-iu_0.00000.pth
11-23 04:33:31.392 Class Uniform Percentage: 0.5
11-23 04:33:31.392 Class Uniform items per Epoch:12388
11-23 04:33:31.395 cls 0 len 12109
11-23 04:33:31.396 cls 1 len 11833
11-23 04:33:31.396 cls 2 len 12301
11-23 04:33:31.396 cls 3 len 10854
11-23 04:33:31.396 cls 4 len 8811
11-23 04:33:31.396 cls 5 len 11928
11-23 04:33:31.396 cls 6 len 7891
11-23 04:33:31.396 cls 7 len 5921
11-23 04:33:31.396 cls 8 len 12132
11-23 04:33:31.396 cls 9 len 11549
11-23 04:33:31.396 cls 10 len 12131
11-23 04:33:31.396 cls 11 len 10691
11-23 04:33:31.396 cls 12 len 986
11-23 04:33:31.396 cls 13 len 10501
11-23 04:33:31.396 cls 14 len 6711
11-23 04:33:31.396 cls 15 len 1861
11-23 04:33:31.396 cls 16 len 493
11-23 04:33:31.396 cls 17 len 1211
11-23 04:33:31.396 cls 18 len 168
11-23 04:34:27.336 [epoch 16], [iter 50 / 1548 : 24817], [loss 0.384568], [lr 0.004182], [time 0.2419]
Whitening Loss tensor([0.0767], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:35:16.414 [epoch 16], [iter 100 / 1548 : 24867], [loss 0.376149], [lr 0.004169], [time 0.2416]
Whitening Loss tensor([0.0855], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:36:05.552 [epoch 16], [iter 150 / 1548 : 24917], [loss 0.340164], [lr 0.004157], [time 0.2419]
Whitening Loss tensor([0.0702], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:36:54.599 [epoch 16], [iter 200 / 1548 : 24967], [loss 0.381514], [lr 0.004145], [time 0.2415]
Whitening Loss tensor([0.0905], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:37:43.619 [epoch 16], [iter 250 / 1548 : 25017], [loss 0.400671], [lr 0.004132], [time 0.2413]
Whitening Loss tensor([0.0690], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:38:32.683 [epoch 16], [iter 300 / 1548 : 25067], [loss 0.399063], [lr 0.004120], [time 0.2415]
Whitening Loss tensor([0.0886], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:39:21.744 [epoch 16], [iter 350 / 1548 : 25117], [loss 0.401344], [lr 0.004107], [time 0.2416]
Whitening Loss tensor([0.0754], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:40:10.792 [epoch 16], [iter 400 / 1548 : 25167], [loss 0.376475], [lr 0.004095], [time 0.2415]
Whitening Loss tensor([0.0720], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:40:59.863 [epoch 16], [iter 450 / 1548 : 25217], [loss 0.356586], [lr 0.004083], [time 0.2416]
Whitening Loss tensor([0.0766], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:41:48.946 [epoch 16], [iter 500 / 1548 : 25267], [loss 0.335705], [lr 0.004070], [time 0.2417]
Whitening Loss tensor([0.0727], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:42:37.931 [epoch 16], [iter 550 / 1548 : 25317], [loss 0.349482], [lr 0.004058], [time 0.2413]
Whitening Loss tensor([0.0720], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:43:27.005 [epoch 16], [iter 600 / 1548 : 25367], [loss 0.345869], [lr 0.004045], [time 0.2416]
Whitening Loss tensor([0.0790], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:44:16.017 [epoch 16], [iter 650 / 1548 : 25417], [loss 0.351879], [lr 0.004033], [time 0.2413]
Whitening Loss tensor([0.0613], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:45:05.072 [epoch 16], [iter 700 / 1548 : 25467], [loss 0.363262], [lr 0.004020], [time 0.2415]
Whitening Loss tensor([0.0722], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:45:54.149 [epoch 16], [iter 750 / 1548 : 25517], [loss 0.397554], [lr 0.004008], [time 0.2416]
Whitening Loss tensor([0.0761], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:46:43.209 [epoch 16], [iter 800 / 1548 : 25567], [loss 0.401937], [lr 0.003995], [time 0.2416]
Whitening Loss tensor([0.0775], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:47:32.258 [epoch 16], [iter 850 / 1548 : 25617], [loss 0.396313], [lr 0.003983], [time 0.2415]
Whitening Loss tensor([0.0873], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:48:21.333 [epoch 16], [iter 900 / 1548 : 25667], [loss 0.340928], [lr 0.003971], [time 0.2416]
Whitening Loss tensor([0.0914], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:49:10.449 [epoch 16], [iter 950 / 1548 : 25717], [loss 0.322322], [lr 0.003958], [time 0.2418]
Whitening Loss tensor([0.0733], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:49:59.477 [epoch 16], [iter 1000 / 1548 : 25767], [loss 0.394293], [lr 0.003946], [time 0.2415]
Whitening Loss tensor([0.0929], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:50:48.589 [epoch 16], [iter 1050 / 1548 : 25817], [loss 0.386418], [lr 0.003933], [time 0.2418]
Whitening Loss tensor([0.0782], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:51:37.632 [epoch 16], [iter 1100 / 1548 : 25867], [loss 0.353042], [lr 0.003921], [time 0.2414]
Whitening Loss tensor([0.0786], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:52:26.734 [epoch 16], [iter 1150 / 1548 : 25917], [loss 0.341085], [lr 0.003908], [time 0.2417]
Whitening Loss tensor([0.0962], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:53:15.718 [epoch 16], [iter 1200 / 1548 : 25967], [loss 0.364263], [lr 0.003896], [time 0.2412]
Whitening Loss tensor([0.1001], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:54:04.808 [epoch 16], [iter 1250 / 1548 : 26017], [loss 0.325433], [lr 0.003883], [time 0.2416]
Whitening Loss tensor([0.0849], device='cuda:0', grad_fn=<DivBackward0>)
Error!! (957, 526) (1052, 1914) 15188
Dropping  tensor(775)
11-23 04:54:53.986 [epoch 16], [iter 1300 / 1548 : 26067], [loss 0.330435], [lr 0.003871], [time 0.2421]
Whitening Loss tensor([0.0741], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:55:43.073 [epoch 16], [iter 1350 / 1548 : 26117], [loss 0.375825], [lr 0.003858], [time 0.2416]
Whitening Loss tensor([0.0897], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:56:32.208 [epoch 16], [iter 1400 / 1548 : 26167], [loss 0.358853], [lr 0.003846], [time 0.2419]
Whitening Loss tensor([0.0721], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:57:21.183 [epoch 16], [iter 1450 / 1548 : 26217], [loss 0.344368], [lr 0.003833], [time 0.2411]
Whitening Loss tensor([0.0698], device='cuda:0', grad_fn=<DivBackward0>)
11-23 04:58:10.270 [epoch 16], [iter 1500 / 1548 : 26267], [loss 0.376756], [lr 0.003821], [time 0.2417]
Whitening Loss tensor([0.0769], device='cuda:0', grad_fn=<DivBackward0>)
Saving pth file...
11-23 04:58:57.978 Saved file to ./logs/1122/r50os16_gtav_isw/11_22_20/last_None_epoch_16_mean-iu_0.00000.pth
11-23 04:58:57.980 Class Uniform Percentage: 0.5
11-23 04:58:57.980 Class Uniform items per Epoch:12388
11-23 04:58:57.983 cls 0 len 12109
11-23 04:58:57.983 cls 1 len 11833
11-23 04:58:57.983 cls 2 len 12301
11-23 04:58:57.983 cls 3 len 10854
11-23 04:58:57.983 cls 4 len 8811
11-23 04:58:57.983 cls 5 len 11928
11-23 04:58:57.983 cls 6 len 7891
11-23 04:58:57.983 cls 7 len 5921
11-23 04:58:57.983 cls 8 len 12132
11-23 04:58:57.983 cls 9 len 11549
11-23 04:58:57.984 cls 10 len 12131
11-23 04:58:57.984 cls 11 len 10691
11-23 04:58:57.984 cls 12 len 986
11-23 04:58:57.984 cls 13 len 10501
11-23 04:58:57.984 cls 14 len 6711
11-23 04:58:57.984 cls 15 len 1861
11-23 04:58:57.984 cls 16 len 493
11-23 04:58:57.984 cls 17 len 1211
11-23 04:58:57.984 cls 18 len 168
11-23 04:59:54.427 [epoch 17], [iter 50 / 1548 : 26365], [loss 0.452286], [lr 0.003796], [time 0.2434]
Whitening Loss tensor([0.0698], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:00:43.614 [epoch 17], [iter 100 / 1548 : 26415], [loss 0.418460], [lr 0.003784], [time 0.2421]
Whitening Loss tensor([0.0789], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:01:32.756 [epoch 17], [iter 150 / 1548 : 26465], [loss 0.376488], [lr 0.003771], [time 0.2418]
Whitening Loss tensor([0.1017], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:02:21.876 [epoch 17], [iter 200 / 1548 : 26515], [loss 0.346078], [lr 0.003758], [time 0.2416]
Whitening Loss tensor([0.0795], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:03:11.047 [epoch 17], [iter 250 / 1548 : 26565], [loss 0.347119], [lr 0.003746], [time 0.2419]
Whitening Loss tensor([0.0731], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:04:00.176 [epoch 17], [iter 300 / 1548 : 26615], [loss 0.342716], [lr 0.003733], [time 0.2417]
Whitening Loss tensor([0.0646], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:04:49.449 [epoch 17], [iter 350 / 1548 : 26665], [loss 0.363515], [lr 0.003721], [time 0.2423]
Whitening Loss tensor([0.0809], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:05:38.593 [epoch 17], [iter 400 / 1548 : 26715], [loss 0.362486], [lr 0.003708], [time 0.2418]
Whitening Loss tensor([0.0838], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:06:27.768 [epoch 17], [iter 450 / 1548 : 26765], [loss 0.344684], [lr 0.003696], [time 0.2418]
Whitening Loss tensor([0.0740], device='cuda:0', grad_fn=<DivBackward0>)
Error!! (957, 526) (1052, 1914) 15188
Dropping  tensor(4498)
11-23 05:07:16.943 [epoch 17], [iter 500 / 1548 : 26815], [loss 0.337959], [lr 0.003683], [time 0.2420]
Whitening Loss tensor([0.0658], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:08:06.130 [epoch 17], [iter 550 / 1548 : 26865], [loss 0.332081], [lr 0.003671], [time 0.2421]
Whitening Loss tensor([0.0734], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:08:55.323 [epoch 17], [iter 600 / 1548 : 26915], [loss 0.335905], [lr 0.003658], [time 0.2421]
Whitening Loss tensor([0.0683], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:09:44.604 [epoch 17], [iter 650 / 1548 : 26965], [loss 0.343904], [lr 0.003645], [time 0.2424]
Whitening Loss tensor([0.0683], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:10:33.796 [epoch 17], [iter 700 / 1548 : 27015], [loss 0.352958], [lr 0.003633], [time 0.2421]
Whitening Loss tensor([0.0889], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:11:23.165 [epoch 17], [iter 750 / 1548 : 27065], [loss 0.390671], [lr 0.003620], [time 0.2429]
Whitening Loss tensor([0.0759], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:12:12.378 [epoch 17], [iter 800 / 1548 : 27115], [loss 0.320564], [lr 0.003608], [time 0.2423]
Whitening Loss tensor([0.0697], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:13:01.727 [epoch 17], [iter 850 / 1548 : 27165], [loss 0.361085], [lr 0.003595], [time 0.2428]
Whitening Loss tensor([0.0710], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:13:51.033 [epoch 17], [iter 900 / 1548 : 27215], [loss 0.345836], [lr 0.003582], [time 0.2424]
Whitening Loss tensor([0.0706], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:14:40.150 [epoch 17], [iter 950 / 1548 : 27265], [loss 0.399384], [lr 0.003570], [time 0.2417]
Whitening Loss tensor([0.0811], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:15:29.420 [epoch 17], [iter 1000 / 1548 : 27315], [loss 0.377193], [lr 0.003557], [time 0.2425]
Whitening Loss tensor([0.0797], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:16:18.554 [epoch 17], [iter 1050 / 1548 : 27365], [loss 0.406439], [lr 0.003545], [time 0.2417]
Whitening Loss tensor([0.0743], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:17:07.814 [epoch 17], [iter 1100 / 1548 : 27415], [loss 0.397596], [lr 0.003532], [time 0.2423]
Whitening Loss tensor([0.0718], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:17:57.075 [epoch 17], [iter 1150 / 1548 : 27465], [loss 0.375705], [lr 0.003519], [time 0.2423]
Whitening Loss tensor([0.0955], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:18:46.339 [epoch 17], [iter 1200 / 1548 : 27515], [loss 0.351507], [lr 0.003507], [time 0.2423]
Whitening Loss tensor([0.0769], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:19:35.543 [epoch 17], [iter 1250 / 1548 : 27565], [loss 0.340464], [lr 0.003494], [time 0.2420]
Whitening Loss tensor([0.0720], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:20:24.732 [epoch 17], [iter 1300 / 1548 : 27615], [loss 0.368024], [lr 0.003481], [time 0.2420]
Whitening Loss tensor([0.0771], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:21:13.875 [epoch 17], [iter 1350 / 1548 : 27665], [loss 0.325203], [lr 0.003469], [time 0.2417]
Whitening Loss tensor([0.0694], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:22:03.024 [epoch 17], [iter 1400 / 1548 : 27715], [loss 0.353394], [lr 0.003456], [time 0.2418]
Whitening Loss tensor([0.0769], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:22:52.257 [epoch 17], [iter 1450 / 1548 : 27765], [loss 0.336573], [lr 0.003443], [time 0.2422]
Whitening Loss tensor([0.0672], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:23:41.441 [epoch 17], [iter 1500 / 1548 : 27815], [loss 0.343755], [lr 0.003431], [time 0.2419]
Whitening Loss tensor([0.0809], device='cuda:0', grad_fn=<DivBackward0>)
Saving pth file...
11-23 05:24:29.103 Saved file to ./logs/1122/r50os16_gtav_isw/11_22_20/last_None_epoch_17_mean-iu_0.00000.pth
11-23 05:24:29.104 Class Uniform Percentage: 0.5
11-23 05:24:29.104 Class Uniform items per Epoch:12388
11-23 05:24:29.107 cls 0 len 12109
11-23 05:24:29.107 cls 1 len 11833
11-23 05:24:29.107 cls 2 len 12301
11-23 05:24:29.107 cls 3 len 10854
11-23 05:24:29.107 cls 4 len 8811
11-23 05:24:29.107 cls 5 len 11928
11-23 05:24:29.107 cls 6 len 7891
11-23 05:24:29.107 cls 7 len 5921
11-23 05:24:29.107 cls 8 len 12132
11-23 05:24:29.107 cls 9 len 11549
11-23 05:24:29.107 cls 10 len 12131
11-23 05:24:29.107 cls 11 len 10691
11-23 05:24:29.107 cls 12 len 986
11-23 05:24:29.107 cls 13 len 10501
11-23 05:24:29.107 cls 14 len 6711
11-23 05:24:29.107 cls 15 len 1861
11-23 05:24:29.107 cls 16 len 493
11-23 05:24:29.107 cls 17 len 1211
11-23 05:24:29.107 cls 18 len 168
11-23 05:25:25.326 [epoch 18], [iter 50 / 1548 : 27913], [loss 0.309949], [lr 0.003406], [time 0.2436]
Whitening Loss tensor([0.0734], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:26:14.645 [epoch 18], [iter 100 / 1548 : 27963], [loss 0.308387], [lr 0.003393], [time 0.2427]
Whitening Loss tensor([0.0646], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:27:04.076 [epoch 18], [iter 150 / 1548 : 28013], [loss 0.334369], [lr 0.003381], [time 0.2431]
Whitening Loss tensor([0.0721], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:27:53.378 [epoch 18], [iter 200 / 1548 : 28063], [loss 0.347765], [lr 0.003368], [time 0.2427]
Whitening Loss tensor([0.0667], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:28:42.802 [epoch 18], [iter 250 / 1548 : 28113], [loss 0.323078], [lr 0.003355], [time 0.2431]
Whitening Loss tensor([0.0730], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:29:32.177 [epoch 18], [iter 300 / 1548 : 28163], [loss 0.358042], [lr 0.003342], [time 0.2429]
Whitening Loss tensor([0.0742], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:30:21.468 [epoch 18], [iter 350 / 1548 : 28213], [loss 0.298543], [lr 0.003330], [time 0.2424]
Whitening Loss tensor([0.0631], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:31:10.760 [epoch 18], [iter 400 / 1548 : 28263], [loss 0.326179], [lr 0.003317], [time 0.2426]
Whitening Loss tensor([0.0770], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:32:00.085 [epoch 18], [iter 450 / 1548 : 28313], [loss 0.327167], [lr 0.003304], [time 0.2426]
Whitening Loss tensor([0.0645], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:32:49.460 [epoch 18], [iter 500 / 1548 : 28363], [loss 0.381822], [lr 0.003292], [time 0.2429]
Whitening Loss tensor([0.0654], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:33:38.780 [epoch 18], [iter 550 / 1548 : 28413], [loss 0.338933], [lr 0.003279], [time 0.2426]
Whitening Loss tensor([0.0853], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:34:28.124 [epoch 18], [iter 600 / 1548 : 28463], [loss 0.325637], [lr 0.003266], [time 0.2426]
Whitening Loss tensor([0.0648], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:35:17.516 [epoch 18], [iter 650 / 1548 : 28513], [loss 0.360203], [lr 0.003253], [time 0.2426]
Whitening Loss tensor([0.0792], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:36:06.870 [epoch 18], [iter 700 / 1548 : 28563], [loss 0.330078], [lr 0.003241], [time 0.2424]
Whitening Loss tensor([0.0643], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:36:56.177 [epoch 18], [iter 750 / 1548 : 28613], [loss 0.296927], [lr 0.003228], [time 0.2423]
Whitening Loss tensor([0.0743], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:37:45.480 [epoch 18], [iter 800 / 1548 : 28663], [loss 0.335959], [lr 0.003215], [time 0.2424]
Whitening Loss tensor([0.0667], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:38:34.737 [epoch 18], [iter 850 / 1548 : 28713], [loss 0.296545], [lr 0.003202], [time 0.2423]
Whitening Loss tensor([0.0781], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:39:24.069 [epoch 18], [iter 900 / 1548 : 28763], [loss 0.306217], [lr 0.003190], [time 0.2425]
Whitening Loss tensor([0.0600], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:40:13.391 [epoch 18], [iter 950 / 1548 : 28813], [loss 0.357068], [lr 0.003177], [time 0.2425]
Whitening Loss tensor([0.0718], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:41:02.703 [epoch 18], [iter 1000 / 1548 : 28863], [loss 0.385252], [lr 0.003164], [time 0.2425]
Whitening Loss tensor([0.0838], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:41:52.041 [epoch 18], [iter 1050 / 1548 : 28913], [loss 0.333861], [lr 0.003151], [time 0.2424]
Whitening Loss tensor([0.0714], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:42:41.368 [epoch 18], [iter 1100 / 1548 : 28963], [loss 0.335671], [lr 0.003138], [time 0.2424]
Whitening Loss tensor([0.0761], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:43:30.731 [epoch 18], [iter 1150 / 1548 : 29013], [loss 0.374464], [lr 0.003126], [time 0.2424]
Whitening Loss tensor([0.0991], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:44:20.044 [epoch 18], [iter 1200 / 1548 : 29063], [loss 0.343996], [lr 0.003113], [time 0.2424]
Whitening Loss tensor([0.0754], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:45:09.384 [epoch 18], [iter 1250 / 1548 : 29113], [loss 0.342760], [lr 0.003100], [time 0.2424]
Whitening Loss tensor([0.0914], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:45:58.694 [epoch 18], [iter 1300 / 1548 : 29163], [loss 0.293713], [lr 0.003087], [time 0.2424]
Whitening Loss tensor([0.0701], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:46:47.940 [epoch 18], [iter 1350 / 1548 : 29213], [loss 0.321128], [lr 0.003074], [time 0.2421]
Whitening Loss tensor([0.0677], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:47:37.256 [epoch 18], [iter 1400 / 1548 : 29263], [loss 0.341036], [lr 0.003062], [time 0.2424]
Whitening Loss tensor([0.0708], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:48:26.472 [epoch 18], [iter 1450 / 1548 : 29313], [loss 0.345821], [lr 0.003049], [time 0.2421]
Whitening Loss tensor([0.0676], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:49:15.738 [epoch 18], [iter 1500 / 1548 : 29363], [loss 0.309804], [lr 0.003036], [time 0.2422]
Whitening Loss tensor([0.0570], device='cuda:0', grad_fn=<DivBackward0>)
Saving pth file...
11-23 05:50:03.577 Saved file to ./logs/1122/r50os16_gtav_isw/11_22_20/last_None_epoch_18_mean-iu_0.00000.pth
11-23 05:50:03.578 Class Uniform Percentage: 0.5
11-23 05:50:03.578 Class Uniform items per Epoch:12388
11-23 05:50:03.581 cls 0 len 12109
11-23 05:50:03.581 cls 1 len 11833
11-23 05:50:03.581 cls 2 len 12301
11-23 05:50:03.581 cls 3 len 10854
11-23 05:50:03.581 cls 4 len 8811
11-23 05:50:03.581 cls 5 len 11928
11-23 05:50:03.581 cls 6 len 7891
11-23 05:50:03.581 cls 7 len 5921
11-23 05:50:03.581 cls 8 len 12132
11-23 05:50:03.581 cls 9 len 11549
11-23 05:50:03.581 cls 10 len 12131
11-23 05:50:03.581 cls 11 len 10691
11-23 05:50:03.581 cls 12 len 986
11-23 05:50:03.581 cls 13 len 10501
11-23 05:50:03.581 cls 14 len 6711
11-23 05:50:03.581 cls 15 len 1861
11-23 05:50:03.582 cls 16 len 493
11-23 05:50:03.582 cls 17 len 1211
11-23 05:50:03.582 cls 18 len 168
11-23 05:51:04.431 [epoch 19], [iter 50 / 1548 : 29461], [loss 0.332111], [lr 0.003011], [time 0.2424]
Whitening Loss tensor([0.0616], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:51:53.630 [epoch 19], [iter 100 / 1548 : 29511], [loss 0.329870], [lr 0.002998], [time 0.2421]
Whitening Loss tensor([0.0674], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:52:42.829 [epoch 19], [iter 150 / 1548 : 29561], [loss 0.332176], [lr 0.002985], [time 0.2421]
Whitening Loss tensor([0.0689], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:53:32.019 [epoch 19], [iter 200 / 1548 : 29611], [loss 0.315228], [lr 0.002972], [time 0.2422]
Whitening Loss tensor([0.0658], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:54:21.317 [epoch 19], [iter 250 / 1548 : 29661], [loss 0.357208], [lr 0.002959], [time 0.2427]
Whitening Loss tensor([0.0687], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:55:10.537 [epoch 19], [iter 300 / 1548 : 29711], [loss 0.314742], [lr 0.002946], [time 0.2422]
Whitening Loss tensor([0.0737], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:55:59.709 [epoch 19], [iter 350 / 1548 : 29761], [loss 0.305293], [lr 0.002933], [time 0.2421]
Whitening Loss tensor([0.0753], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:56:48.948 [epoch 19], [iter 400 / 1548 : 29811], [loss 0.338174], [lr 0.002921], [time 0.2424]
Whitening Loss tensor([0.0811], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:57:38.165 [epoch 19], [iter 450 / 1548 : 29861], [loss 0.338878], [lr 0.002908], [time 0.2423]
Whitening Loss tensor([0.0749], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:58:27.418 [epoch 19], [iter 500 / 1548 : 29911], [loss 0.329375], [lr 0.002895], [time 0.2423]
Whitening Loss tensor([0.0750], device='cuda:0', grad_fn=<DivBackward0>)
11-23 05:59:16.595 [epoch 19], [iter 550 / 1548 : 29961], [loss 0.320824], [lr 0.002882], [time 0.2421]
Whitening Loss tensor([0.0703], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:00:05.792 [epoch 19], [iter 600 / 1548 : 30011], [loss 0.329564], [lr 0.002869], [time 0.2422]
Whitening Loss tensor([0.0713], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:00:55.037 [epoch 19], [iter 650 / 1548 : 30061], [loss 0.314742], [lr 0.002856], [time 0.2423]
Whitening Loss tensor([0.0704], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:01:44.390 [epoch 19], [iter 700 / 1548 : 30111], [loss 0.301805], [lr 0.002843], [time 0.2428]
Whitening Loss tensor([0.0569], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:02:33.748 [epoch 19], [iter 750 / 1548 : 30161], [loss 0.343892], [lr 0.002830], [time 0.2428]
Whitening Loss tensor([0.0671], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:03:22.991 [epoch 19], [iter 800 / 1548 : 30211], [loss 0.339222], [lr 0.002817], [time 0.2423]
Whitening Loss tensor([0.0760], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:04:12.272 [epoch 19], [iter 850 / 1548 : 30261], [loss 0.325050], [lr 0.002804], [time 0.2424]
Whitening Loss tensor([0.0726], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:05:01.468 [epoch 19], [iter 900 / 1548 : 30311], [loss 0.328011], [lr 0.002791], [time 0.2421]
Whitening Loss tensor([0.0709], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:05:50.715 [epoch 19], [iter 950 / 1548 : 30361], [loss 0.322353], [lr 0.002778], [time 0.2423]
Whitening Loss tensor([0.0705], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:06:39.974 [epoch 19], [iter 1000 / 1548 : 30411], [loss 0.286802], [lr 0.002765], [time 0.2424]
Whitening Loss tensor([0.0623], device='cuda:0', grad_fn=<DivBackward0>)
Error!! (957, 526) (1052, 1914) 15188
Dropping  tensor(9235)
11-23 06:07:29.238 [epoch 19], [iter 1050 / 1548 : 30461], [loss 0.302260], [lr 0.002752], [time 0.2424]
Whitening Loss tensor([0.0652], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:08:18.512 [epoch 19], [iter 1100 / 1548 : 30511], [loss 0.316802], [lr 0.002739], [time 0.2425]
Whitening Loss tensor([0.0706], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:09:07.776 [epoch 19], [iter 1150 / 1548 : 30561], [loss 0.343329], [lr 0.002726], [time 0.2424]
Whitening Loss tensor([0.0660], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:09:57.091 [epoch 19], [iter 1200 / 1548 : 30611], [loss 0.307818], [lr 0.002713], [time 0.2427]
Whitening Loss tensor([0.0683], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:10:46.385 [epoch 19], [iter 1250 / 1548 : 30661], [loss 0.363041], [lr 0.002700], [time 0.2426]
Whitening Loss tensor([0.0718], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:11:35.605 [epoch 19], [iter 1300 / 1548 : 30711], [loss 0.336189], [lr 0.002687], [time 0.2422]
Whitening Loss tensor([0.0604], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:12:24.836 [epoch 19], [iter 1350 / 1548 : 30761], [loss 0.354420], [lr 0.002674], [time 0.2423]
Whitening Loss tensor([0.0773], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:13:14.114 [epoch 19], [iter 1400 / 1548 : 30811], [loss 0.341567], [lr 0.002661], [time 0.2425]
Whitening Loss tensor([0.0687], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:14:03.413 [epoch 19], [iter 1450 / 1548 : 30861], [loss 0.320324], [lr 0.002648], [time 0.2426]
Whitening Loss tensor([0.0599], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:14:52.734 [epoch 19], [iter 1500 / 1548 : 30911], [loss 0.293208], [lr 0.002635], [time 0.2426]
Whitening Loss tensor([0.0600], device='cuda:0', grad_fn=<DivBackward0>)
Saving pth file...
11-23 06:15:40.524 Saved file to ./logs/1122/r50os16_gtav_isw/11_22_20/last_None_epoch_19_mean-iu_0.00000.pth
11-23 06:15:40.525 Class Uniform Percentage: 0.5
11-23 06:15:40.525 Class Uniform items per Epoch:12388
11-23 06:15:40.528 cls 0 len 12109
11-23 06:15:40.528 cls 1 len 11833
11-23 06:15:40.528 cls 2 len 12301
11-23 06:15:40.528 cls 3 len 10854
11-23 06:15:40.528 cls 4 len 8811
11-23 06:15:40.528 cls 5 len 11928
11-23 06:15:40.528 cls 6 len 7891
11-23 06:15:40.528 cls 7 len 5921
11-23 06:15:40.528 cls 8 len 12132
11-23 06:15:40.528 cls 9 len 11549
11-23 06:15:40.528 cls 10 len 12131
11-23 06:15:40.528 cls 11 len 10691
11-23 06:15:40.528 cls 12 len 986
11-23 06:15:40.528 cls 13 len 10501
11-23 06:15:40.528 cls 14 len 6711
11-23 06:15:40.528 cls 15 len 1861
11-23 06:15:40.528 cls 16 len 493
11-23 06:15:40.529 cls 17 len 1211
11-23 06:15:40.529 cls 18 len 168
11-23 06:16:36.846 [epoch 20], [iter 50 / 1548 : 31009], [loss 0.337003], [lr 0.002610], [time 0.2430]
Whitening Loss tensor([0.0720], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:17:26.040 [epoch 20], [iter 100 / 1548 : 31059], [loss 0.326604], [lr 0.002597], [time 0.2426]
Whitening Loss tensor([0.0784], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:18:15.191 [epoch 20], [iter 150 / 1548 : 31109], [loss 0.306952], [lr 0.002583], [time 0.2422]
Whitening Loss tensor([0.0726], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:19:04.402 [epoch 20], [iter 200 / 1548 : 31159], [loss 0.295577], [lr 0.002570], [time 0.2426]
Whitening Loss tensor([0.0672], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:19:53.609 [epoch 20], [iter 250 / 1548 : 31209], [loss 0.308479], [lr 0.002557], [time 0.2424]
Whitening Loss tensor([0.0604], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:20:42.881 [epoch 20], [iter 300 / 1548 : 31259], [loss 0.321003], [lr 0.002544], [time 0.2429]
Whitening Loss tensor([0.0643], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:21:32.110 [epoch 20], [iter 350 / 1548 : 31309], [loss 0.318152], [lr 0.002531], [time 0.2425]
Whitening Loss tensor([0.0658], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:22:21.278 [epoch 20], [iter 400 / 1548 : 31359], [loss 0.326479], [lr 0.002518], [time 0.2423]
Whitening Loss tensor([0.0567], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:23:10.471 [epoch 20], [iter 450 / 1548 : 31409], [loss 0.323517], [lr 0.002505], [time 0.2423]
Whitening Loss tensor([0.0823], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:23:59.674 [epoch 20], [iter 500 / 1548 : 31459], [loss 0.305201], [lr 0.002492], [time 0.2425]
Whitening Loss tensor([0.0624], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:24:48.788 [epoch 20], [iter 550 / 1548 : 31509], [loss 0.352482], [lr 0.002479], [time 0.2421]
Whitening Loss tensor([0.0875], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:25:38.004 [epoch 20], [iter 600 / 1548 : 31559], [loss 0.304878], [lr 0.002465], [time 0.2425]
Whitening Loss tensor([0.0739], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:26:27.284 [epoch 20], [iter 650 / 1548 : 31609], [loss 0.313332], [lr 0.002452], [time 0.2428]
Whitening Loss tensor([0.0617], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:27:16.555 [epoch 20], [iter 700 / 1548 : 31659], [loss 0.306583], [lr 0.002439], [time 0.2428]
Whitening Loss tensor([0.0734], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:28:05.746 [epoch 20], [iter 750 / 1548 : 31709], [loss 0.309985], [lr 0.002426], [time 0.2423]
Whitening Loss tensor([0.0543], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:28:54.926 [epoch 20], [iter 800 / 1548 : 31759], [loss 0.335991], [lr 0.002413], [time 0.2422]
Whitening Loss tensor([0.0703], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:29:44.152 [epoch 20], [iter 850 / 1548 : 31809], [loss 0.325817], [lr 0.002400], [time 0.2425]
Whitening Loss tensor([0.0601], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:30:33.359 [epoch 20], [iter 900 / 1548 : 31859], [loss 0.310964], [lr 0.002386], [time 0.2424]
Whitening Loss tensor([0.0667], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:31:22.555 [epoch 20], [iter 950 / 1548 : 31909], [loss 0.335672], [lr 0.002373], [time 0.2424]
Whitening Loss tensor([0.0666], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:32:11.765 [epoch 20], [iter 1000 / 1548 : 31959], [loss 0.338384], [lr 0.002360], [time 0.2426]
Whitening Loss tensor([0.0674], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:33:00.922 [epoch 20], [iter 1050 / 1548 : 32009], [loss 0.351101], [lr 0.002347], [time 0.2423]
Whitening Loss tensor([0.0658], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:33:50.078 [epoch 20], [iter 1100 / 1548 : 32059], [loss 0.381671], [lr 0.002334], [time 0.2422]
Whitening Loss tensor([0.0771], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:34:39.320 [epoch 20], [iter 1150 / 1548 : 32109], [loss 0.306351], [lr 0.002320], [time 0.2428]
Whitening Loss tensor([0.0740], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:35:28.508 [epoch 20], [iter 1200 / 1548 : 32159], [loss 0.316304], [lr 0.002307], [time 0.2422]
Whitening Loss tensor([0.0760], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:36:17.667 [epoch 20], [iter 1250 / 1548 : 32209], [loss 0.317664], [lr 0.002294], [time 0.2423]
Whitening Loss tensor([0.0728], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:37:06.914 [epoch 20], [iter 1300 / 1548 : 32259], [loss 0.319126], [lr 0.002281], [time 0.2425]
Whitening Loss tensor([0.0646], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:37:56.161 [epoch 20], [iter 1350 / 1548 : 32309], [loss 0.329469], [lr 0.002267], [time 0.2426]
Whitening Loss tensor([0.0712], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:38:45.335 [epoch 20], [iter 1400 / 1548 : 32359], [loss 0.324848], [lr 0.002254], [time 0.2423]
Whitening Loss tensor([0.0822], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:39:34.571 [epoch 20], [iter 1450 / 1548 : 32409], [loss 0.289046], [lr 0.002241], [time 0.2424]
Whitening Loss tensor([0.0585], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:40:23.668 [epoch 20], [iter 1500 / 1548 : 32459], [loss 0.285680], [lr 0.002228], [time 0.2420]
Whitening Loss tensor([0.0835], device='cuda:0', grad_fn=<DivBackward0>)
Saving pth file...
11-23 06:41:11.418 Saved file to ./logs/1122/r50os16_gtav_isw/11_22_20/last_None_epoch_20_mean-iu_0.00000.pth
11-23 06:41:11.419 Class Uniform Percentage: 0.5
11-23 06:41:11.419 Class Uniform items per Epoch:12388
11-23 06:41:11.422 cls 0 len 12109
11-23 06:41:11.422 cls 1 len 11833
11-23 06:41:11.422 cls 2 len 12301
11-23 06:41:11.422 cls 3 len 10854
11-23 06:41:11.423 cls 4 len 8811
11-23 06:41:11.423 cls 5 len 11928
11-23 06:41:11.423 cls 6 len 7891
11-23 06:41:11.423 cls 7 len 5921
11-23 06:41:11.423 cls 8 len 12132
11-23 06:41:11.423 cls 9 len 11549
11-23 06:41:11.423 cls 10 len 12131
11-23 06:41:11.423 cls 11 len 10691
11-23 06:41:11.423 cls 12 len 986
11-23 06:41:11.423 cls 13 len 10501
11-23 06:41:11.423 cls 14 len 6711
11-23 06:41:11.423 cls 15 len 1861
11-23 06:41:11.423 cls 16 len 493
11-23 06:41:11.423 cls 17 len 1211
11-23 06:41:11.423 cls 18 len 168
11-23 06:42:07.504 [epoch 21], [iter 50 / 1548 : 32557], [loss 0.295172], [lr 0.002202], [time 0.2434]
Whitening Loss tensor([0.0564], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:42:56.955 [epoch 21], [iter 100 / 1548 : 32607], [loss 0.294409], [lr 0.002188], [time 0.2436]
Whitening Loss tensor([0.0686], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:43:46.220 [epoch 21], [iter 150 / 1548 : 32657], [loss 0.305754], [lr 0.002175], [time 0.2428]
Whitening Loss tensor([0.0574], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:44:35.582 [epoch 21], [iter 200 / 1548 : 32707], [loss 0.291634], [lr 0.002162], [time 0.2434]
Whitening Loss tensor([0.0597], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:45:24.900 [epoch 21], [iter 250 / 1548 : 32757], [loss 0.298959], [lr 0.002148], [time 0.2432]
Whitening Loss tensor([0.0668], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:46:14.242 [epoch 21], [iter 300 / 1548 : 32807], [loss 0.327423], [lr 0.002135], [time 0.2434]
Whitening Loss tensor([0.0504], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:47:03.523 [epoch 21], [iter 350 / 1548 : 32857], [loss 0.309081], [lr 0.002121], [time 0.2429]
Whitening Loss tensor([0.0559], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:47:52.769 [epoch 21], [iter 400 / 1548 : 32907], [loss 0.276275], [lr 0.002108], [time 0.2429]
Whitening Loss tensor([0.0689], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:48:42.008 [epoch 21], [iter 450 / 1548 : 32957], [loss 0.314312], [lr 0.002095], [time 0.2429]
Whitening Loss tensor([0.0714], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:49:31.252 [epoch 21], [iter 500 / 1548 : 33007], [loss 0.314646], [lr 0.002081], [time 0.2431]
Whitening Loss tensor([0.0575], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:50:20.424 [epoch 21], [iter 550 / 1548 : 33057], [loss 0.276926], [lr 0.002068], [time 0.2425]
Whitening Loss tensor([0.0675], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:51:09.674 [epoch 21], [iter 600 / 1548 : 33107], [loss 0.302664], [lr 0.002055], [time 0.2429]
Whitening Loss tensor([0.0593], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:51:58.976 [epoch 21], [iter 650 / 1548 : 33157], [loss 0.293336], [lr 0.002041], [time 0.2431]
Whitening Loss tensor([0.0698], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:52:48.300 [epoch 21], [iter 700 / 1548 : 33207], [loss 0.343420], [lr 0.002028], [time 0.2434]
Whitening Loss tensor([0.0491], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:53:37.651 [epoch 21], [iter 750 / 1548 : 33257], [loss 0.320163], [lr 0.002014], [time 0.2435]
Whitening Loss tensor([0.0734], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:54:26.983 [epoch 21], [iter 800 / 1548 : 33307], [loss 0.302019], [lr 0.002001], [time 0.2432]
Whitening Loss tensor([0.0675], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:55:16.354 [epoch 21], [iter 850 / 1548 : 33357], [loss 0.300260], [lr 0.001987], [time 0.2436]
Whitening Loss tensor([0.0667], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:56:05.689 [epoch 21], [iter 900 / 1548 : 33407], [loss 0.310474], [lr 0.001974], [time 0.2434]
Whitening Loss tensor([0.0635], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:56:54.951 [epoch 21], [iter 950 / 1548 : 33457], [loss 0.312429], [lr 0.001960], [time 0.2431]
Whitening Loss tensor([0.0699], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:57:44.300 [epoch 21], [iter 1000 / 1548 : 33507], [loss 0.313335], [lr 0.001947], [time 0.2433]
Whitening Loss tensor([0.0655], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:58:33.610 [epoch 21], [iter 1050 / 1548 : 33557], [loss 0.285465], [lr 0.001933], [time 0.2432]
Whitening Loss tensor([0.0688], device='cuda:0', grad_fn=<DivBackward0>)
11-23 06:59:22.907 [epoch 21], [iter 1100 / 1548 : 33607], [loss 0.304673], [lr 0.001920], [time 0.2432]
Whitening Loss tensor([0.0597], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:00:12.102 [epoch 21], [iter 1150 / 1548 : 33657], [loss 0.323274], [lr 0.001906], [time 0.2427]
Whitening Loss tensor([0.0687], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:01:01.429 [epoch 21], [iter 1200 / 1548 : 33707], [loss 0.288793], [lr 0.001893], [time 0.2432]
Whitening Loss tensor([0.0635], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:01:50.793 [epoch 21], [iter 1250 / 1548 : 33757], [loss 0.293701], [lr 0.001879], [time 0.2435]
Whitening Loss tensor([0.0582], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:02:40.143 [epoch 21], [iter 1300 / 1548 : 33807], [loss 0.305669], [lr 0.001866], [time 0.2434]
Whitening Loss tensor([0.0751], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:03:29.503 [epoch 21], [iter 1350 / 1548 : 33857], [loss 0.283729], [lr 0.001852], [time 0.2435]
Whitening Loss tensor([0.0836], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:04:18.754 [epoch 21], [iter 1400 / 1548 : 33907], [loss 0.292728], [lr 0.001839], [time 0.2430]
Whitening Loss tensor([0.0613], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:05:08.099 [epoch 21], [iter 1450 / 1548 : 33957], [loss 0.296857], [lr 0.001825], [time 0.2432]
Whitening Loss tensor([0.0710], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:05:57.423 [epoch 21], [iter 1500 / 1548 : 34007], [loss 0.310266], [lr 0.001811], [time 0.2433]
Whitening Loss tensor([0.0596], device='cuda:0', grad_fn=<DivBackward0>)
Saving pth file...
11-23 07:06:45.205 Saved file to ./logs/1122/r50os16_gtav_isw/11_22_20/last_None_epoch_21_mean-iu_0.00000.pth
11-23 07:06:45.206 Class Uniform Percentage: 0.5
11-23 07:06:45.206 Class Uniform items per Epoch:12388
11-23 07:06:45.208 cls 0 len 12109
11-23 07:06:45.208 cls 1 len 11833
11-23 07:06:45.208 cls 2 len 12301
11-23 07:06:45.208 cls 3 len 10854
11-23 07:06:45.208 cls 4 len 8811
11-23 07:06:45.208 cls 5 len 11928
11-23 07:06:45.208 cls 6 len 7891
11-23 07:06:45.208 cls 7 len 5921
11-23 07:06:45.209 cls 8 len 12132
11-23 07:06:45.209 cls 9 len 11549
11-23 07:06:45.209 cls 10 len 12131
11-23 07:06:45.209 cls 11 len 10691
11-23 07:06:45.209 cls 12 len 986
11-23 07:06:45.209 cls 13 len 10501
11-23 07:06:45.209 cls 14 len 6711
11-23 07:06:45.209 cls 15 len 1861
11-23 07:06:45.209 cls 16 len 493
11-23 07:06:45.209 cls 17 len 1211
11-23 07:06:45.209 cls 18 len 168
Error!! (957, 526) (1052, 1914) 15188
Dropping  tensor(5221)
11-23 07:07:41.835 [epoch 22], [iter 50 / 1548 : 34105], [loss 0.288470], [lr 0.001785], [time 0.2434]
Whitening Loss tensor([0.0586], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:08:31.178 [epoch 22], [iter 100 / 1548 : 34155], [loss 0.288768], [lr 0.001771], [time 0.2434]
Whitening Loss tensor([0.0513], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:09:20.486 [epoch 22], [iter 150 / 1548 : 34205], [loss 0.309968], [lr 0.001757], [time 0.2433]
Whitening Loss tensor([0.0554], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:10:09.815 [epoch 22], [iter 200 / 1548 : 34255], [loss 0.291083], [lr 0.001744], [time 0.2433]
Whitening Loss tensor([0.0646], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:10:59.115 [epoch 22], [iter 250 / 1548 : 34305], [loss 0.285189], [lr 0.001730], [time 0.2433]
Whitening Loss tensor([0.0570], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:11:48.489 [epoch 22], [iter 300 / 1548 : 34355], [loss 0.279643], [lr 0.001716], [time 0.2435]
Whitening Loss tensor([0.0528], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:12:37.777 [epoch 22], [iter 350 / 1548 : 34405], [loss 0.272232], [lr 0.001703], [time 0.2431]
Whitening Loss tensor([0.0503], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:13:27.053 [epoch 22], [iter 400 / 1548 : 34455], [loss 0.281706], [lr 0.001689], [time 0.2431]
Whitening Loss tensor([0.0530], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:14:16.375 [epoch 22], [iter 450 / 1548 : 34505], [loss 0.303264], [lr 0.001675], [time 0.2431]
Whitening Loss tensor([0.0569], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:15:05.613 [epoch 22], [iter 500 / 1548 : 34555], [loss 0.278805], [lr 0.001662], [time 0.2428]
Whitening Loss tensor([0.0555], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:15:54.938 [epoch 22], [iter 550 / 1548 : 34605], [loss 0.282545], [lr 0.001648], [time 0.2434]
Whitening Loss tensor([0.0530], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:16:44.242 [epoch 22], [iter 600 / 1548 : 34655], [loss 0.298315], [lr 0.001634], [time 0.2431]
Whitening Loss tensor([0.0615], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:17:33.607 [epoch 22], [iter 650 / 1548 : 34705], [loss 0.301127], [lr 0.001620], [time 0.2435]
Whitening Loss tensor([0.0645], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:18:22.824 [epoch 22], [iter 700 / 1548 : 34755], [loss 0.291607], [lr 0.001607], [time 0.2428]
Whitening Loss tensor([0.0690], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:19:12.057 [epoch 22], [iter 750 / 1548 : 34805], [loss 0.272071], [lr 0.001593], [time 0.2429]
Whitening Loss tensor([0.0601], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:20:01.351 [epoch 22], [iter 800 / 1548 : 34855], [loss 0.289018], [lr 0.001579], [time 0.2431]
Whitening Loss tensor([0.0561], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:20:50.752 [epoch 22], [iter 850 / 1548 : 34905], [loss 0.275779], [lr 0.001565], [time 0.2433]
Whitening Loss tensor([0.0531], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:21:40.066 [epoch 22], [iter 900 / 1548 : 34955], [loss 0.307979], [lr 0.001551], [time 0.2433]
Whitening Loss tensor([0.0663], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:22:29.435 [epoch 22], [iter 950 / 1548 : 35005], [loss 0.299601], [lr 0.001538], [time 0.2435]
Whitening Loss tensor([0.0598], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:23:18.735 [epoch 22], [iter 1000 / 1548 : 35055], [loss 0.296775], [lr 0.001524], [time 0.2433]
Whitening Loss tensor([0.0590], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:24:07.967 [epoch 22], [iter 1050 / 1548 : 35105], [loss 0.266212], [lr 0.001510], [time 0.2429]
Whitening Loss tensor([0.0561], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:24:57.150 [epoch 22], [iter 1100 / 1548 : 35155], [loss 0.310242], [lr 0.001496], [time 0.2425]
Whitening Loss tensor([0.0567], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:25:46.336 [epoch 22], [iter 1150 / 1548 : 35205], [loss 0.286902], [lr 0.001482], [time 0.2425]
Whitening Loss tensor([0.0662], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:26:35.552 [epoch 22], [iter 1200 / 1548 : 35255], [loss 0.311485], [lr 0.001468], [time 0.2427]
Whitening Loss tensor([0.0676], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:27:24.805 [epoch 22], [iter 1250 / 1548 : 35305], [loss 0.313510], [lr 0.001454], [time 0.2429]
Whitening Loss tensor([0.0625], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:28:14.070 [epoch 22], [iter 1300 / 1548 : 35355], [loss 0.287460], [lr 0.001440], [time 0.2430]
Whitening Loss tensor([0.0542], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:29:03.306 [epoch 22], [iter 1350 / 1548 : 35405], [loss 0.289838], [lr 0.001426], [time 0.2429]
Whitening Loss tensor([0.0555], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:29:52.562 [epoch 22], [iter 1400 / 1548 : 35455], [loss 0.313828], [lr 0.001412], [time 0.2428]
Whitening Loss tensor([0.0596], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:30:41.867 [epoch 22], [iter 1450 / 1548 : 35505], [loss 0.259178], [lr 0.001398], [time 0.2433]
Whitening Loss tensor([0.0699], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:31:31.177 [epoch 22], [iter 1500 / 1548 : 35555], [loss 0.304330], [lr 0.001384], [time 0.2432]
Whitening Loss tensor([0.0693], device='cuda:0', grad_fn=<DivBackward0>)
Saving pth file...
11-23 07:32:18.946 Saved file to ./logs/1122/r50os16_gtav_isw/11_22_20/last_None_epoch_22_mean-iu_0.00000.pth
11-23 07:32:18.947 Class Uniform Percentage: 0.5
11-23 07:32:18.947 Class Uniform items per Epoch:12388
11-23 07:32:18.950 cls 0 len 12109
11-23 07:32:18.950 cls 1 len 11833
11-23 07:32:18.950 cls 2 len 12301
11-23 07:32:18.950 cls 3 len 10854
11-23 07:32:18.950 cls 4 len 8811
11-23 07:32:18.950 cls 5 len 11928
11-23 07:32:18.950 cls 6 len 7891
11-23 07:32:18.950 cls 7 len 5921
11-23 07:32:18.950 cls 8 len 12132
11-23 07:32:18.950 cls 9 len 11549
11-23 07:32:18.950 cls 10 len 12131
11-23 07:32:18.950 cls 11 len 10691
11-23 07:32:18.950 cls 12 len 986
11-23 07:32:18.950 cls 13 len 10501
11-23 07:32:18.950 cls 14 len 6711
11-23 07:32:18.950 cls 15 len 1861
11-23 07:32:18.950 cls 16 len 493
11-23 07:32:18.951 cls 17 len 1211
11-23 07:32:18.951 cls 18 len 168
11-23 07:33:15.104 [epoch 23], [iter 50 / 1548 : 35653], [loss 0.279714], [lr 0.001357], [time 0.2431]
Whitening Loss tensor([0.0609], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:34:04.342 [epoch 23], [iter 100 / 1548 : 35703], [loss 0.301549], [lr 0.001343], [time 0.2424]
Whitening Loss tensor([0.0750], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:34:53.634 [epoch 23], [iter 150 / 1548 : 35753], [loss 0.294564], [lr 0.001329], [time 0.2430]
Whitening Loss tensor([0.0548], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:35:42.923 [epoch 23], [iter 200 / 1548 : 35803], [loss 0.273742], [lr 0.001315], [time 0.2428]
Whitening Loss tensor([0.0490], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:36:32.176 [epoch 23], [iter 250 / 1548 : 35853], [loss 0.299594], [lr 0.001300], [time 0.2428]
Whitening Loss tensor([0.0542], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:37:21.436 [epoch 23], [iter 300 / 1548 : 35903], [loss 0.294811], [lr 0.001286], [time 0.2428]
Whitening Loss tensor([0.0728], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:38:10.735 [epoch 23], [iter 350 / 1548 : 35953], [loss 0.282125], [lr 0.001272], [time 0.2428]
Whitening Loss tensor([0.0571], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:38:59.993 [epoch 23], [iter 400 / 1548 : 36003], [loss 0.321793], [lr 0.001258], [time 0.2427]
Whitening Loss tensor([0.0615], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:39:49.188 [epoch 23], [iter 450 / 1548 : 36053], [loss 0.317134], [lr 0.001244], [time 0.2426]
Whitening Loss tensor([0.0548], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:40:38.510 [epoch 23], [iter 500 / 1548 : 36103], [loss 0.298304], [lr 0.001230], [time 0.2430]
Whitening Loss tensor([0.0655], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:41:27.809 [epoch 23], [iter 550 / 1548 : 36153], [loss 0.289224], [lr 0.001216], [time 0.2431]
Whitening Loss tensor([0.0520], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:42:17.121 [epoch 23], [iter 600 / 1548 : 36203], [loss 0.284477], [lr 0.001201], [time 0.2430]
Whitening Loss tensor([0.0684], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:43:06.427 [epoch 23], [iter 650 / 1548 : 36253], [loss 0.262975], [lr 0.001187], [time 0.2428]
Whitening Loss tensor([0.0512], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:43:55.807 [epoch 23], [iter 700 / 1548 : 36303], [loss 0.307940], [lr 0.001173], [time 0.2433]
Whitening Loss tensor([0.0507], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:44:45.041 [epoch 23], [iter 750 / 1548 : 36353], [loss 0.285736], [lr 0.001158], [time 0.2427]
Whitening Loss tensor([0.0640], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:45:34.251 [epoch 23], [iter 800 / 1548 : 36403], [loss 0.289393], [lr 0.001144], [time 0.2423]
Whitening Loss tensor([0.0614], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:46:23.464 [epoch 23], [iter 850 / 1548 : 36453], [loss 0.296285], [lr 0.001130], [time 0.2425]
Whitening Loss tensor([0.0553], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:47:12.697 [epoch 23], [iter 900 / 1548 : 36503], [loss 0.277057], [lr 0.001116], [time 0.2426]
Whitening Loss tensor([0.0563], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:48:01.867 [epoch 23], [iter 950 / 1548 : 36553], [loss 0.268895], [lr 0.001101], [time 0.2423]
Whitening Loss tensor([0.0567], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:48:51.078 [epoch 23], [iter 1000 / 1548 : 36603], [loss 0.283437], [lr 0.001087], [time 0.2426]
Whitening Loss tensor([0.0499], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:49:40.375 [epoch 23], [iter 1050 / 1548 : 36653], [loss 0.265018], [lr 0.001072], [time 0.2429]
Whitening Loss tensor([0.0579], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:50:29.654 [epoch 23], [iter 1100 / 1548 : 36703], [loss 0.259733], [lr 0.001058], [time 0.2427]
Whitening Loss tensor([0.0579], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:51:18.899 [epoch 23], [iter 1150 / 1548 : 36753], [loss 0.302201], [lr 0.001043], [time 0.2426]
Whitening Loss tensor([0.0572], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:52:08.186 [epoch 23], [iter 1200 / 1548 : 36803], [loss 0.266815], [lr 0.001029], [time 0.2430]
Whitening Loss tensor([0.0503], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:52:57.495 [epoch 23], [iter 1250 / 1548 : 36853], [loss 0.292193], [lr 0.001015], [time 0.2429]
Whitening Loss tensor([0.0581], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:53:46.711 [epoch 23], [iter 1300 / 1548 : 36903], [loss 0.279008], [lr 0.001000], [time 0.2427]
Whitening Loss tensor([0.0526], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:54:35.977 [epoch 23], [iter 1350 / 1548 : 36953], [loss 0.282988], [lr 0.000985], [time 0.2427]
Whitening Loss tensor([0.0625], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:55:25.221 [epoch 23], [iter 1400 / 1548 : 37003], [loss 0.272457], [lr 0.000971], [time 0.2426]
Whitening Loss tensor([0.0610], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:56:14.490 [epoch 23], [iter 1450 / 1548 : 37053], [loss 0.250868], [lr 0.000956], [time 0.2428]
Whitening Loss tensor([0.0607], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:57:03.807 [epoch 23], [iter 1500 / 1548 : 37103], [loss 0.262513], [lr 0.000942], [time 0.2429]
Whitening Loss tensor([0.0566], device='cuda:0', grad_fn=<DivBackward0>)
Saving pth file...
11-23 07:57:51.640 Saved file to ./logs/1122/r50os16_gtav_isw/11_22_20/last_None_epoch_23_mean-iu_0.00000.pth
11-23 07:57:51.641 Class Uniform Percentage: 0.5
11-23 07:57:51.641 Class Uniform items per Epoch:12388
11-23 07:57:51.644 cls 0 len 12109
11-23 07:57:51.644 cls 1 len 11833
11-23 07:57:51.644 cls 2 len 12301
11-23 07:57:51.644 cls 3 len 10854
11-23 07:57:51.644 cls 4 len 8811
11-23 07:57:51.644 cls 5 len 11928
11-23 07:57:51.644 cls 6 len 7891
11-23 07:57:51.644 cls 7 len 5921
11-23 07:57:51.644 cls 8 len 12132
11-23 07:57:51.644 cls 9 len 11549
11-23 07:57:51.644 cls 10 len 12131
11-23 07:57:51.644 cls 11 len 10691
11-23 07:57:51.644 cls 12 len 986
11-23 07:57:51.644 cls 13 len 10501
11-23 07:57:51.644 cls 14 len 6711
11-23 07:57:51.644 cls 15 len 1861
11-23 07:57:51.644 cls 16 len 493
11-23 07:57:51.644 cls 17 len 1211
11-23 07:57:51.644 cls 18 len 168
11-23 07:58:48.249 [epoch 24], [iter 50 / 1548 : 37201], [loss 0.287057], [lr 0.000913], [time 0.2438]
Whitening Loss tensor([0.0640], device='cuda:0', grad_fn=<DivBackward0>)
11-23 07:59:37.697 [epoch 24], [iter 100 / 1548 : 37251], [loss 0.287299], [lr 0.000898], [time 0.2438]
Whitening Loss tensor([0.0534], device='cuda:0', grad_fn=<DivBackward0>)
11-23 08:00:27.120 [epoch 24], [iter 150 / 1548 : 37301], [loss 0.286450], [lr 0.000884], [time 0.2437]
Whitening Loss tensor([0.0607], device='cuda:0', grad_fn=<DivBackward0>)
11-23 08:01:16.423 [epoch 24], [iter 200 / 1548 : 37351], [loss 0.285689], [lr 0.000869], [time 0.2431]
Whitening Loss tensor([0.0652], device='cuda:0', grad_fn=<DivBackward0>)
11-23 08:02:05.709 [epoch 24], [iter 250 / 1548 : 37401], [loss 0.273582], [lr 0.000854], [time 0.2430]
Whitening Loss tensor([0.0609], device='cuda:0', grad_fn=<DivBackward0>)
Error!! (957, 526) (1052, 1914) 15188
Dropping  tensor(3104)
11-23 08:02:54.979 [epoch 24], [iter 300 / 1548 : 37451], [loss 0.255607], [lr 0.000839], [time 0.2428]
Whitening Loss tensor([0.0542], device='cuda:0', grad_fn=<DivBackward0>)
11-23 08:03:44.234 [epoch 24], [iter 350 / 1548 : 37501], [loss 0.261093], [lr 0.000824], [time 0.2430]
Whitening Loss tensor([0.0451], device='cuda:0', grad_fn=<DivBackward0>)
11-23 08:04:33.500 [epoch 24], [iter 400 / 1548 : 37551], [loss 0.272689], [lr 0.000810], [time 0.2431]
Whitening Loss tensor([0.0477], device='cuda:0', grad_fn=<DivBackward0>)
11-23 08:05:22.784 [epoch 24], [iter 450 / 1548 : 37601], [loss 0.259041], [lr 0.000795], [time 0.2430]
Whitening Loss tensor([0.0492], device='cuda:0', grad_fn=<DivBackward0>)
11-23 08:06:12.017 [epoch 24], [iter 500 / 1548 : 37651], [loss 0.279418], [lr 0.000780], [time 0.2428]
Whitening Loss tensor([0.0504], device='cuda:0', grad_fn=<DivBackward0>)
11-23 08:07:01.238 [epoch 24], [iter 550 / 1548 : 37701], [loss 0.272435], [lr 0.000765], [time 0.2427]
Whitening Loss tensor([0.0566], device='cuda:0', grad_fn=<DivBackward0>)
11-23 08:07:50.484 [epoch 24], [iter 600 / 1548 : 37751], [loss 0.283958], [lr 0.000750], [time 0.2428]
Whitening Loss tensor([0.0444], device='cuda:0', grad_fn=<DivBackward0>)
11-23 08:08:39.722 [epoch 24], [iter 650 / 1548 : 37801], [loss 0.239625], [lr 0.000735], [time 0.2428]
Whitening Loss tensor([0.0590], device='cuda:0', grad_fn=<DivBackward0>)
11-23 08:09:28.991 [epoch 24], [iter 700 / 1548 : 37851], [loss 0.248166], [lr 0.000720], [time 0.2430]
Whitening Loss tensor([0.0587], device='cuda:0', grad_fn=<DivBackward0>)
11-23 08:10:18.230 [epoch 24], [iter 750 / 1548 : 37901], [loss 0.279843], [lr 0.000705], [time 0.2429]
Whitening Loss tensor([0.0664], device='cuda:0', grad_fn=<DivBackward0>)
11-23 08:11:07.463 [epoch 24], [iter 800 / 1548 : 37951], [loss 0.273211], [lr 0.000689], [time 0.2429]
Whitening Loss tensor([0.0577], device='cuda:0', grad_fn=<DivBackward0>)
11-23 08:11:56.756 [epoch 24], [iter 850 / 1548 : 38001], [loss 0.267097], [lr 0.000674], [time 0.2431]
Whitening Loss tensor([0.0429], device='cuda:0', grad_fn=<DivBackward0>)
11-23 08:12:46.018 [epoch 24], [iter 900 / 1548 : 38051], [loss 0.295206], [lr 0.000659], [time 0.2428]
Whitening Loss tensor([0.0522], device='cuda:0', grad_fn=<DivBackward0>)
11-23 08:13:35.287 [epoch 24], [iter 950 / 1548 : 38101], [loss 0.254882], [lr 0.000644], [time 0.2429]
Whitening Loss tensor([0.0770], device='cuda:0', grad_fn=<DivBackward0>)
11-23 08:14:24.517 [epoch 24], [iter 1000 / 1548 : 38151], [loss 0.271433], [lr 0.000629], [time 0.2428]
Whitening Loss tensor([0.0530], device='cuda:0', grad_fn=<DivBackward0>)
11-23 08:15:13.757 [epoch 24], [iter 1050 / 1548 : 38201], [loss 0.279628], [lr 0.000613], [time 0.2429]
Whitening Loss tensor([0.0547], device='cuda:0', grad_fn=<DivBackward0>)
11-23 08:16:02.951 [epoch 24], [iter 1100 / 1548 : 38251], [loss 0.279719], [lr 0.000598], [time 0.2426]
Whitening Loss tensor([0.0527], device='cuda:0', grad_fn=<DivBackward0>)
11-23 08:16:52.263 [epoch 24], [iter 1150 / 1548 : 38301], [loss 0.263671], [lr 0.000583], [time 0.2431]
Whitening Loss tensor([0.0586], device='cuda:0', grad_fn=<DivBackward0>)
11-23 08:17:41.515 [epoch 24], [iter 1200 / 1548 : 38351], [loss 0.265372], [lr 0.000567], [time 0.2428]
Whitening Loss tensor([0.0622], device='cuda:0', grad_fn=<DivBackward0>)
11-23 08:18:30.747 [epoch 24], [iter 1250 / 1548 : 38401], [loss 0.279061], [lr 0.000552], [time 0.2427]
Whitening Loss tensor([0.0443], device='cuda:0', grad_fn=<DivBackward0>)
11-23 08:19:20.008 [epoch 24], [iter 1300 / 1548 : 38451], [loss 0.250993], [lr 0.000536], [time 0.2430]
Whitening Loss tensor([0.0501], device='cuda:0', grad_fn=<DivBackward0>)
11-23 08:20:09.387 [epoch 24], [iter 1350 / 1548 : 38501], [loss 0.256944], [lr 0.000520], [time 0.2436]
Whitening Loss tensor([0.0479], device='cuda:0', grad_fn=<DivBackward0>)
11-23 08:20:58.626 [epoch 24], [iter 1400 / 1548 : 38551], [loss 0.280825], [lr 0.000505], [time 0.2427]
Whitening Loss tensor([0.0596], device='cuda:0', grad_fn=<DivBackward0>)
11-23 08:21:47.907 [epoch 24], [iter 1450 / 1548 : 38601], [loss 0.267613], [lr 0.000489], [time 0.2430]
Whitening Loss tensor([0.0632], device='cuda:0', grad_fn=<DivBackward0>)
11-23 08:22:37.092 [epoch 24], [iter 1500 / 1548 : 38651], [loss 0.271906], [lr 0.000473], [time 0.2426]
Whitening Loss tensor([0.0598], device='cuda:0', grad_fn=<DivBackward0>)
Saving pth file...
11-23 08:23:24.841 Saved file to ./logs/1122/r50os16_gtav_isw/11_22_20/last_None_epoch_24_mean-iu_0.00000.pth
11-23 08:23:24.842 Class Uniform Percentage: 0.5
11-23 08:23:24.842 Class Uniform items per Epoch:12388
11-23 08:23:24.845 cls 0 len 12109
11-23 08:23:24.845 cls 1 len 11833
11-23 08:23:24.845 cls 2 len 12301
11-23 08:23:24.845 cls 3 len 10854
11-23 08:23:24.845 cls 4 len 8811
11-23 08:23:24.845 cls 5 len 11928
11-23 08:23:24.845 cls 6 len 7891
11-23 08:23:24.845 cls 7 len 5921
11-23 08:23:24.845 cls 8 len 12132
11-23 08:23:24.845 cls 9 len 11549
11-23 08:23:24.845 cls 10 len 12131
11-23 08:23:24.845 cls 11 len 10691
11-23 08:23:24.845 cls 12 len 986
11-23 08:23:24.845 cls 13 len 10501
11-23 08:23:24.846 cls 14 len 6711
11-23 08:23:24.846 cls 15 len 1861
11-23 08:23:24.846 cls 16 len 493
11-23 08:23:24.846 cls 17 len 1211
11-23 08:23:24.846 cls 18 len 168
11-23 08:24:20.749 [epoch 25], [iter 50 / 1548 : 38749], [loss 0.254837], [lr 0.000442], [time 0.2420]
Whitening Loss tensor([0.0486], device='cuda:0', grad_fn=<DivBackward0>)
11-23 08:25:09.687 [epoch 25], [iter 100 / 1548 : 38799], [loss 0.283640], [lr 0.000426], [time 0.2415]
Whitening Loss tensor([0.0580], device='cuda:0', grad_fn=<DivBackward0>)
11-23 08:25:58.542 [epoch 25], [iter 150 / 1548 : 38849], [loss 0.268845], [lr 0.000410], [time 0.2411]
Whitening Loss tensor([0.0476], device='cuda:0', grad_fn=<DivBackward0>)
11-23 08:26:47.476 [epoch 25], [iter 200 / 1548 : 38899], [loss 0.277165], [lr 0.000394], [time 0.2414]
Whitening Loss tensor([0.0613], device='cuda:0', grad_fn=<DivBackward0>)
11-23 08:27:36.334 [epoch 25], [iter 250 / 1548 : 38949], [loss 0.257227], [lr 0.000378], [time 0.2411]
Whitening Loss tensor([0.0576], device='cuda:0', grad_fn=<DivBackward0>)
11-23 08:28:25.195 [epoch 25], [iter 300 / 1548 : 38999], [loss 0.276539], [lr 0.000362], [time 0.2411]
Whitening Loss tensor([0.0546], device='cuda:0', grad_fn=<DivBackward0>)
11-23 08:29:13.988 [epoch 25], [iter 350 / 1548 : 39049], [loss 0.291169], [lr 0.000346], [time 0.2408]
Whitening Loss tensor([0.0458], device='cuda:0', grad_fn=<DivBackward0>)
11-23 08:30:02.774 [epoch 25], [iter 400 / 1548 : 39099], [loss 0.260699], [lr 0.000329], [time 0.2407]
Whitening Loss tensor([0.0535], device='cuda:0', grad_fn=<DivBackward0>)
11-23 08:30:51.610 [epoch 25], [iter 450 / 1548 : 39149], [loss 0.280074], [lr 0.000313], [time 0.2410]
Whitening Loss tensor([0.0511], device='cuda:0', grad_fn=<DivBackward0>)
11-23 08:31:40.407 [epoch 25], [iter 500 / 1548 : 39199], [loss 0.281761], [lr 0.000296], [time 0.2407]
Whitening Loss tensor([0.0531], device='cuda:0', grad_fn=<DivBackward0>)
11-23 08:32:29.154 [epoch 25], [iter 550 / 1548 : 39249], [loss 0.250082], [lr 0.000279], [time 0.2406]
Whitening Loss tensor([0.0543], device='cuda:0', grad_fn=<DivBackward0>)
11-23 08:33:17.985 [epoch 25], [iter 600 / 1548 : 39299], [loss 0.255878], [lr 0.000263], [time 0.2409]
Whitening Loss tensor([0.0476], device='cuda:0', grad_fn=<DivBackward0>)
11-23 08:34:06.828 [epoch 25], [iter 650 / 1548 : 39349], [loss 0.263073], [lr 0.000246], [time 0.2410]
Whitening Loss tensor([0.0560], device='cuda:0', grad_fn=<DivBackward0>)
11-23 08:34:55.680 [epoch 25], [iter 700 / 1548 : 39399], [loss 0.271948], [lr 0.000229], [time 0.2411]
Whitening Loss tensor([0.0531], device='cuda:0', grad_fn=<DivBackward0>)
11-23 08:35:44.440 [epoch 25], [iter 750 / 1548 : 39449], [loss 0.273191], [lr 0.000211], [time 0.2406]
Whitening Loss tensor([0.0629], device='cuda:0', grad_fn=<DivBackward0>)
11-23 08:36:33.286 [epoch 25], [iter 800 / 1548 : 39499], [loss 0.275964], [lr 0.000194], [time 0.2410]
Whitening Loss tensor([0.0466], device='cuda:0', grad_fn=<DivBackward0>)
11-23 08:37:22.139 [epoch 25], [iter 850 / 1548 : 39549], [loss 0.256184], [lr 0.000177], [time 0.2410]
Whitening Loss tensor([0.0511], device='cuda:0', grad_fn=<DivBackward0>)
11-23 08:38:10.986 [epoch 25], [iter 900 / 1548 : 39599], [loss 0.256676], [lr 0.000159], [time 0.2411]
Whitening Loss tensor([0.0597], device='cuda:0', grad_fn=<DivBackward0>)
11-23 08:38:59.767 [epoch 25], [iter 950 / 1548 : 39649], [loss 0.262455], [lr 0.000141], [time 0.2408]
Whitening Loss tensor([0.0493], device='cuda:0', grad_fn=<DivBackward0>)
11-23 08:39:48.680 [epoch 25], [iter 1000 / 1548 : 39699], [loss 0.265596], [lr 0.000123], [time 0.2413]
Whitening Loss tensor([0.0566], device='cuda:0', grad_fn=<DivBackward0>)
11-23 08:40:37.618 [epoch 25], [iter 1050 / 1548 : 39749], [loss 0.281094], [lr 0.000104], [time 0.2414]
Whitening Loss tensor([0.0553], device='cuda:0', grad_fn=<DivBackward0>)
11-23 08:41:26.578 [epoch 25], [iter 1100 / 1548 : 39799], [loss 0.252772], [lr 0.000085], [time 0.2416]
Whitening Loss tensor([0.0607], device='cuda:0', grad_fn=<DivBackward0>)
11-23 08:42:15.407 [epoch 25], [iter 1150 / 1548 : 39849], [loss 0.244020], [lr 0.000066], [time 0.2410]
Whitening Loss tensor([0.0574], device='cuda:0', grad_fn=<DivBackward0>)
11-23 08:43:04.169 [epoch 25], [iter 1200 / 1548 : 39899], [loss 0.260379], [lr 0.000046], [time 0.2407]
Whitening Loss tensor([0.0499], device='cuda:0', grad_fn=<DivBackward0>)
11-23 08:43:53.054 [epoch 25], [iter 1250 / 1548 : 39949], [loss 0.272748], [lr 0.000025], [time 0.2413]
Whitening Loss tensor([0.0611], device='cuda:0', grad_fn=<DivBackward0>)
11-23 08:44:41.897 [epoch 25], [iter 1300 / 1548 : 39999], [loss 0.265971], [lr 0.000001], [time 0.2410]
Whitening Loss tensor([0.0533], device='cuda:0', grad_fn=<DivBackward0>)
Training Ends!
Source domain evaluation starts!
Saving pth file...
11-23 08:44:47.440 Saved file to ./logs/1122/r50os16_gtav_isw/11_22_20/last_None_epoch_25_mean-iu_0.00000.pth
11-23 08:44:47.442 Class Uniform Percentage: 0.5
11-23 08:44:47.442 Class Uniform items per Epoch:12388
11-23 08:44:47.444 cls 0 len 12109
11-23 08:44:47.444 cls 1 len 11833
11-23 08:44:47.444 cls 2 len 12301
11-23 08:44:47.444 cls 3 len 10854
11-23 08:44:47.444 cls 4 len 8811
11-23 08:44:47.444 cls 5 len 11928
11-23 08:44:47.444 cls 6 len 7891
11-23 08:44:47.444 cls 7 len 5921
11-23 08:44:47.444 cls 8 len 12132
11-23 08:44:47.444 cls 9 len 11549
11-23 08:44:47.444 cls 10 len 12131
11-23 08:44:47.445 cls 11 len 10691
11-23 08:44:47.445 cls 12 len 986
11-23 08:44:47.445 cls 13 len 10501
11-23 08:44:47.445 cls 14 len 6711
11-23 08:44:47.445 cls 15 len 1861
11-23 08:44:47.445 cls 16 len 493
11-23 08:44:47.445 cls 17 len 1211
11-23 08:44:47.445 cls 18 len 168
Training Ends!
Source domain evaluation starts!
11-23 08:44:49.551 validating: 1 / 3191
11-23 08:44:57.217 validating: 21 / 3191
11-23 08:45:04.311 validating: 41 / 3191
11-23 08:45:11.582 validating: 61 / 3191
11-23 08:45:18.966 validating: 81 / 3191
11-23 08:45:26.910 validating: 101 / 3191
11-23 08:45:34.212 validating: 121 / 3191
11-23 08:45:41.762 validating: 141 / 3191
11-23 08:45:48.783 validating: 161 / 3191
11-23 08:45:56.065 validating: 181 / 3191
11-23 08:46:03.322 validating: 201 / 3191
11-23 08:46:10.350 validating: 221 / 3191
11-23 08:46:17.333 validating: 241 / 3191
11-23 08:46:24.368 validating: 261 / 3191
11-23 08:46:31.542 validating: 281 / 3191
11-23 08:46:38.520 validating: 301 / 3191
11-23 08:46:45.705 validating: 321 / 3191
11-23 08:46:52.876 validating: 341 / 3191
11-23 08:46:59.991 validating: 361 / 3191
11-23 08:47:07.416 validating: 381 / 3191
11-23 08:47:14.666 validating: 401 / 3191
11-23 08:47:21.647 validating: 421 / 3191
11-23 08:47:28.485 validating: 441 / 3191
11-23 08:47:35.432 validating: 461 / 3191
11-23 08:47:42.563 validating: 481 / 3191
11-23 08:47:49.563 validating: 501 / 3191
11-23 08:47:56.690 validating: 521 / 3191
11-23 08:48:04.099 validating: 541 / 3191
11-23 08:48:11.365 validating: 561 / 3191
11-23 08:48:18.395 validating: 581 / 3191
11-23 08:48:25.567 validating: 601 / 3191
11-23 08:48:32.461 validating: 621 / 3191
11-23 08:48:39.578 validating: 641 / 3191
11-23 08:48:46.955 validating: 661 / 3191
11-23 08:48:54.052 validating: 681 / 3191
11-23 08:49:00.940 validating: 701 / 3191
11-23 08:49:07.928 validating: 721 / 3191
11-23 08:49:14.893 validating: 741 / 3191
11-23 08:49:21.943 validating: 761 / 3191
11-23 08:49:29.045 validating: 781 / 3191
11-23 08:49:36.012 validating: 801 / 3191
11-23 08:49:42.916 validating: 821 / 3191
11-23 08:49:49.945 validating: 841 / 3191
11-23 08:49:57.231 validating: 861 / 3191
11-23 08:50:04.486 validating: 881 / 3191
11-23 08:50:11.609 validating: 901 / 3191
11-23 08:50:18.904 validating: 921 / 3191
11-23 08:50:25.938 validating: 941 / 3191
11-23 08:50:32.958 validating: 961 / 3191
11-23 08:50:40.086 validating: 981 / 3191
11-23 08:50:47.196 validating: 1001 / 3191
11-23 08:50:54.165 validating: 1021 / 3191
11-23 08:51:01.102 validating: 1041 / 3191
11-23 08:51:08.314 validating: 1061 / 3191
11-23 08:51:15.689 validating: 1081 / 3191
11-23 08:51:22.960 validating: 1101 / 3191
11-23 08:51:29.968 validating: 1121 / 3191
11-23 08:51:37.155 validating: 1141 / 3191
11-23 08:51:44.170 validating: 1161 / 3191
11-23 08:51:51.197 validating: 1181 / 3191
11-23 08:51:58.458 validating: 1201 / 3191
11-23 08:52:05.413 validating: 1221 / 3191
11-23 08:52:12.492 validating: 1241 / 3191
11-23 08:52:19.464 validating: 1261 / 3191
11-23 08:52:26.462 validating: 1281 / 3191
11-23 08:52:33.443 validating: 1301 / 3191
11-23 08:52:40.428 validating: 1321 / 3191
11-23 08:52:47.521 validating: 1341 / 3191
11-23 08:52:54.622 validating: 1361 / 3191
11-23 08:53:01.693 validating: 1381 / 3191
11-23 08:53:08.856 validating: 1401 / 3191
11-23 08:53:15.973 validating: 1421 / 3191
11-23 08:53:23.009 validating: 1441 / 3191
11-23 08:53:30.168 validating: 1461 / 3191
11-23 08:53:37.080 validating: 1481 / 3191
11-23 08:53:44.278 validating: 1501 / 3191
11-23 08:53:51.322 validating: 1521 / 3191
11-23 08:53:58.410 validating: 1541 / 3191
11-23 08:54:05.253 validating: 1561 / 3191
11-23 08:54:12.188 validating: 1581 / 3191
11-23 08:54:19.456 validating: 1601 / 3191
11-23 08:54:26.266 validating: 1621 / 3191
11-23 08:54:33.187 validating: 1641 / 3191
11-23 08:54:40.279 validating: 1661 / 3191
11-23 08:54:47.328 validating: 1681 / 3191
11-23 08:54:54.527 validating: 1701 / 3191
11-23 08:55:01.541 validating: 1721 / 3191
11-23 08:55:08.420 validating: 1741 / 3191
11-23 08:55:15.303 validating: 1761 / 3191
11-23 08:55:22.198 validating: 1781 / 3191
11-23 08:55:29.159 validating: 1801 / 3191
11-23 08:55:36.130 validating: 1821 / 3191
11-23 08:55:42.984 validating: 1841 / 3191
11-23 08:55:50.059 validating: 1861 / 3191
11-23 08:55:56.989 validating: 1881 / 3191
11-23 08:56:03.968 validating: 1901 / 3191
11-23 08:56:10.865 validating: 1921 / 3191
11-23 08:56:17.855 validating: 1941 / 3191
11-23 08:56:25.013 validating: 1961 / 3191
11-23 08:56:32.073 validating: 1981 / 3191
11-23 08:56:39.230 validating: 2001 / 3191
11-23 08:56:46.178 validating: 2021 / 3191
11-23 08:56:53.129 validating: 2041 / 3191
11-23 08:57:00.410 validating: 2061 / 3191
11-23 08:57:07.458 validating: 2081 / 3191
11-23 08:57:14.419 validating: 2101 / 3191
11-23 08:57:21.368 validating: 2121 / 3191
11-23 08:57:28.358 validating: 2141 / 3191
11-23 08:57:35.405 validating: 2161 / 3191
11-23 08:57:42.476 validating: 2181 / 3191
11-23 08:57:49.619 validating: 2201 / 3191
11-23 08:57:56.640 validating: 2221 / 3191
11-23 08:58:03.623 validating: 2241 / 3191
11-23 08:58:10.811 validating: 2261 / 3191
11-23 08:58:17.852 validating: 2281 / 3191
11-23 08:58:25.070 validating: 2301 / 3191
11-23 08:58:32.081 validating: 2321 / 3191
11-23 08:58:39.084 validating: 2341 / 3191
11-23 08:58:46.482 validating: 2361 / 3191
11-23 08:58:53.689 validating: 2381 / 3191
11-23 08:59:00.911 validating: 2401 / 3191
11-23 08:59:08.011 validating: 2421 / 3191
11-23 08:59:15.138 validating: 2441 / 3191
11-23 08:59:22.186 validating: 2461 / 3191
11-23 08:59:29.289 validating: 2481 / 3191
11-23 08:59:36.376 validating: 2501 / 3191
11-23 08:59:43.292 validating: 2521 / 3191
11-23 08:59:50.307 validating: 2541 / 3191
11-23 08:59:57.570 validating: 2561 / 3191
11-23 09:00:04.717 validating: 2581 / 3191
11-23 09:00:11.828 validating: 2601 / 3191
11-23 09:00:18.902 validating: 2621 / 3191
11-23 09:00:26.022 validating: 2641 / 3191
11-23 09:00:33.116 validating: 2661 / 3191
11-23 09:00:40.406 validating: 2681 / 3191
11-23 09:00:47.488 validating: 2701 / 3191
Error!! (1914, 1046) (697, 1276) 20804
Dropping  5442
Error!! (1914, 1046) (697, 1276) 20806
Dropping  5444
Error!! (1914, 1046) (697, 1276) 20805
Dropping  5443
Error!! (1914, 1046) (697, 1276) 20807
Dropping  5445
Error!! (1914, 1046) (697, 1276) 20806
Dropping  5444
Error!! (1914, 1046) (697, 1276) 20808
Dropping  5446
Error!! (1914, 1046) (697, 1276) 20808
Dropping  5446
Error!! (1914, 1046) (697, 1276) 20807
Dropping  5445
Error!! (1914, 1046) (697, 1276) 20809
Dropping  5447
Error!! (1914, 1046) (697, 1276) 20809
Dropping  5447
Error!! (1914, 1046) (697, 1276) 20808
Dropping  5446
Error!! (1914, 1046) (697, 1276) 20810
Dropping  5448
Error!! (1914, 1046) (697, 1276) 20810
Dropping  5448
Error!! (1914, 1046) (697, 1276) 20809
Dropping  5447
Error!! (1914, 1046) (697, 1276) 20811
Dropping  5449
Error!! (1914, 1046) (697, 1276) 20811
Dropping  5449
Error!! (1914, 1046) (697, 1276) 20810
Dropping  5448
Error!! (1914, 1046) (697, 1276) 20812
Dropping  5450
Error!! (1914, 1046) (697, 1276) 20812
Dropping  5450
Error!! (1914, 1046) (697, 1276) 20811
Dropping  5449
Error!! (1914, 1046) (697, 1276) 20813
Dropping  5451
Error!! (1914, 1046) (697, 1276) 20813
Dropping  5451
Error!! (1914, 1046) (697, 1276) 20812
Dropping  5450
Error!! (1914, 1046) (697, 1276) 20814
Dropping  5452
Error!! (1914, 1046) (697, 1276) 20814
Dropping  5452
Error!! (1914, 1046) (697, 1276) 20813
Dropping  5451
Error!! (1914, 1046) (697, 1276) 20815
Dropping  5453
Error!! (1914, 1046) (697, 1276) 20815
Dropping  5453
Error!! (1914, 1046) (697, 1276) 20814
Dropping  5452
Error!! (1914, 1046) (697, 1276) 20816
Dropping  5454
Error!! (1914, 1046) (697, 1276) 20816
Dropping  5454
Error!! (1914, 1046) (697, 1276) 20815
Dropping  5453
Error!! (1914, 1046) (697, 1276) 20817
Dropping  5455
Error!! (1914, 1046) (697, 1276) 20817
Dropping  5455
Error!! (1914, 1046) (697, 1276) 20816
Dropping  5454
Error!! (1914, 1046) (697, 1276) 20818
Dropping  5456
Error!! (1914, 1046) (697, 1276) 20818
Dropping  5456
Error!! (1914, 1046) (697, 1276) 20817
Dropping  5455
Error!! (1914, 1046) (697, 1276) 20819
Dropping  5457
Error!! (1914, 1046) (697, 1276) 20819
Dropping  5457
Error!! (1914, 1046) (697, 1276) 20818
Dropping  5456
Error!! (1914, 1046) (697, 1276) 20820
Dropping  5458
Error!! (1914, 1046) (697, 1276) 20820
Dropping  5458
Error!! (1914, 1046) (697, 1276) 20819
Dropping  5457
Error!! (1914, 1046) (697, 1276) 20821
Dropping  5459
Error!! (1914, 1046) (697, 1276) 20821
Dropping  5459
Error!! (1914, 1046) (697, 1276) 20820
Dropping  5458
Error!! (1914, 1046) (697, 1276) 20822
Dropping  5460
Error!! (1914, 1046) (697, 1276) 20822
Dropping  5460
Error!! (1914, 1046) (697, 1276) 20821
Dropping  5459
Error!! (1914, 1046) (697, 1276) 20823
Dropping  5461
Error!! (1914, 1046) (697, 1276) 20823
Dropping  5461
Error!! (1914, 1046) (697, 1276) 20822
Dropping  5460
Error!! (1914, 1046) (697, 1276) 20824
Dropping  5462
Error!! (1914, 1046) (697, 1276) 20824
Dropping  5462
Error!! (1914, 1046) (697, 1276) 20823
Dropping  5461
Error!! (1914, 1046) (697, 1276) 20825
Dropping  5463
Error!! (1914, 1046) (697, 1276) 20825
Dropping  5463
Error!! (1914, 1046) (697, 1276) 20824
Dropping  5462
Error!! (1914, 1046) (697, 1276) 20826
Dropping  5464
Error!! (1914, 1046) (697, 1276) 20826
Dropping  5464
Error!! (1914, 1046) (697, 1276) 20825
Dropping  5463
Error!! (1914, 1046) (697, 1276) 20827
Dropping  5465
Error!! (1914, 1046) (697, 1276) 20827
Dropping  5465
Error!! (1914, 1046) (697, 1276) 20826
Dropping  5464
Error!! (1914, 1046) (697, 1276) 20828
Dropping  5466
Error!! (1914, 1046) (697, 1276) 20828
Dropping  5466
Error!! (1914, 1046) (697, 1276) 20827
Dropping  5465
Error!! (1914, 1046) (697, 1276) 20829
Dropping  5467
Error!! (1914, 1046) (697, 1276) 20829
Dropping  5467
Error!! (1914, 1046) (697, 1276) 20828
Dropping  5466
Error!! (1914, 1046) (697, 1276) 20830
Dropping  5468
Error!! (1914, 1046) (697, 1276) 20830
Dropping  5468
Error!! (1914, 1046) (697, 1276) 20831
Dropping  5469
Error!! (1914, 1046) (697, 1276) 20831
Dropping  5469
Error!! (1914, 1046) (697, 1276) 20829
Dropping  5467
Error!! (1914, 1046) (697, 1276) 20832
Dropping  5470
Error!! (1914, 1046) (697, 1276) 20832
Dropping  5470
Error!! (1914, 1046) (697, 1276) 20830
Dropping  5468
Error!! (1914, 1046) (697, 1276) 20833
Dropping  5471
Error!! (1914, 1046) (697, 1276) 20833
Dropping  5471
Error!! (1914, 1046) (697, 1276) 20831
Dropping  5469
Error!! (1914, 1046) (697, 1276) 20834
Dropping  5472
Error!! (1914, 1046) (697, 1276) 20834
Dropping  5472
Error!! (1914, 1046) (697, 1276) 20832
Dropping  5470
Error!! (1914, 1046) (697, 1276) 20858
Dropping  5473
Error!! (1914, 1046) (697, 1276) 20858
Dropping  5473
Error!! (1914, 1046) (697, 1276) 20833
Dropping  5471
Error!! (1914, 1046) (697, 1276) 20859
Dropping  5474
Error!! (1914, 1046) (697, 1276) 20859
Dropping  5474
Error!! (1914, 1046) (697, 1276) 20834
Dropping  5472
Error!! (1914, 1046) (697, 1276) 20860
Dropping  5475
Error!! (1914, 1046) (697, 1276) 20860
Dropping  5475
Error!! (1914, 1046) (697, 1276) 20858
Dropping  5473
Error!! (1914, 1046) (697, 1276) 20859
Dropping  5474
Error!! (1914, 1046) (697, 1276) 20860
Dropping  5475
Error!! (1914, 1046) (697, 1276) 20810
Dropping  5448
Error!! (1914, 1046) (697, 1276) 20803
Dropping  5441
11-23 09:00:54.766 validating: 2721 / 3191
Error!! (1914, 1046) (697, 1276) 20811
Dropping  5449
Error!! (1914, 1046) (697, 1276) 20804
Dropping  5442
Error!! (1914, 1046) (697, 1276) 20812
Dropping  5450
Error!! (1914, 1046) (697, 1276) 20805
Dropping  5443
Error!!Error!!  (1914, 1046)(1914, 1046)  (697, 1276)(697, 1276)  2081320806

Dropping Dropping   54515444

Error!! (1914, 1046) (697, 1276) 20807
Dropping  5445
Error!! (1914, 1046) (697, 1276) 20814
Dropping  5452
Error!! (1914, 1046) (697, 1276) 20808
Dropping  5446
Error!! (1914, 1046) (697, 1276) 20815
Dropping  5453
Error!! (1914, 1046) (697, 1276) 20816
Dropping  5454
Error!! (1914, 1046) (697, 1276) 20809
Dropping  5447
Error!! (1914, 1046) (697, 1276) 20810
Dropping  5448
Error!! (1914, 1046) (697, 1276) 20817
Dropping  5455
Error!! (1914, 1046) (697, 1276) 20811
Dropping  5449
Error!! (1914, 1046) (697, 1276) 20807
Dropping  5445
Error!! (1914, 1046) (697, 1276) 20818
Dropping  5456
Error!! (1914, 1046) (697, 1276) 20812
Dropping  5450
Error!! (1914, 1046) (697, 1276) 20808
Dropping  5446
Error!! (1914, 1046) (697, 1276) 20819
Dropping  5457
Error!! (1914, 1046) (697, 1276) 20809
Dropping  5447
Error!! (1914, 1046) (697, 1276) 20813
Dropping  5451
Error!! (1914, 1046) (697, 1276) 20820
Dropping  5458
Error!! (1914, 1046) (697, 1276) 20814
Dropping  5452
Error!! (1914, 1046) (697, 1276) 20810
Dropping  5448
Error!! (1914, 1046) (697, 1276) 20821
Dropping  5459
Error!! (1914, 1046) (697, 1276) 20815
Dropping  5453
Error!! (1914, 1046) (697, 1276) 20811
Dropping  5449
Error!! (1914, 1046) (697, 1276) 20822
Dropping  5460
Error!! (1914, 1046) (697, 1276) 20816
Dropping  5454
Error!! (1914, 1046) (697, 1276) 20816
Dropping  5454
Error!! (1914, 1046) (697, 1276) 20812
Dropping  5450
Error!! (1914, 1046) (697, 1276) 20814
Dropping  5452
Error!! (1914, 1046) (697, 1276) 20823
Dropping  5461
Error!! (1914, 1046) (697, 1276) 20817
Dropping  5455
Error!! (1914, 1046) (697, 1276) 20813
Dropping  5451
Error!! (1914, 1046) (697, 1276) 20815
Dropping  5453
Error!! (1914, 1046) (697, 1276) 20817
Dropping  5455
Error!! (1914, 1046) (697, 1276) 20824
Dropping  5462
Error!! (1914, 1046) (697, 1276) 20818
Dropping  5456
Error!! (1914, 1046) (697, 1276) 20814
Dropping  5452
Error!! (1914, 1046) (697, 1276) 20816
Dropping  5454
Error!! (1914, 1046) (697, 1276) 20818
Dropping  5456
Error!! (1914, 1046) (697, 1276) 20825
Dropping  5463
Error!! (1914, 1046) (697, 1276) 20819
Dropping  5457
Error!! (1914, 1046) (697, 1276) 20815
Dropping  5453
Error!! (1914, 1046) (697, 1276) 20817
Dropping  5455
Error!! (1914, 1046) (697, 1276) 20819
Dropping  5457
Error!! (1914, 1046) (697, 1276) 20812
Dropping  5450
Error!! (1914, 1046) (697, 1276) 20826
Dropping  5464
Error!! (1914, 1046) (697, 1276) 20816
Dropping  5454
Error!! (1914, 1046) (697, 1276) 20820
Dropping  5458
Error!! (1914, 1046) (697, 1276) 20818
Dropping  5456
Error!! (1914, 1046) (697, 1276) 20820
Dropping  5458
Error!! (1914, 1046) (697, 1276) 20813
Dropping  5451
Error!! (1914, 1046) (697, 1276) 20827
Dropping  5465
Error!!Error!!  (1914, 1046) (1914, 1046)(697, 1276)  (697, 1276)20817 
20821Dropping 
 5455Dropping 
 5459
Error!! (1914, 1046) (697, 1276) 20819
Dropping  5457
Error!! (1914, 1046) (697, 1276) 20814
Dropping  5452
Error!! (1914, 1046) (697, 1276) 20821
Dropping  5459
Error!! (1914, 1046) (697, 1276) 20828
Dropping  5466
Error!! (1914, 1046) (697, 1276) 20822
Dropping  5460
Error!! (1914, 1046) (697, 1276) 20818
Dropping  5456
Error!! (1914, 1046) (697, 1276) 20820
Dropping  5458
Error!! (1914, 1046) (697, 1276) 20815
Dropping  5453
Error!! (1914, 1046) (697, 1276) 20829
Dropping  5467
Error!! (1914, 1046) (697, 1276) 20822
Dropping  5460
Error!! (1914, 1046) (697, 1276) 20823
Dropping  5461
Error!! (1914, 1046) (697, 1276) 20819
Dropping  5457
Error!! (1914, 1046) (697, 1276) 20821
Dropping  5459
Error!! (1914, 1046) (697, 1276) 20816
Dropping  5454
Error!! (1914, 1046) (697, 1276) 20830
Dropping  5468
Error!! (1914, 1046) (697, 1276) 20824
Dropping  5462
Error!! (1914, 1046) (697, 1276) 20820
Dropping  5458
Error!! (1914, 1046) (697, 1276) 20823
Dropping  5461
Error!! (1914, 1046) (697, 1276) 20822
Dropping  5460
Error!! (1914, 1046) (697, 1276) 20817
Dropping  5455
Error!! (1914, 1046) (697, 1276) 20831
Dropping  5469
Error!! (1914, 1046) (697, 1276) 20825
Dropping  5463
Error!! (1914, 1046) (697, 1276) 20824
Dropping  5462
Error!! (1914, 1046) (697, 1276) 20823
Dropping  5461
Error!! (1914, 1046) (697, 1276) 20821
Dropping  5459
Error!! (1914, 1046) (697, 1276) 20818
Dropping  5456
Error!! (1914, 1046) (697, 1276) 20832
Dropping  5470
Error!! (1914, 1046) (697, 1276) 20826
Dropping  5464
Error!! (1914, 1046) (697, 1276) 20824
Dropping  5462
Error!! (1914, 1046) (697, 1276) 20825
Dropping  5463
Error!! (1914, 1046) (697, 1276) 20822
Dropping  5460
Error!! (1914, 1046) (697, 1276) 20819
Dropping  5457
Error!! (1914, 1046) (697, 1276) 20833
Dropping  5471
Error!! (1914, 1046) (697, 1276) 20827
Dropping  5465
Error!! (1914, 1046) (697, 1276) 20825
Dropping  5463
Error!! (1914, 1046) (697, 1276) 20805
Dropping  5443
Error!! (1914, 1046) (697, 1276) 20823
Dropping  5461
Error!! (1914, 1046) (697, 1276) 20826
Dropping  5464
Error!! (1914, 1046) (697, 1276) 20820
Dropping  5458
Error!! (1914, 1046) (697, 1276) 20834
Dropping  5472
Error!! (1914, 1046) (697, 1276) 20828
Dropping  5466
Error!! (1914, 1046) (697, 1276) 20826
Dropping  5464
Error!! (1914, 1046) (697, 1276) 20806
Dropping  5444
Error!! (1914, 1046) (697, 1276) 20824
Dropping  5462
Error!! (1914, 1046) (697, 1276) 20827
Dropping  5465
Error!! (1914, 1046) (697, 1276) 20821
Dropping  5459
Error!! (1914, 1046) (697, 1276) 20858
Dropping  5473
Error!! (1914, 1046) (697, 1276) 20829
Dropping  5467
Error!! (1914, 1046) (697, 1276) 20827
Dropping  5465
Error!! (1914, 1046) (697, 1276) 20807
Dropping  5445
Error!! (1914, 1046) (697, 1276) 20825
Dropping  5463
Error!! (1914, 1046) (697, 1276) 20828
Dropping  5466
Error!! (1914, 1046) (697, 1276) 20822
Dropping  5460
Error!! (1914, 1046) (697, 1276) 20859
Dropping  5474
Error!! (1914, 1046) (697, 1276) 20830
Dropping  5468
Error!! (1914, 1046) (697, 1276) 20828
Dropping  5466
Error!! (1914, 1046) (697, 1276) 20826
Dropping  5464
Error!! (1914, 1046) (697, 1276) 20808
Dropping  5446
Error!! (1914, 1046) (697, 1276) 20823
Dropping  5461
Error!! (1914, 1046) (697, 1276) 20829
Dropping  5467
Error!! (1914, 1046) (697, 1276) 20860
Dropping  5475
Error!! (1914, 1046) (697, 1276) 20831
Dropping  5469
Error!! (1914, 1046) (697, 1276) 20829
Dropping  5467
Error!! (1914, 1046) (697, 1276) 20827
Dropping  5465
Error!! (1914, 1046) (697, 1276) 20809
Dropping  5447
Error!! (1914, 1046) (697, 1276) 20824
Dropping  5462
Error!! (1914, 1046) (697, 1276) 20830
Dropping  5468
Error!! (1914, 1046) (697, 1276) 20832
Dropping  5470
Error!! (1914, 1046) (697, 1276) 20830
Dropping  5468
Error!! (1914, 1046) (697, 1276) 20810
Dropping  5448
Error!! (1914, 1046) (697, 1276) 20828
Dropping  5466
Error!! (1914, 1046) (697, 1276) 20831
Dropping  5469
Error!! (1914, 1046) (697, 1276) 20825
Dropping  5463
Error!! (1914, 1046) (697, 1276) 20833
Dropping  5471
Error!! (1914, 1046) (697, 1276) 20831
Dropping  5469
Error!! (1914, 1046) (697, 1276) 20811
Dropping  5449
Error!! (1914, 1046) (697, 1276) 20829
Dropping  5467
Error!! (1914, 1046) (697, 1276) 20832
Dropping  5470
Error!! (1914, 1046) (697, 1276) 20826
Dropping  5464
Error!! (1914, 1046) (697, 1276) 20834
Dropping  5472
Error!! (1914, 1046) (697, 1276) 20832
Dropping  5470
Error!! (1914, 1046) (697, 1276) 20812
Dropping  5450
Error!! (1914, 1046) (697, 1276) 20830
Dropping  5468
Error!! (1914, 1046) (697, 1276) 20833
Dropping  5471
Error!! (1914, 1046) (697, 1276) 20827
Dropping  5465
Error!! (1914, 1046) (697, 1276) 20809
Dropping  5447
Error!! (1914, 1046) (697, 1276) 20858
Dropping  5473
Error!! (1914, 1046) (697, 1276) 20833
Dropping  5471
Error!! (1914, 1046) (697, 1276) 20813
Dropping  5451
Error!! (1914, 1046) (697, 1276) 20831
Dropping  5469
Error!! (1914, 1046) (697, 1276) 20834
Dropping  5472
Error!! (1914, 1046) (697, 1276) 20810
Dropping  5448
Error!! (1914, 1046) (697, 1276) 20828
Dropping  5466
Error!! (1914, 1046) (697, 1276) 20859
Dropping  5474
Error!! (1914, 1046) (697, 1276) 20834
Dropping  5472
Error!! (1914, 1046) (697, 1276) 20814
Dropping  5452
Error!! (1914, 1046) (697, 1276) 20832
Dropping  5470
Error!! (1914, 1046) (697, 1276) 20858
Dropping  5473
Error!! (1914, 1046) (697, 1276) 20860
Dropping  5475
Error!! (1914, 1046) (697, 1276) 20811
Dropping  5449
Error!! (1914, 1046) (697, 1276) 20829
Dropping  5467
Error!! (1914, 1046) (697, 1276) 20858
Dropping  5473
Error!!Error!!  (1914, 1046)(1914, 1046)  (697, 1276)(697, 1276)  2081520833

Dropping Dropping   54535471

Error!! (1914, 1046) (697, 1276) 20812
Dropping  5450
Error!! (1914, 1046) (697, 1276) 20859
Dropping  5474
Error!! (1914, 1046) (697, 1276) 20859
Dropping  5474
Error!! (1914, 1046) (697, 1276) 20830
Dropping  5468
Error!! (1914, 1046) (697, 1276) 20816
Dropping  5454
Error!! (1914, 1046) (697, 1276) 20834
Dropping  5472
Error!! (1914, 1046) (697, 1276) 20813
Dropping  5451
Error!! (1914, 1046) (697, 1276) 20831
Dropping  5469
Error!!Error!!  (1914, 1046)(1914, 1046)  (697, 1276)(697, 1276)  2086020860

Dropping Dropping   54755475

Error!! (1914, 1046) (697, 1276) 20858
Dropping  5473
Error!! (1914, 1046) (697, 1276) 20817
Dropping  5455
Error!! (1914, 1046) (697, 1276) 20814
Dropping  5452
Error!! (1914, 1046) (697, 1276) 20832
Dropping  5470
Error!! (1914, 1046) (697, 1276) 20859
Dropping  5474
Error!! (1914, 1046) (697, 1276) 20818
Dropping  5456
Error!! (1914, 1046) (697, 1276) 20815
Dropping  5453
Error!! (1914, 1046) (697, 1276) 20833
Dropping  5471
Error!! (1914, 1046) (697, 1276) 20860
Dropping  5475
Error!! (1914, 1046) (697, 1276) 20819
Dropping  5457
Error!! (1914, 1046) (697, 1276) 20816
Dropping  5454
Error!! (1914, 1046) (697, 1276) 20834
Dropping  5472
Error!! (1914, 1046) (697, 1276) 20820
Dropping  5458
Error!! (1914, 1046) (697, 1276) 20858
Dropping  5473
Error!! (1914, 1046) (697, 1276) 20817
Dropping  5455
Error!! (1914, 1046) (697, 1276) 20821
Dropping  5459
Error!! (1914, 1046) (697, 1276) 20859
Dropping  5474
Error!! (1914, 1046) (697, 1276) 20818
Dropping  5456
Error!! (1914, 1046) (697, 1276) 20822
Dropping  5460
Error!! (1914, 1046) (697, 1276) 20860
Dropping  5475
Error!! (1914, 1046) (697, 1276) 20819
Dropping  5457
Error!! (1914, 1046) (697, 1276) 20823
Dropping  5461
Error!! (1914, 1046) (697, 1276) 20820
Dropping  5458
Error!! (1914, 1046) (697, 1276) 20824
Dropping  5462
Error!! (1914, 1046) (697, 1276) 20821
Dropping  5459
Error!! (1914, 1046) (697, 1276) 20825
Dropping  5463
Error!! (1914, 1046) (697, 1276) 20822
Dropping  5460
Error!! (1914, 1046) (697, 1276) 20826
Dropping  5464
Error!! (1914, 1046) (697, 1276) 20823
Dropping  5461
Error!! (1914, 1046) (697, 1276) 20818
Dropping  5456
Error!! (1914, 1046) (697, 1276) 20827
Dropping  5465
Error!! (1914, 1046) (697, 1276) 20824
Dropping  5462
Error!! (1914, 1046) (697, 1276) 20819
Dropping  5457
Error!! (1914, 1046) (697, 1276) 20828
Dropping  5466
Error!! (1914, 1046) (697, 1276) 20825
Dropping  5463
Error!! (1914, 1046) (697, 1276) 20820
Dropping  5458
Error!! (1914, 1046) (697, 1276) 20829
Dropping  5467
Error!! (1914, 1046) (697, 1276) 20826
Dropping  5464
Error!! (1914, 1046) (697, 1276) 20821
Dropping  5459
Error!! (1914, 1046) (697, 1276) 20830
Dropping  5468
Error!! (1914, 1046) (697, 1276) 20827
Dropping  5465
Error!! (1914, 1046) (697, 1276) 20822
Dropping  5460
Error!! (1914, 1046) (697, 1276) 20831
Dropping  5469
Error!! (1914, 1046) (697, 1276) 20828
Dropping  5466
Error!! (1914, 1046) (697, 1276) 20824
Dropping  5462
Error!! (1914, 1046) (697, 1276) 20823
Dropping  5461
Error!! (1914, 1046) (697, 1276) 20832
Dropping  5470
Error!! (1914, 1046) (697, 1276) 20825
Dropping  5463
Error!! (1914, 1046) (697, 1276) 20829
Dropping  5467
Error!! (1914, 1046) (697, 1276) 20824
Dropping  5462
Error!! (1914, 1046) (697, 1276) 20833
Dropping  5471
Error!! (1914, 1046) (697, 1276) 20826
Dropping  5464
Error!! (1914, 1046) (697, 1276) 20830
Dropping  5468
Error!! (1914, 1046) (697, 1276) 20822
Dropping  5460
Error!! (1914, 1046) (697, 1276) 20825
Dropping  5463
Error!! (1914, 1046) (697, 1276) 20834
Dropping  5472
Error!! (1914, 1046) (697, 1276) 20811
Dropping  5449
Error!! (1914, 1046) (697, 1276) 20827
Dropping  5465
Error!! (1914, 1046) (697, 1276) 20831
Dropping  5469
Error!! (1914, 1046) (697, 1276) 20823
Dropping  5461
Error!! (1914, 1046) (697, 1276) 20826
Dropping  5464
Error!! (1914, 1046) (697, 1276) 20858
Dropping  5473
Error!! (1914, 1046) (697, 1276) 20812
Dropping  5450
Error!! (1914, 1046) (697, 1276) 20828
Dropping  5466
Error!! (1914, 1046) (697, 1276) 20832
Dropping  5470
Error!! (1914, 1046) (697, 1276) 20824
Dropping  5462
Error!! (1914, 1046) (697, 1276) 20827
Dropping  5465
Error!! (1914, 1046) (697, 1276) 20859
Dropping  5474
Error!! (1914, 1046) (697, 1276) 20813
Dropping  5451
Error!! (1914, 1046) (697, 1276) 20815
Dropping  5453
Error!! (1914, 1046) (697, 1276) 20833
Dropping  5471
Error!! (1914, 1046) (697, 1276) 20829
Dropping  5467
Error!! (1914, 1046) (697, 1276) 20825
Dropping  5463
Error!! (1914, 1046) (697, 1276) 20828
Dropping  5466
Error!! (1914, 1046) (697, 1276) 20860
Dropping  5475
Error!! (1914, 1046) (697, 1276) 20814
Dropping  5452
Error!! (1914, 1046) (697, 1276) 20816
Dropping  5454
Error!! (1914, 1046) (697, 1276) 20834
Dropping  5472
Error!! (1914, 1046) (697, 1276) 20830
Dropping  5468
Error!! (1914, 1046) (697, 1276) 20826
Dropping  5464
Error!! (1914, 1046) (697, 1276) 20829
Dropping  5467
Error!! (1914, 1046) (697, 1276) 20815
Dropping  5453
Error!! (1914, 1046) (697, 1276) 20831
Dropping  5469
Error!! (1914, 1046) (697, 1276) 20817
Dropping  5455
Error!! (1914, 1046) (697, 1276) 20858
Dropping  5473
Error!! (1914, 1046) (697, 1276) 20827
Dropping  5465
Error!! (1914, 1046) (697, 1276) 20830
Dropping  5468
Error!! (1914, 1046) (697, 1276) 20816
Dropping  5454
Error!! (1914, 1046) (697, 1276) 20832
Dropping  5470
Error!! (1914, 1046) (697, 1276) 20818
Dropping  5456
Error!! (1914, 1046) (697, 1276) 20859
Dropping  5474
Error!! (1914, 1046) (697, 1276) 20828
Dropping  5466
Error!! (1914, 1046) (697, 1276) 20820
Dropping  5458
Error!! (1914, 1046) (697, 1276) 20831
Dropping  5469
Error!! (1914, 1046) (697, 1276) 20833
Dropping  5471
Error!! (1914, 1046) (697, 1276) 20817
Dropping  5455
Error!! (1914, 1046) (697, 1276) 20819
Dropping  5457
Error!! (1914, 1046) (697, 1276) 20860
Dropping  5475
Error!! (1914, 1046) (697, 1276) 20829
Dropping  5467
Error!! (1914, 1046) (697, 1276) 20821
Dropping  5459
Error!! (1914, 1046) (697, 1276) 20832
Dropping  5470
Error!! (1914, 1046) (697, 1276) 20834
Dropping  5472
Error!! (1914, 1046) (697, 1276) 20818
Dropping  5456
Error!! (1914, 1046) (697, 1276) 20820
Dropping  5458
Error!! (1914, 1046) (697, 1276) 20830
Dropping  5468
Error!! (1914, 1046) (697, 1276) 20833
Dropping  5471
Error!! (1914, 1046) (697, 1276) 20858
Dropping  5473
Error!! (1914, 1046) (697, 1276) 20822
Dropping  5460
Error!! (1914, 1046) (697, 1276) 20819
Dropping  5457
Error!! (1914, 1046) (697, 1276) 20831
Dropping  5469
Error!! (1914, 1046) (697, 1276) 20821
Dropping  5459
Error!! (1914, 1046) (697, 1276) 20834
Dropping  5472
Error!! (1914, 1046) (697, 1276) 20859
Dropping  5474
Error!! (1914, 1046) (697, 1276) 20823
Dropping  5461
Error!! (1914, 1046) (697, 1276) 20820
Dropping  5458
Error!! (1914, 1046) (697, 1276) 20832
Dropping  5470
Error!! (1914, 1046) (697, 1276) 20822
Dropping  5460
Error!! (1914, 1046) (697, 1276) 20860
Dropping  5475
Error!! (1914, 1046) (697, 1276) 20824
Dropping  5462
Error!! (1914, 1046) (697, 1276) 20858
Dropping  5473
Error!! (1914, 1046) (697, 1276) 20833
Dropping  5471
Error!! (1914, 1046) (697, 1276) 20821
Dropping  5459
Error!! (1914, 1046) (697, 1276) 20823
Dropping  5461
Error!! (1914, 1046) (697, 1276) 20825
Dropping  5463
Error!! (1914, 1046) (697, 1276) 20859
Dropping  5474
Error!! (1914, 1046) (697, 1276) 20834
Dropping  5472
Error!! (1914, 1046) (697, 1276) 20822
Dropping  5460
Error!! (1914, 1046) (697, 1276) 20824
Dropping  5462
Error!! (1914, 1046) (697, 1276) 20826
Dropping  5464
Error!! (1914, 1046) (697, 1276) 20860
Dropping  5475
Error!! (1914, 1046) (697, 1276) 20858
Dropping  5473
Error!! (1914, 1046) (697, 1276) 20823
Dropping  5461
Error!! (1914, 1046) (697, 1276) 20825
Dropping  5463
Error!! (1914, 1046) (697, 1276) 20827
Dropping  5465
Error!! (1914, 1046) (697, 1276) 20859
Dropping  5474
Error!! (1914, 1046) (697, 1276) 20824
Dropping  5462
Error!! (1914, 1046) (697, 1276) 20826
Dropping  5464
Error!! (1914, 1046) (697, 1276) 20828
Dropping  5466
Error!! (1914, 1046) (697, 1276) 20860
Dropping  5475
Error!! (1914, 1046) (697, 1276) 20825
Dropping  5463
Error!! (1914, 1046) (697, 1276) 20827
Dropping  5465
Error!! (1914, 1046) (697, 1276) 20829
Dropping  5467
Error!! (1914, 1046) (697, 1276) 20826
Dropping  5464
Error!! (1914, 1046) (697, 1276) 20828
Dropping  5466
Error!! (1914, 1046) (697, 1276) 20830
Dropping  5468
Error!! (1914, 1046) (697, 1276) 20827
Dropping  5465
Error!! (1914, 1046) (697, 1276) 20829
Dropping  5467
Error!! (1914, 1046) (697, 1276) 20831
Dropping  5469
Error!! (1914, 1046) (697, 1276) 20828
Dropping  5466
Error!! (1914, 1046) (697, 1276) 20830
Dropping  5468
Error!! (1914, 1046) (697, 1276) 20832
Dropping  5470
Error!! (1914, 1046) (697, 1276) 20829
Dropping  5467
Error!! (1914, 1046) (697, 1276) 20831
Dropping  5469
Error!! (1914, 1046) (697, 1276) 20833
Dropping  5471
Error!! (1914, 1046) (697, 1276) 20832
Dropping  5470
Error!! (1914, 1046) (697, 1276) 20830
Dropping  5468
Error!! (1914, 1046) (697, 1276) 20834
Dropping  5472
Error!! (1914, 1046) (697, 1276) 20833
Dropping  5471
Error!! (1914, 1046) (697, 1276) 20831
Dropping  5469
Error!! (1914, 1046) (697, 1276) 20858
Dropping  5473
Error!! (1914, 1046) (697, 1276) 20813
Dropping  5451
Error!! (1914, 1046) (697, 1276) 20834
Dropping  5472
Error!! (1914, 1046) (697, 1276) 20832
Dropping  5470
Error!! (1914, 1046) (697, 1276) 20859
Dropping  5474
Error!! (1914, 1046) (697, 1276) 20814
Dropping  5452
Error!! (1914, 1046) (697, 1276) 20833
Dropping  5471
Error!! (1914, 1046) (697, 1276) 20858
Dropping  5473
Error!! (1914, 1046) (697, 1276) 20860
Dropping  5475
Error!! (1914, 1046) (697, 1276) 20815
Dropping  5453
Error!! (1914, 1046) (697, 1276) 20834
Dropping  5472
Error!! (1914, 1046) (697, 1276) 20859
Dropping  5474
Error!! (1914, 1046) (697, 1276) 20816
Dropping  5454
Error!! (1914, 1046) (697, 1276) 20858
Dropping  5473
Error!! (1914, 1046) (697, 1276) 20860
Dropping  5475
Error!! (1914, 1046) (697, 1276) 20817
Dropping  5455
Error!! (1914, 1046) (697, 1276) 20859
Dropping  5474
Error!! (1914, 1046) (697, 1276) 20817
Dropping  5455
Error!! (1914, 1046) (697, 1276) 20818
Dropping  5456
Error!! (1914, 1046) (697, 1276) 20860
Dropping  5475
Error!! (1914, 1046) (697, 1276) 20818
Dropping  5456
Error!! (1914, 1046) (697, 1276) 20832
Dropping  5470
Error!! (1914, 1046) (697, 1276) 20819
Dropping  5457
Error!! (1914, 1046) (697, 1276) 20819
Dropping  5457
Error!! (1914, 1046) (697, 1276) 20833
Dropping  5471
Error!! (1914, 1046) (697, 1276) 20820
Dropping  5458
Error!! (1914, 1046) (697, 1276) 20820
Dropping  5458
Error!! (1914, 1046) (697, 1276) 20834
Dropping  5472
Error!! (1914, 1046) (697, 1276) 20821
Dropping  5459
Error!! (1914, 1046) (697, 1276) 20821
Dropping  5459
Error!! (1914, 1046) (697, 1276) 20858
Dropping  5473
Error!! (1914, 1046) (697, 1276) 20822
Dropping  5460
Error!! (1914, 1046) (697, 1276) 20826
Dropping  5464
Error!! (1914, 1046) (697, 1276) 20859
Dropping  5474
Error!! (1914, 1046) (697, 1276) 20822
Dropping  5460
Error!! (1914, 1046) (697, 1276) 20823
Dropping  5461
Error!! (1914, 1046) (697, 1276) 20830
Dropping  5468
Error!! (1914, 1046) (697, 1276) 20827
Dropping  5465
Error!! (1914, 1046) (697, 1276) 20860
Dropping  5475
Error!! (1914, 1046) (697, 1276) 20823
Dropping  5461
Error!! (1914, 1046) (697, 1276) 20831
Dropping  5469
Error!! (1914, 1046) (697, 1276) 20824
Dropping  5462
Error!! (1914, 1046) (697, 1276) 20828
Dropping  5466
Error!! (1914, 1046) (697, 1276) 20824
Dropping  5462
Error!! (1914, 1046) (697, 1276) 20832
Dropping  5470
Error!! (1914, 1046) (697, 1276) 20825
Dropping  5463
Error!! (1914, 1046) (697, 1276) 20829
Dropping  5467
Error!! (1914, 1046) (697, 1276) 20825
Dropping  5463
Error!! (1914, 1046) (697, 1276) 20833
Dropping  5471
Error!! (1914, 1046) (697, 1276) 20826
Dropping  5464
Error!! (1914, 1046) (697, 1276) 20830
Dropping  5468
Error!! (1914, 1046) (697, 1276) 20826
Dropping  5464
Error!! (1914, 1046) (697, 1276) 20834
Dropping  5472
Error!! (1914, 1046) (697, 1276) 20827
Dropping  5465
Error!! (1914, 1046) (697, 1276) 20831
Dropping  5469
Error!! (1914, 1046) (697, 1276) 20827
Dropping  5465
Error!! (1914, 1046) (697, 1276) 20858
Dropping  5473
Error!! (1914, 1046) (697, 1276) 20828
Dropping  5466
Error!! (1914, 1046) (697, 1276) 20832
Dropping  5470
Error!! (1914, 1046) (697, 1276) 20828
Dropping  5466
Error!! (1914, 1046) (697, 1276) 20859
Dropping  5474
Error!! (1914, 1046) (697, 1276) 20829
Dropping  5467
Error!! (1914, 1046) (697, 1276) 20833
Dropping  5471
Error!! (1914, 1046) (697, 1276) 20860
Dropping  5475
Error!! (1914, 1046) (697, 1276) 20829
Dropping  5467
Error!! (1914, 1046) (697, 1276) 20834
Dropping  5472
Error!! (1914, 1046) (697, 1276) 20830
Dropping  5468
Error!! (1914, 1046) (697, 1276) 20830
Dropping  5468
Error!! (1914, 1046) (697, 1276) 20831
Dropping  5469
Error!! (1914, 1046) (697, 1276) 20858
Dropping  5473
Error!! (1914, 1046) (697, 1276) 20831
Dropping  5469
Error!! (1914, 1046) (697, 1276) 20832
Dropping  5470
Error!! (1914, 1046) (697, 1276) 20859
Dropping  5474
Error!! (1914, 1046) (697, 1276) 20832
Dropping  5470
Error!! (1914, 1046) (697, 1276) 20828
Dropping  5466
Error!! (1914, 1046) (697, 1276) 20833
Dropping  5471
Error!! (1914, 1046) (697, 1276) 20860
Dropping  5475
Error!! (1914, 1046) (697, 1276) 20833
Dropping  5471
Error!! (1914, 1046) (697, 1276) 20823
Dropping  5461
Error!! (1914, 1046) (697, 1276) 20829
Dropping  5467
Error!! (1914, 1046) (697, 1276) 20834
Dropping  5472
Error!! (1914, 1046) (697, 1276) 20834
Dropping  5472
Error!! (1914, 1046) (697, 1276) 20824
Dropping  5462
Error!! (1914, 1046) (697, 1276) 20830
Dropping  5468
Error!! (1914, 1046) (697, 1276) 20858
Dropping  5473
Error!! (1914, 1046) (697, 1276) 20858
Dropping  5473
Error!! (1914, 1046) (697, 1276) 20819
Dropping  5457
Error!! (1914, 1046) (697, 1276) 20825
Dropping  5463
Error!! (1914, 1046) (697, 1276) 20831
Dropping  5469
Error!! (1914, 1046) (697, 1276) 20859
Dropping  5474
Error!! (1914, 1046) (697, 1276) 20859
Dropping  5474
Error!! (1914, 1046) (697, 1276) 20820
Dropping  5458
Error!! (1914, 1046) (697, 1276) 20826
Dropping  5464
Error!! (1914, 1046) (697, 1276) 20832
Dropping  5470
Error!! (1914, 1046) (697, 1276) 20860
Dropping  5475
Error!! (1914, 1046) (697, 1276) 20860
Dropping  5475
Error!! (1914, 1046) (697, 1276) 20821
Dropping  5459
Error!! (1914, 1046) (697, 1276) 20827
Dropping  5465
Error!! (1914, 1046) (697, 1276) 20833
Dropping  5471
Error!! (1914, 1046) (697, 1276) 20822
Dropping  5460
Error!! (1914, 1046) (697, 1276) 20834
Dropping  5472
Error!! (1914, 1046) (697, 1276) 20828
Dropping  5466
Error!! (1914, 1046) (697, 1276) 20823
Dropping  5461
Error!! (1914, 1046) (697, 1276) 20858
Dropping  5473
Error!! (1914, 1046) (697, 1276) 20829
Dropping  5467
Error!! (1914, 1046) (697, 1276) 20824
Dropping  5462
Error!! (1914, 1046) (697, 1276) 20859
Dropping  5474
Error!! (1914, 1046) (697, 1276) 20830
Dropping  5468
Error!! (1914, 1046) (697, 1276) 20825
Dropping  5463
Error!! (1914, 1046) (697, 1276) 20860
Dropping  5475
Error!! (1914, 1046) (697, 1276) 20831
Dropping  5469
Error!! (1914, 1046) (697, 1276) 20826
Dropping  5464
Error!! (1914, 1046) (697, 1276) 20832
Dropping  5470
Error!! (1914, 1046) (697, 1276) 20833
Dropping  5471
Error!! (1914, 1046) (697, 1276) 20827
Dropping  5465
Error!! (1914, 1046) (697, 1276) 20834
Dropping  5472
Error!! (1914, 1046) (697, 1276) 20828
Dropping  5466
Error!! (1914, 1046) (697, 1276) 20858
Dropping  5473
Error!! (1914, 1046) (697, 1276) 20829
Dropping  5467
Error!! (1914, 1046) (697, 1276) 20859
Dropping  5474
Error!! (1914, 1046) (697, 1276) 20830
Dropping  5468
Error!! (1914, 1046) (697, 1276) 20860
Dropping  5475
Error!! (1914, 1046) (697, 1276) 20831
Dropping  5469
Error!! (1914, 1046) (697, 1276) 20832
Dropping  5470
Error!! (1914, 1046) (697, 1276) 20833
Dropping  5471
Error!! (1914, 1046) (697, 1276) 20834
Dropping  5472
Error!! (1914, 1046) (697, 1276) 20834
Dropping  5472
Error!! (1914, 1046) (697, 1276) 20858
Dropping  5473
Error!! (1914, 1046) (697, 1276) 20858
Dropping  5473
Error!! (1914, 1046) (697, 1276) 20859
Dropping  5474
Error!! (1914, 1046) (697, 1276) 20859
Dropping  5474
Error!! (1914, 1046) (697, 1276) 20860
Dropping  5475
Error!! (1914, 1046) (697, 1276) 20860
Dropping  5475
Error!! (1914, 1046) (697, 1276) 20825
Dropping  5463
Error!! (1914, 1046) (697, 1276) 20821
Dropping  5459
Error!! (1914, 1046) (697, 1276) 20826
Dropping  5464
Error!! (1914, 1046) (697, 1276) 20822
Dropping  5460
Error!! (1914, 1046) (697, 1276) 20827
Dropping  5465
Error!! (1914, 1046) (697, 1276) 20823
Dropping  5461
Error!! (1914, 1046) (697, 1276) 20828
Dropping  5466
Error!! (1914, 1046) (697, 1276) 20859
Dropping  5474
Error!! (1914, 1046) (697, 1276) 20824
Dropping  5462
Error!! (1914, 1046) (697, 1276) 20829
Dropping  5467
Error!! (1914, 1046) (697, 1276) 20860
Dropping  5475
Error!! (1914, 1046) (697, 1276) 20825
Dropping  5463
Error!! (1914, 1046) (697, 1276) 20830
Dropping  5468
Error!! (1914, 1046) (697, 1276) 20826
Dropping  5464
Error!! (1914, 1046) (697, 1276) 20831
Dropping  5469
Error!! (1914, 1046) (697, 1276) 20827
Dropping  5465
Error!! (1914, 1046) (697, 1276) 20832
Dropping  5470
Error!! (1914, 1046) (697, 1276) 20828
Dropping  5466
Error!! (1914, 1046) (697, 1276) 20833
Dropping  5471
Error!! (1914, 1046) (697, 1276) 20829
Dropping  5467
Error!! (1914, 1046) (697, 1276) 20834
Dropping  5472
Error!! (1914, 1046) (697, 1276) 20831
Dropping  5469
Error!! (1914, 1046) (697, 1276) 20830
Dropping  5468
Error!! (1914, 1046) (697, 1276) 20858
Dropping  5473
Error!! (1914, 1046) (697, 1276) 20832
Dropping  5470
Error!! (1914, 1046) (697, 1276) 20831
Dropping  5469
Error!! (1914, 1046) (697, 1276) 20833
Dropping  5471
Error!! (1914, 1046) (697, 1276) 20859
Dropping  5474
Error!! (1914, 1046) (697, 1276) 20832
Dropping  5470
Error!! (1914, 1046) (697, 1276) 20834
Dropping  5472
Error!! (1914, 1046) (697, 1276) 20860
Dropping  5475
Error!! (1914, 1046) (697, 1276) 20833
Dropping  5471
Error!! (1914, 1046) (697, 1276) 20858
Dropping  5473
Error!! (1914, 1046) (697, 1276) 20834
Dropping  5472
Error!! (1914, 1046) (697, 1276) 20859
Dropping  5474
Error!! (1914, 1046) (697, 1276) 20858
Dropping  5473
Error!! (1914, 1046) (697, 1276) 20860
Dropping  5475
Error!! (1914, 1046) (697, 1276) 20827
Dropping  5465
Error!! (1914, 1046) (697, 1276) 20859
Dropping  5474
Error!! (1914, 1046) (697, 1276) 20828
Dropping  5466
Error!! (1914, 1046) (697, 1276) 20860
Dropping  5475
Error!! (1914, 1046) (697, 1276) 20829
Dropping  5467
Error!! (1914, 1046) (697, 1276) 20830
Dropping  5468
Error!! (1914, 1046) (697, 1276) 20831
Dropping  5469
Error!! (1914, 1046) (697, 1276) 20832
Dropping  5470
Error!! (1914, 1046) (697, 1276) 20833
Dropping  5471
Error!! (1914, 1046) (697, 1276) 20834
Dropping  5472
Error!! (1914, 1046) (697, 1276) 20858
Dropping  5473
Error!! (1914, 1046) (697, 1276) 20859
Dropping  5474
Error!! (1914, 1046) (697, 1276) 20860
Dropping  5475
Error!! (1914, 1046) (697, 1276) 20833
Dropping  5471
Error!! (1914, 1046) (697, 1276) 20834
Dropping  5472
Error!! (1914, 1046) (697, 1276) 20858
Dropping  5473
Error!! (1914, 1046) (697, 1276) 20859
Dropping  5474
Error!! (1914, 1046) (697, 1276) 20829
Dropping  5467
Error!! (1914, 1046) (697, 1276) 20860
Dropping  5475
Error!! (1914, 1046) (697, 1276) 20830
Dropping  5468
11-23 09:01:07.174 validating: 2741 / 3191
Error!! (1914, 1046) (697, 1276) 20831
Dropping  5469
Error!! (1914, 1046) (697, 1276) 20832
Dropping  5470
Error!! (1914, 1046) (697, 1276) 20833
Dropping  5471
Error!! (1914, 1046) (697, 1276) 20834
Dropping  5472
Error!! (1914, 1046) (697, 1276) 20858
Dropping  5473
Error!! (1914, 1046) (697, 1276) 20859
Dropping  5474
Error!! (1914, 1046) (697, 1276) 20858
Dropping  5473
Error!! (1914, 1046) (697, 1276) 20860
Dropping  5475
Error!! (1914, 1046) (697, 1276) 20859
Dropping  5474
Error!! (1914, 1046) (697, 1276) 20860
Dropping  5475
Error!! (1914, 1046) (697, 1276) 20860
Dropping  5475
11-23 09:01:14.684 validating: 2761 / 3191
11-23 09:01:22.003 validating: 2781 / 3191
11-23 09:01:29.101 validating: 2801 / 3191
11-23 09:01:36.135 validating: 2821 / 3191
11-23 09:01:43.169 validating: 2841 / 3191
11-23 09:01:50.261 validating: 2861 / 3191
11-23 09:01:57.195 validating: 2881 / 3191
11-23 09:02:04.278 validating: 2901 / 3191
11-23 09:02:11.372 validating: 2921 / 3191
11-23 09:02:18.502 validating: 2941 / 3191
11-23 09:02:25.701 validating: 2961 / 3191
11-23 09:02:32.705 validating: 2981 / 3191
11-23 09:02:39.703 validating: 3001 / 3191
11-23 09:02:46.744 validating: 3021 / 3191
11-23 09:02:53.952 validating: 3041 / 3191
11-23 09:03:01.054 validating: 3061 / 3191
11-23 09:03:08.214 validating: 3081 / 3191
11-23 09:03:15.193 validating: 3101 / 3191
11-23 09:03:22.240 validating: 3121 / 3191
11-23 09:03:29.345 validating: 3141 / 3191
11-23 09:03:36.553 validating: 3161 / 3191
11-23 09:03:43.795 validating: 3181 / 3191
11-23 09:03:51.501 Dataset name: gtav
11-23 09:03:51.502 IoU:
11-23 09:03:51.502 label_id      label    iU    Precision Recall TP     FP    FN
11-23 09:03:51.502  0                0    94.4    1.0       1.0   31.9     0.0     0.0
11-23 09:03:51.502  1                1    82.2    0.9       0.9    9.7     0.1     0.1
11-23 09:03:51.502  2                2    89.7    0.9       0.9   20.4     0.1     0.1
11-23 09:03:51.502  3                3    54.8    0.7       0.7    1.3     0.4     0.4
11-23 09:03:51.502  4                4    31.3    0.4       0.6    0.2     1.4     0.8
11-23 09:03:51.502  5                5    55.3    0.8       0.7    0.9     0.3     0.5
11-23 09:03:51.502  6                6    64.7    0.7       0.8    0.1     0.4     0.2
11-23 09:03:51.502  7                7    63.8    0.7       0.9    0.0     0.4     0.1
11-23 09:03:51.502  8                8    85.8    0.9       0.9    9.1     0.1     0.1
11-23 09:03:51.502  9                9    74.3    0.8       0.9    2.2     0.2     0.2
11-23 09:03:51.502 10               10    96.8    1.0       1.0   14.0     0.0     0.0
11-23 09:03:51.502 11               11    74.3    0.8       0.9    0.4     0.2     0.1
11-23 09:03:51.502 12               12    64.2    0.8       0.8    0.0     0.3     0.2
11-23 09:03:51.503 13               13    90.2    1.0       0.9    2.5     0.0     0.1
11-23 09:03:51.503 14               14    89.3    0.9       0.9    1.2     0.1     0.1
11-23 09:03:51.503 15               15    93.0    1.0       1.0    0.2     0.0     0.0
/home/tx/Workspace/RobustNet/utils/misc.py:278: RuntimeWarning: divide by zero encountered in float_scalars
  iu_false_positive[idx] / iu_true_positive[idx])
/home/tx/Workspace/RobustNet/utils/misc.py:279: RuntimeWarning: divide by zero encountered in float_scalars
  fn = '{:5.1f}'.format(iu_false_negative[idx] / iu_true_positive[idx])
11-23 09:03:51.503 16               16     0.0    0.0       0.0    0.0     inf     inf
11-23 09:03:51.503 17               17    71.2    0.8       0.9    0.0     0.3     0.2
11-23 09:03:51.503 18               18    48.5    0.7       0.6    0.0     0.3     0.7
11-23 09:03:51.503 mean 0.6967936158180237
Unseen domain evaluation starts!
Extra validating... This won't save pth file
11-23 09:03:52.247 -----------------------------------------------------------------------------------------------------------
11-23 09:03:52.247 [epoch 26], [dataset name gtav], [val loss 0.17283], [acc 0.94052], [acc_cls 0.78304], [mean_iu 0.69679], [fwavacc 0.89204]
11-23 09:03:52.247 best record: [dataset name gtav], [val loss 0.17283], [acc 0.94052], [acc_cls 0.78304], [mean_iu 0.69679], [fwavacc 0.89204], [epoch 26], 
11-23 09:03:52.247 -----------------------------------------------------------------------------------------------------------
/home/tx/.conda/envs/robustnet/lib/python3.7/site-packages/torchvision/utils.py:50: UserWarning: range will be deprecated, please use value_range instead.
  warnings.warn(warning)
Unseen domain evaluation starts!
Extra validating... This won't save pth file
11-23 09:03:53.849 validating: 1 / 250
11-23 09:03:57.493 validating: 21 / 250
11-23 09:04:00.958 validating: 41 / 250
11-23 09:04:04.056 validating: 61 / 250
11-23 09:04:06.872 validating: 81 / 250
11-23 09:04:10.184 validating: 101 / 250
11-23 09:04:13.085 validating: 121 / 250
11-23 09:04:16.379 validating: 141 / 250
11-23 09:04:19.315 validating: 161 / 250
11-23 09:04:22.410 validating: 181 / 250
11-23 09:04:25.452 validating: 201 / 250
11-23 09:04:29.131 validating: 221 / 250
11-23 09:04:32.384 validating: 241 / 250
11-23 09:04:34.831 Dataset name: cityscapes
11-23 09:04:34.831 IoU:
11-23 09:04:34.831 label_id      label    iU    Precision Recall TP     FP    FN
11-23 09:04:34.831  0                0    62.8    0.6       1.0   24.4     0.5     0.0
11-23 09:04:34.831  1                1    26.8    0.7       0.3    3.7     0.5     2.3
11-23 09:04:34.831  2                2    67.0    0.9       0.7   20.8     0.1     0.4
11-23 09:04:34.831  3                3    23.5    0.4       0.4    0.3     1.4     1.8
11-23 09:04:34.831  4                4    26.6    0.4       0.5    0.3     1.6     1.2
11-23 09:04:34.831  5                5    24.9    0.3       0.8    0.4     2.7     0.3
11-23 09:04:34.832  6                6    30.7    0.3       0.9    0.1     2.1     0.2
11-23 09:04:34.832  7                7    15.1    0.2       0.9    0.1     5.5     0.1
11-23 09:04:34.832  8                8    85.5    0.9       0.9   16.2     0.1     0.1
11-23 09:04:34.832  9                9    36.4    0.6       0.5    0.5     0.7     1.0
11-23 09:04:34.832 10               10    67.2    0.8       0.9    2.6     0.3     0.2
11-23 09:04:34.832 11               11    60.8    0.8       0.7    1.0     0.3     0.4
11-23 09:04:34.832 12               12     8.4    0.1       0.6    0.0    10.2     0.7
11-23 09:04:34.832 13               13    80.5    0.9       0.9    5.8     0.1     0.1
11-23 09:04:34.832 14               14    23.6    0.4       0.4    0.1     1.5     1.7
11-23 09:04:34.832 15               15    27.3    0.3       0.8    0.1     2.3     0.3
11-23 09:04:34.832 16               16     4.9    0.1       0.4    0.0    18.0     1.4
11-23 09:04:34.832 17               17    14.7    0.2       0.4    0.0     4.4     1.4
11-23 09:04:34.832 18               18    14.1    0.1       0.9    0.1     6.0     0.1
11-23 09:04:34.832 mean 0.36894041299819946
11-23 09:04:34.832 -----------------------------------------------------------------------------------------------------------
11-23 09:04:34.833 [epoch 26], [dataset name cityscapes], [val loss 0.68988], [acc 0.76399], [acc_cls 0.47097], [mean_iu 0.36894], [fwavacc 0.64414]
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
